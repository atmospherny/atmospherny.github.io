<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Atmospherny feed</title>
<style type="text/css">
@font-face {
    font-family: 'PT Serif';
    src: url(PT_Serif-Web-Regular.ttf);
    font-style: normal;
}

body {
	margin: 0;
}

#container {
	width: 100%;
	max-width: 860px;
	color: #333;
	margin: auto;
	background-color: #eee;
	border-style: solid;
	border-width: 2px;
	border-color: #009;
	padding: 1em;
}

button {
	width: 100%;
	margin: 0;
	background-color: #fff;
	border-style: dashed;
	border-color: #333;
	border-width: 1px;
	font-size: 48pt;
	padding: 0.5em;
	color: #222;
	font-family: Monospace;
	outline: none;
	-webkit-transition: background-color 2s;
	-moz-transition: background-color 2s;
	-o-transition: background-color 2s;
	transition: background-color 2s;
}

button:hover {
	background-color: #aaa;
	-webkit-transition: background-color 0.2s;
	-moz-transition: background-color 0.2s;
	-o-transition: background-color 0.2s;
	transition: background-color 0.2s;
}

table {
	width: 100%;
}

#current {
	font-size: 32px;
	font-family: 'PT Serif';
	padding: 0.3em;
}

#current * {
	font-size: 32px;	
}

summary {
	background-color: #ddd;
	border-style: dotted;
	border-width: 1px;
	padding: 0.3em;
	outline: none;
	cursor: pointer;
}

#progress {
	text-align: center;
}

#weekday {
	text-align: center;
}

a {
	color: #333;
}

a:visited {
	color: #888;
}

</style>

<script type="text/javascript">
var currentIndex = 0;
var myChoice = new Object();

var days = ['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday'];

function o(s) {
	return document.getElementById(s);
}

function next() {
	currentIndex++;
	if (currentIndex == o("titles").innerHTML.split("{{{{ARTICLE_PARSER}}}}").length) currentIndex=0;
	update();
	updateIndex();
}

function prev() {
	currentIndex--;
	if (currentIndex < 0) currentIndex = o("titles").innerHTML.split("{{{{ARTICLE_PARSER}}}}").length - 1;
	update();
	updateIndex();
}

function choose() {
	myChoice["a"+currentIndex] = "tr";
	update()
}

function clubs() {
	myChoice["a"+currentIndex] = "bl";
	update()
}

function spades() {
	myChoice["a"+currentIndex] = "br";
	update()	
}

function unchoose() {
	myChoice["a"+currentIndex] = "tl";
	update()
}

function ifchosen() {
	switch(myChoice["a"+currentIndex]) {
		case "tr":
			o("current-title").innerHTML = "&hearts; " + o("current-title").innerHTML
			break;
		case "br":
			o("current-title").innerHTML = "&spades; " + o("current-title").innerHTML
			break;
		case "bl":
			o("current-title").innerHTML = "&clubs; " + o("current-title").innerHTML
			break;
		default:
			o("current-title").innerHTML = "&diams; " + o("current-title").innerHTML		
	}
}

function update() {
	o("current-title").innerHTML = o("titles").innerHTML.split("{{{{ARTICLE_PARSER}}}}")[currentIndex];
	o("current-abstract").innerHTML = o("abstracts").innerHTML.split("{{{{ARTICLE_PARSER}}}}")[currentIndex] + "<br>" + "<a href='" + o("links").innerHTML.split("{{{{ARTICLE_PARSER}}}}")[currentIndex] + "' target='_blank'>download pdf</a>";
	ifchosen();
	updateStorage();
	o("progress").innerHTML = "#" + (currentIndex + 1);
}

function updateStorage() {
	localStorage["myChoice"] = JSON.stringify(myChoice)
}

function updateIndex() {
	localStorage["currentIndex"] = currentIndex;
}

function updateChoice() {
	try {
		myChoice = JSON.parse(localStorage["myChoice"]);
		currentIndex = parseInt(localStorage["currentIndex"]);
		if (typeof(currentIndex)!= "number") {
			localStorage["currentIndex"] = 0;
			currentIndex = 0;
		}
	} catch(e) {
		localStorage["currentIndex"] = 0;
		localStorage["myChoice"] = JSON.stringify("{}");
		currentIndex = 0;
		myChoice = {};
	} 
}

function listA() {
	buf = ""
	for(var i in myChoice) {
		if ((myChoice[i] == "tr") || (myChoice[i] == "bl")) buf += "#" + (parseInt(i.split("a")[1]) + 1) + " "
	}
	window.prompt("Share your choice:", buf);
}

function listB() {
	buf = ""
	for(var i in myChoice) {
		if ((myChoice[i] == "tr") || (myChoice[i] == "br")) buf += "#" + (parseInt(i.split("a")[1]) + 1) + " "
	}
	window.prompt("Share your choice:", buf);
}

function setWeekday() {
	var d = new Date();
	var n = d.getUTCHours();
	if (o("weekday").innerHTML != days[now.getDay()]) {
		if((days[now.getDay()] != "Saturday") && (days[now.getDay()] != "Sunday")) {
			if(n>3) {
				localStorage["myChoice"] = JSON.stringify({});
				localStorage["currentIndex"] = 0;
			}
		}
	}
}

</script>
</head>
<body onload="updateChoice();update()">

<div id="container">
<div id="current">

<div id="weekday">Friday</div>
<div id="progress"></div>

<p id="current-title">

</p>

<details>
<summary>abstract & pdf</summary>
<p id="current-abstract">

</p>
</details>

</div>
<table>
<tr>
<td><button onclick="prev()">
<
</button></td><td><button onclick="next()">
>
</button></td>
</tr>
<tr>
<td><button onclick="unchoose()">
&diams;
</button></td><td><button onclick="choose()">
&hearts;
</button></td>
</tr>
<tr>
<td><button onclick="clubs()">
&clubs;
</button></td><td><button onclick="spades()">
&spades;
</button></td>
</tr>

<tr><td><button onclick="listA()">{&clubs;,&hearts;}</button></td><td><button onclick="listB()">{&spades;,&hearts;}</button></td></tr>

</table>

</div>

<div id="viewer">

</div>

<div style="visibility:hidden; display: none;" id="titles">Star Formation Quenching Timescale of Central Galaxies in a Hierarchical Universe.{{{{ARTICLE_PARSER}}}}Stellar clusterings around \"Isolated\" Massive YSOs in the LMC.{{{{ARTICLE_PARSER}}}}Extending Semi-numeric Reionisation Models to the First Stars and Galaxies.{{{{ARTICLE_PARSER}}}}Spherical Harmonic Analyses of Intensity Mapping Power Spectra.{{{{ARTICLE_PARSER}}}}The Most Massive Galaxies and Black Holes Allowed by $\Lambda$CDM.{{{{ARTICLE_PARSER}}}}The Long Term Evolution of ASASSN-14li.{{{{ARTICLE_PARSER}}}}Are Cosmological Gas Accretion Streams Multiphase and Turbulent?.{{{{ARTICLE_PARSER}}}}Star formation triggered by galaxy interactions in modified gravity.{{{{ARTICLE_PARSER}}}}The Razor's Edge of Collapse: The Transition Point from Lognormal to Powerlaw in Molecular Cloud PDFs.{{{{ARTICLE_PARSER}}}}On-sky single-mode fiber coupling measurements at the Large Binocular Telescope.{{{{ARTICLE_PARSER}}}}The iLocater cryostat: design and thermal control strategy for precision radial velocity measurements.{{{{ARTICLE_PARSER}}}}iLocater: A Diffraction-limited Doppler Spectrometer for the Large Binocular Telescope.{{{{ARTICLE_PARSER}}}}Three-dimensional distribution of ejecta in Supernova 1987A at 10 000 days.{{{{ARTICLE_PARSER}}}}No hydrogen exosphere detected around the super-Earth HD97658 b.{{{{ARTICLE_PARSER}}}}The Close AGN Reference Survey (CARS) - Mrk 1018 returns to the shadows after 30 years as a Seyfert 1.{{{{ARTICLE_PARSER}}}}The Close AGN Reference Survey (CARS) - What is causing Mrk1018's return to the shadows after 30 years?.{{{{ARTICLE_PARSER}}}}A Panchromatic View of Relativistic Jets in Narrow-Line Seyfert 1 Galaxies.{{{{ARTICLE_PARSER}}}}SPIRITS 15c and SPIRITS 14buu: Two Obscured Supernovae in the Nearby Star-Forming Galaxy IC 2163.{{{{ARTICLE_PARSER}}}}Search for Ultra-relativistic Magnetic Monopoles with the Pierre Auger Observatory.{{{{ARTICLE_PARSER}}}}Fragmentation Kinematics in Comet 332P/Ikeya-Murakami.{{{{ARTICLE_PARSER}}}}The Final Fates of Accreting Supermassive Stars.{{{{ARTICLE_PARSER}}}}Do individual Spitzer young stellar object candidates enclose multiple UKIDSS sources?.{{{{ARTICLE_PARSER}}}}Early observations of the nearby type Ia supernova SN 2015F.{{{{ARTICLE_PARSER}}}}The Type Ia Supernova Color-Magnitude Relation and Host Galaxy Dust: A Simple Hierarchical Bayesian Model.{{{{ARTICLE_PARSER}}}}Polarimetric studies of magnetic turbulence with interferometer.{{{{ARTICLE_PARSER}}}}Perturbations and quantum relaxation.{{{{ARTICLE_PARSER}}}}DDO216-A1: a central globular cluster in a low-luminosity transition type galaxy.{{{{ARTICLE_PARSER}}}}Detecting Sub-lunar Mass Compact Objects toward the Local Group Galaxies.{{{{ARTICLE_PARSER}}}}On the relationship between circumstellar disc size and X-ray outbursts in Be/X-ray binaries.{{{{ARTICLE_PARSER}}}}Anomalous spectral lines and relic quantum nonequilibrium.{{{{ARTICLE_PARSER}}}}Stable clustering and the resolution of dissipationless cosmological N-body simulations.{{{{ARTICLE_PARSER}}}}Time-dependent injection as a model for rapid blazar flares.{{{{ARTICLE_PARSER}}}}Stronger Reflection from Black Hole Accretion Disks in Soft X-ray States.{{{{ARTICLE_PARSER}}}}A method to deconvolve stellar rotational velocities II.{{{{ARTICLE_PARSER}}}}An analytical solution in the complex plane for the luminosity distance in flat cosmology.{{{{ARTICLE_PARSER}}}}Bright hot impacts by erupted fragments falling back on the Sun: magnetic channelling.{{{{ARTICLE_PARSER}}}}ET Probes: Looking Here as Well as There.{{{{ARTICLE_PARSER}}}}H$_\alpha$-activity and ages for stars in the SARG survey.{{{{ARTICLE_PARSER}}}}Direct Search for keV Sterile Neutrino Dark Matter with a Stable Dysprosium Target.{{{{ARTICLE_PARSER}}}}A Very Bright, Very Hot, and Very Long Flaring Event from the M Dwarf Binary System DG CVn.{{{{ARTICLE_PARSER}}}}The Pan-STARRS 1 Discoveries of five new Neptune Trojans.{{{{ARTICLE_PARSER}}}}Infrared complex refractive index of astrophysical ices exposed to cosmic rays simulated in the laboratory.{{{{ARTICLE_PARSER}}}}On the Lack of a Radio Afterglow from Some Gamma-ray Bursts - Insight into Their Progenitors?.{{{{ARTICLE_PARSER}}}}Oscillations Beyond Three-Neutrino Mixing.{{{{ARTICLE_PARSER}}}}Altitudinal dependence of meteor radio afterglows measured via optical counterparts.{{{{ARTICLE_PARSER}}}}Nonthermal Gravitino Production after Large Field Inflation.{{{{ARTICLE_PARSER}}}}Black hole formation from axion stars.{{{{ARTICLE_PARSER}}}}The Bok Globule BHR 160: structure and star formation.{{{{ARTICLE_PARSER}}}}On the Inference of the Cosmic Ray Ionization Rate $\zeta$ from the HCO$^+$-to-DCO$^+$ Abundance Ratio: The Effect of Nuclear Spin.{{{{ARTICLE_PARSER}}}}Shortcomings of New Parametrizations of Inflation.{{{{ARTICLE_PARSER}}}}Long-term stability and temperature variability of Iris AO segmented MEMS deformable mirrors.{{{{ARTICLE_PARSER}}}}Coronal type III radio bursts and their X-ray flare and interplanetary type III counterparts.{{{{ARTICLE_PARSER}}}}Highly siderophile elements were stripped from Earth's mantle by iron sulfide segregation.{{{{ARTICLE_PARSER}}}}The Post-Starburst Evolution of Tidal Disruption Event Host Galaxies.{{{{ARTICLE_PARSER}}}}Dynamo Sensitivity in Solar Analogs with 50 Years of Ca II H & K Activity.{{{{ARTICLE_PARSER}}}}Two-point correlators revisited: Fast and slow scales in multifield models of Inflation.{{{{ARTICLE_PARSER}}}}A Search For X-ray Emission From Colliding Magnetospheres In Young Eccentric Stellar Binaries.{{{{ARTICLE_PARSER}}}}Effect of surface-mantle water exchange parameterizations on exoplanet ocean depths.{{{{ARTICLE_PARSER}}}}Complete Reionization Constraints from Planck 2015 Polarization.{{{{ARTICLE_PARSER}}}}Super-Earths as Failed Cores in Orbital Migration Traps.{{{{ARTICLE_PARSER}}}}Cloud Atlas: Discovery of Patchy Clouds and High-amplitude Rotational Modulations In a Young, Extremely Red L-type Brown Dwarf.{{{{ARTICLE_PARSER}}}}The quenching time scale and quenching rate of galaxies.{{{{ARTICLE_PARSER}}}}NuSTAR observations of WISE J1036+0449, a Galaxy at z$\sim1$ obscured by hot dust.{{{{ARTICLE_PARSER}}}}N-Body Simulations of Collective Effects in Spiral and Barred Galaxies.{{{{ARTICLE_PARSER}}}}What determines large scale galaxy clustering: halo mass or local density?.{{{{ARTICLE_PARSER}}}}A Deep Search For Faint Galaxies Associated With Very Low-redshift C IV Absorbers: III. The Mass- and Environment-dependent Circumgalactic Medium.{{{{ARTICLE_PARSER}}}}Spherical and nonspherical models of primordial black hole formation: exact solutions.{{{{ARTICLE_PARSER}}}}Giant Clumps in Simulated High-z Galaxies: Properties, Evolution and Dependence on Feedback.{{{{ARTICLE_PARSER}}}}Dark matter component decaying after recombination: lensing constraints with Planck data.{{{{ARTICLE_PARSER}}}}Information Gain on Reheating: the One Bit Milestone.{{{{ARTICLE_PARSER}}}}Exorcising the Ostrogradsky ghost in coupled systems.{{{{ARTICLE_PARSER}}}}Robust Likelihoods for Inflationary Gravitational Waves from Maps of Cosmic Microwave Background Polarization.{{{{ARTICLE_PARSER}}}}Hello Darkness My Old Friend: The Fading of the Nearby TDE ASASSN-14ae.{{{{ARTICLE_PARSER}}}}Gravitational Particle Production in Oscillating Background and Its Cosmological Implications.{{{{ARTICLE_PARSER}}}}Dependence of Small Planet Frequency on Stellar Metallicity Hidden by Their Prevalence.{{{{ARTICLE_PARSER}}}}Constraint on Matter Power Spectrum on $10^6-10^9M_\odot$ Scales from ${\large\tau_e}$.{{{{ARTICLE_PARSER}}}}From GLC to double-null coordinates and illustration with static black holes.{{{{ARTICLE_PARSER}}}}Evolution of an Accretion Disk in Binary Black Hole Systems.{{{{ARTICLE_PARSER}}}}ALMA Reveals Weak [NII] Emission in \"Typical\" Galaxies and Intense Starbursts at z=5-6.{{{{ARTICLE_PARSER}}}}Magnetar-like X-ray Bursts from a Rotation Powered Pulsar, PSR J1119-6127.{{{{ARTICLE_PARSER}}}}Ultimate precision in cosmic-ray radio detection --- the SKA.{{{{ARTICLE_PARSER}}}}The solar chromosphere as induction disk and the inverse Joule-Thomson effect.{{{{ARTICLE_PARSER}}}}Space Development and Space Science Together, an Historic Opportunity.{{{{ARTICLE_PARSER}}}}Nonlinear regimes in mean-field full-sphere dynamo.{{{{ARTICLE_PARSER}}}}Large X-ray Flares on Stars Detected with MAXI/GSC: A Universal Correlation between the Duration of a Flare and its X-ray Luminosity.{{{{ARTICLE_PARSER}}}}Measurement of the Muon Production Depths at the Pierre Auger Observatory.{{{{ARTICLE_PARSER}}}}On gravitational waves from classical three body problem.{{{{ARTICLE_PARSER}}}}Accurate, Empirical Radii and Masses of Planets with Gaia Parallaxes.{{{{ARTICLE_PARSER}}}}First Gaia Local Group Dynamics: Magellanic Clouds Proper Motion and Rotation.{{{{ARTICLE_PARSER}}}}The MASSIVE Survey - V. Spatially-Resolved Stellar Angular Momentum, Velocity Dispersion, and Higher Moments of the 41 Most Massive Local Early-Type Galaxies.{{{{ARTICLE_PARSER}}}}PSR B0329+54: Substructure in the scatter-broadened image discovered with RadioAstron on baselines up to 330,000 km.{{{{ARTICLE_PARSER}}}}Anisotropic flow measurements in Pb--Pb collisions at 5.02 TeV with ALICE.{{{{ARTICLE_PARSER}}}}Measurement of the omega -> pi^0 e^+e^- and eta -> e^+e^-g Dalitz decays with the A2 setup at MAMI.{{{{ARTICLE_PARSER}}}}MESON2016 -- Concluding Remarks.{{{{ARTICLE_PARSER}}}}Review of the electric dipole moment of light nuclei.{{{{ARTICLE_PARSER}}}}Nonperturbative-transverse-momentum effects and evolution in dihadron and direct photon-hadron angular correlations in $p$$+$$p$ collisions at $\sqrt{s}$=510 GeV.{{{{ARTICLE_PARSER}}}}Three pion nucleon coupling constants.{{{{ARTICLE_PARSER}}}}Characterization and performances of DOSION, a dosimetry equipment dedicated to radiobiology experiments taking place at GANIL.{{{{ARTICLE_PARSER}}}}Extraction of the proton charge radius from experiments.{{{{ARTICLE_PARSER}}}}Anisotropic flow measurements in Pb--Pb collisions at 5.02 TeV with ALICE.{{{{ARTICLE_PARSER}}}}Momentum distribution of N$^*$ in nuclei.{{{{ARTICLE_PARSER}}}}Chiral medium produced by parallel electric and magnetic fields.{{{{ARTICLE_PARSER}}}}MESON2016 -- Concluding Remarks.{{{{ARTICLE_PARSER}}}}Model prediction for temperature dependence of meson pole masses from lattice QCD results on meson screening masses.{{{{ARTICLE_PARSER}}}}Strong Couplings of Three Mesons with Charm(ing) Involvement.{{{{ARTICLE_PARSER}}}}Review of the electric dipole moment of light nuclei.{{{{ARTICLE_PARSER}}}}Hydrodynamization and transient modes of expanding plasma in kinetic theory.{{{{ARTICLE_PARSER}}}}The $\eta^\prime$ transition form factor from space- and time-like experimental data.{{{{ARTICLE_PARSER}}}}A Lagrangian formulation of relativistic Israel-Stewart hydrodynamics.{{{{ARTICLE_PARSER}}}}Three pion nucleon coupling constants.{{{{ARTICLE_PARSER}}}}On completeness of coherent states in noncommutative spaces with generalised uncertainty principle.{{{{ARTICLE_PARSER}}}}Minkowski measurability criteria for compact sets and relative fractal drums in Euclidean spaces.{{{{ARTICLE_PARSER}}}}The Fermionic Signature Operator and Hadamard States in the Presence of a Plane Electromagnetic Wave.{{{{ARTICLE_PARSER}}}}Near integrability of kink lattice with higher order interactions.{{{{ARTICLE_PARSER}}}}Random motions with space-varying velocities.{{{{ARTICLE_PARSER}}}}Discrete Dirac-K\\"ahler equation and its formulation in algebraic form.{{{{ARTICLE_PARSER}}}}Relative energy gap for harmonic maps of Riemann surfaces into real analytic Riemannian manifolds.{{{{ARTICLE_PARSER}}}}Hartree-Fock Corrections in a Mean-field Limit for Fermions.{{{{ARTICLE_PARSER}}}}Non-simple SLE curves are not determined by their range.{{{{ARTICLE_PARSER}}}}Strong converse exponent for classical-quantum channel coding.{{{{ARTICLE_PARSER}}}}A phase field approach for optimal boundary control of damage processes in two-dimensional viscoelastic media.{{{{ARTICLE_PARSER}}}}Complex dimensions of fractals and meromorphic extensions of fractal zeta functions.{{{{ARTICLE_PARSER}}}}Formal integrals and Noether operators of nonlinear hyperbolic partial differential systems admitting a rich set of symmetries.{{{{ARTICLE_PARSER}}}}The Kontsevich tetrahedral flows revisited.{{{{ARTICLE_PARSER}}}}Exact Baker-Campbell-Hausdorff formula for the contact Heisenberg algebra.{{{{ARTICLE_PARSER}}}}Random Matrix Ensembles with Split Limiting Behavior.{{{{ARTICLE_PARSER}}}}Quasilocal charges and the complete GGE for field theories with non-diagonal scattering.{{{{ARTICLE_PARSER}}}}Unbounded memory advantage in stochastic simulation using quantum mechanics.{{{{ARTICLE_PARSER}}}}Scaling of tripartite entanglement at impurity quantum phase transitions.{{{{ARTICLE_PARSER}}}}Connecting the quantum and classical worlds.{{{{ARTICLE_PARSER}}}}On completeness of coherent states in noncommutative spaces with generalised uncertainty principle.{{{{ARTICLE_PARSER}}}}Perturbations and quantum relaxation.{{{{ARTICLE_PARSER}}}}Polarization- and frequency-tunable microwave circuit for selective excitation of nitrogen-vacancy spins in diamond.{{{{ARTICLE_PARSER}}}}Optimized pulses for Raman excitation through the continuum: verification using multi-configurational time-dependent Hartree-Fock.{{{{ARTICLE_PARSER}}}}Geometric control theory for quantum back-action evasion.{{{{ARTICLE_PARSER}}}}Direct identification of dilute surface spins on Al$_2$O$_3$: Origin of flux noise in quantum circuits.{{{{ARTICLE_PARSER}}}}Anomalous spectral lines and relic quantum nonequilibrium.{{{{ARTICLE_PARSER}}}}Quantum synchronization in disordered superconducting metamaterials.{{{{ARTICLE_PARSER}}}}From Path Integrals to Tensor Networks for AdS/CFT.{{{{ARTICLE_PARSER}}}}Trade-off between speed and cost in shortcuts to adiabaticity.{{{{ARTICLE_PARSER}}}}Counter-propagating spontaneous four wave mixing: photon-pair factorability and ultra-narrowband single photon.{{{{ARTICLE_PARSER}}}}Poking holes and cutting corners to achieve Clifford gates with the surface code.{{{{ARTICLE_PARSER}}}}Dimensional reduction in Bose-Einstein condensed clouds of atoms confined in tight potentials of any geometry and any interaction strength.{{{{ARTICLE_PARSER}}}}Adaptive phase estimation with two-mode squeezed-vacuum and parity measurement.{{{{ARTICLE_PARSER}}}}Excitation of surface plasmon polariton modes with multiple nitrogen vacancy centers in single nanodiamonds.{{{{ARTICLE_PARSER}}}}Private states, quantum data hiding and the swapping of perfect secrecy.{{{{ARTICLE_PARSER}}}}Local excitation of surface plasmon polaritons using nitrogen-vacancy centers.{{{{ARTICLE_PARSER}}}}Chance in the Everett interpretation.{{{{ARTICLE_PARSER}}}}Hartree-Fock Corrections in a Mean-field Limit for Fermions.{{{{ARTICLE_PARSER}}}}Effects of Interactions on Bose-Einstein Condensation.{{{{ARTICLE_PARSER}}}}Cabello's nonlocality for generalized three-qubit GHZ states.{{{{ARTICLE_PARSER}}}}Signatures of composite bosons stepping off the Fock-state ladder.{{{{ARTICLE_PARSER}}}}Synthetic dimensions and spin-orbit coupling with an optical clock transition.{{{{ARTICLE_PARSER}}}}Conceptual aspects of geometric quantum computation.{{{{ARTICLE_PARSER}}}}Strong converse exponent for classical-quantum channel coding.{{{{ARTICLE_PARSER}}}}Sparse Quantum Codes from Quantum Circuits.{{{{ARTICLE_PARSER}}}}Using a biased qubit to probe complex systems.{{{{ARTICLE_PARSER}}}}Synchronizing quantum and classical clocks made of quantum particles.{{{{ARTICLE_PARSER}}}}Estimating the coherence of noise in quantum control of a solid-state qubit.{{{{ARTICLE_PARSER}}}}Interband heating processes in a periodically driven optical lattice.{{{{ARTICLE_PARSER}}}}Increased coherence time in narrowed bath states in quantum dots.{{{{ARTICLE_PARSER}}}}Work distribution in a photonic system.{{{{ARTICLE_PARSER}}}}Ultimate precision bound of quantum and sub-wavelength imaging.{{{{ARTICLE_PARSER}}}}On the Measurement Problem and the EPR Paradox.{{{{ARTICLE_PARSER}}}}Quantum tunneling from paths in complex time.{{{{ARTICLE_PARSER}}}}Creating cat states in one-dimensional quantum walks using delocalized initial states.{{{{ARTICLE_PARSER}}}}Violation of classical physics by a mesoscopic system.{{{{ARTICLE_PARSER}}}}Finite-key analysis for time-energy high-dimensional quantum key distribution.{{{{ARTICLE_PARSER}}}}Information Content of Gravitational Radiation and the Vacuum.{{{{ARTICLE_PARSER}}}}Variational tensor network renormalization in imaginary time: benchmark results in the Hubbard model at finite temperature.{{{{ARTICLE_PARSER}}}}Almost quantum correlations in tripartite Bell scenarios.{{{{ARTICLE_PARSER}}}}Quantum-statistical approach to electromagnetic wave propagation and dissipation inside dielectric media and nanophotonic and plasmonic waveguides.{{{{ARTICLE_PARSER}}}}SpaceTime from Hilbert Space: Decompositions of Hilbert Space as Instances of Time.{{{{ARTICLE_PARSER}}}}Exact Baker-Campbell-Hausdorff formula for the contact Heisenberg algebra.{{{{ARTICLE_PARSER}}}}Quantum lattice gas algorithmic representation of gauge field theory.{{{{ARTICLE_PARSER}}}}Constructing UMEB from maximally entangled basis.{{{{ARTICLE_PARSER}}}}An Adaptive Psychoacoustic Model for Automatic Speech Recognition.{{{{ARTICLE_PARSER}}}}Bayesian Reinforcement Learning: A Survey.{{{{ARTICLE_PARSER}}}}Unrestricted State Complexity of Binary Operations on Regular and Ideal Languages.{{{{ARTICLE_PARSER}}}}On the Computational Complexity of Minimal Cumulative Cost Graph Pebbling.{{{{ARTICLE_PARSER}}}}A Large Contextual Dataset for Classification, Detection and Counting of Cars with Deep Learning.{{{{ARTICLE_PARSER}}}}Sampling Generative Networks: Notes on a Few Effective Techniques.{{{{ARTICLE_PARSER}}}}Sort Race.{{{{ARTICLE_PARSER}}}}Parallel Dynamics Computation using Prefix Sum Operations.{{{{ARTICLE_PARSER}}}}Tsallis Regularized Optimal Transport and Ecological Inference.{{{{ARTICLE_PARSER}}}}On the Security of Millimeter Wave Vehicular Communication Systems using Random Antenna Subsets.{{{{ARTICLE_PARSER}}}}cesium: Open-Source Platform for Time-Series Inference.{{{{ARTICLE_PARSER}}}}Column Networks for Collective Classification.{{{{ARTICLE_PARSER}}}}Scheduling Autonomous Vehicle Platoons Through an Unregulated Intersection.{{{{ARTICLE_PARSER}}}}Function-Based Access Control (FBAC): From Access Control Matrix to Access Control Tensor.{{{{ARTICLE_PARSER}}}}C-Share: Optical Circuits Sharing for Software-Defined Data-Centers.{{{{ARTICLE_PARSER}}}}Distributed power allocation for D2D communications underlaying/overlaying OFDMA cellular networks.{{{{ARTICLE_PARSER}}}}Channel Estimation and Data Equalization in Frequency-Selective MIMO Systems with One-Bit Quantization.{{{{ARTICLE_PARSER}}}}Matrix Product State for Higher-Order Tensor Compression and Classification.{{{{ARTICLE_PARSER}}}}Structural Bounds on the Dyadic Effect.{{{{ARTICLE_PARSER}}}}A Multi-Service Oriented Multiple-Access Scheme For M2M Support in Future LTE.{{{{ARTICLE_PARSER}}}}SCTP User Message Interleaving Integration and Validation.{{{{ARTICLE_PARSER}}}}Enabling Soft Vertical Handover for MIPv6 in OMNeT++.{{{{ARTICLE_PARSER}}}}Performance and Security Evaluation of SDN Networks in OMNeT++/INET.{{{{ARTICLE_PARSER}}}}Resource Selection for Federated Search on the Web.{{{{ARTICLE_PARSER}}}}Structured Dropout for Weak Label and Multi-Instance Learning and Its Application to Score-Informed Source Separation.{{{{ARTICLE_PARSER}}}}Reducing individuals' risk sensitiveness can promote positive and non-alarmist views about catastrophic events in an agent-based simulation.{{{{ARTICLE_PARSER}}}}A parallel pattern for iterative stencil + reduce.{{{{ARTICLE_PARSER}}}}A Novel HW/SW Based NoC Router Self-Testing Methodology.{{{{ARTICLE_PARSER}}}}On Memory Footprints of Partitioned Sparse Matrices.{{{{ARTICLE_PARSER}}}}On the Equivalency of Reliability and Security Metrics for Wireline Networks.{{{{ARTICLE_PARSER}}}}Minimum Eccentricity Shortest Path Problem: an Approximation Algorithm and Relation with the k-Laminarity Problem.{{{{ARTICLE_PARSER}}}}When to make a step? Tackling the timing problem in multi-contact locomotion by TOPP-MPC.{{{{ARTICLE_PARSER}}}}New MDS or near MDS self-dual codes over finite fields.{{{{ARTICLE_PARSER}}}}Automating Large-Scale Simulation and Data Analysis with OMNeT++: Lession Learned and Future Perspectives.{{{{ARTICLE_PARSER}}}}WiFi-Direct Simulation for INET in OMNeT++.{{{{ARTICLE_PARSER}}}}C-Planarity of Overlapping Clusterings Including Unions of Two Partitions.{{{{ARTICLE_PARSER}}}}Recursive nearest agglomeration (ReNA): fast clustering for approximation of structured signals.{{{{ARTICLE_PARSER}}}}From H&M to Gap for Lightweight BWT Merging.{{{{ARTICLE_PARSER}}}}Factored Neural Machine Translation.{{{{ARTICLE_PARSER}}}}Distributed Estimation of the Operating State of a Single-Bus DC MicroGrid without an External Communication Interface.{{{{ARTICLE_PARSER}}}}Context Aware Nonnegative Matrix Factorization Clustering.{{{{ARTICLE_PARSER}}}}Institutionalization in Efficient Markets: The Case of Price Bubbles.{{{{ARTICLE_PARSER}}}}Sequencing Chess.{{{{ARTICLE_PARSER}}}}Conformity in virtual environments: a hybrid neurophysiological and psychosocial approach.{{{{ARTICLE_PARSER}}}}Lost and Found: Detecting Small Road Hazards for Self-Driving Vehicles.{{{{ARTICLE_PARSER}}}}Collective Awareness Platforms and Digital Social Innovation Mediating Consensus Seeking in Problem Situations.{{{{ARTICLE_PARSER}}}}Results of a Collective Awareness Platforms Investigation.{{{{ARTICLE_PARSER}}}}Non-trivial reputation effects on social decision making in virtual environment.{{{{ARTICLE_PARSER}}}}Denoising Message Passing for X-ray Computed Tomography Reconstruction.{{{{ARTICLE_PARSER}}}}Collective Intelligence Heuristic: An Experimental Evidence.{{{{ARTICLE_PARSER}}}}Passivity-Based Distributed Optimization with Communication Delays Using PI Consensus Algorithm.{{{{ARTICLE_PARSER}}}}War-Algorithm Accountability.{{{{ARTICLE_PARSER}}}}Maximal Repetition and Zero Entropy Rate.{{{{ARTICLE_PARSER}}}}MALL proof nets identify proofs modulo rule commutation.{{{{ARTICLE_PARSER}}}}Improving the Accuracy of Stereo Visual Odometry Using Visual Illumination Estimation.{{{{ARTICLE_PARSER}}}}Glassbox: Dynamic Analysis Platform for Malware Android Applications on Real Devices.{{{{ARTICLE_PARSER}}}}Concordance and the Smallest Covering Set of Preference Orderings.{{{{ARTICLE_PARSER}}}}K-Medoids For K-Means Seeding.{{{{ARTICLE_PARSER}}}}Controllability Gramian Spectra of Random Networks.{{{{ARTICLE_PARSER}}}}The Robotarium: A remotely accessible swarm robotics research testbed.{{{{ARTICLE_PARSER}}}}RALL - Routing-Aware Of Path Length, Link Quality, And Traffic Load For Wireless Sensor Networks.{{{{ARTICLE_PARSER}}}}Joint Attention in Autonomous Driving (JAAD).{{{{ARTICLE_PARSER}}}}A Portable, 3D-Printing Enabled Multi-Vehicle Platform for Robotics Research and Education.{{{{ARTICLE_PARSER}}}}An overview of gradient descent optimization algorithms.{{{{ARTICLE_PARSER}}}}Perceptual Quality Prediction on Authentically Distorted Images Using a Bag of Features Approach.{{{{ARTICLE_PARSER}}}}Transport-based analysis, modeling, and learning from signal and data distributions.{{{{ARTICLE_PARSER}}}}Characterizing the Language of Online Communities and its Relation to Community Reception.{{{{ARTICLE_PARSER}}}}Confining Windows Inter-Process Communications for OS-Level Virtual Machine.{{{{ARTICLE_PARSER}}}}Mechanism Design with Exchangeable Allocations.{{{{ARTICLE_PARSER}}}}Virtualizing System and Ordinary Services in Windows-based OS-Level Virtual Machines.{{{{ARTICLE_PARSER}}}}Coherence Pursuit: Fast, Simple, and Robust Principal Component Analysis.{{{{ARTICLE_PARSER}}}}Professional and Citizen Bibliometrics: Complementarities and ambivalences in the development and use of indicators.{{{{ARTICLE_PARSER}}}}Semantics for UGV Registration in GPS-denied Environments.{{{{ARTICLE_PARSER}}}}Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.{{{{ARTICLE_PARSER}}}}The multi-vehicle covering tour problem: building routes for urban patrolling.{{{{ARTICLE_PARSER}}}}Cooperation and Competition when Bidding for Complex Projects: Centralized and Decentralized Perspectives.{{{{ARTICLE_PARSER}}}}Learning from networked examples.{{{{ARTICLE_PARSER}}}}On MultiAspect Graphs.{{{{ARTICLE_PARSER}}}}Strong converse exponent for classical-quantum channel coding.{{{{ARTICLE_PARSER}}}}Sparse Quantum Codes from Quantum Circuits.{{{{ARTICLE_PARSER}}}}Random Forest for the Contextual Bandit Problem - extended version.{{{{ARTICLE_PARSER}}}}ORFEL: efficient detection of defamation or illegitimate promotion in online recommendation.{{{{ARTICLE_PARSER}}}}M-Flash: Fast Billion-scale Graph Computation Using a Bimodal Block Processing Model.{{{{ARTICLE_PARSER}}}}Top-k Query Processing on Encrypted Databases with Strong Security Guarantees.{{{{ARTICLE_PARSER}}}}Post-transcriptional knowledge in pathway analysis increases the accuracy of phenotypes classification.{{{{ARTICLE_PARSER}}}}False Discoveries Occur Early on the Lasso Path.{{{{ARTICLE_PARSER}}}}UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking.{{{{ARTICLE_PARSER}}}}Ramsey-type theorems for lines in 3-space.{{{{ARTICLE_PARSER}}}}Preconditioned Stochastic Gradient Descent.{{{{ARTICLE_PARSER}}}}A Novel Reduced Model for Electrical Networks with Constant Power Loads.{{{{ARTICLE_PARSER}}}}Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic BP, and the information-computation gap.{{{{ARTICLE_PARSER}}}}Tensor Sparse and Low-Rank based Submodule Clustering Method for Multi-way Data.{{{{ARTICLE_PARSER}}}}Speech vocoding for laboratory phonology.{{{{ARTICLE_PARSER}}}}Spoofing Detection Goes Noisy: An Analysis of Synthetic Speech Detection in the Presence of Additive Noise.{{{{ARTICLE_PARSER}}}}ARX modeling of unstable linear systems.{{{{ARTICLE_PARSER}}}}Information Utilization Ratio in Heuristic Optimization Algorithms.{{{{ARTICLE_PARSER}}}}A Discrete and Bounded Envy-Free Cake Cutting Protocol for Any Number of Agents.{{{{ARTICLE_PARSER}}}}Low-Rank Matrix Recovery using Gabidulin Codes in Characteristic Zero.{{{{ARTICLE_PARSER}}}}Decoding Interleaved Gabidulin Codes using Alekhnovich's Algorithm.{{{{ARTICLE_PARSER}}}}Semi-Supervised Representation Learning based on Probabilistic Labeling.{{{{ARTICLE_PARSER}}}}Space-Time Codes Based on Rank-Metric Codes and Their Decoding.{{{{ARTICLE_PARSER}}}}The State of the Art in Cartograms.{{{{ARTICLE_PARSER}}}}Randomized Ternary Search Tries.{{{{ARTICLE_PARSER}}}}CLEAR: Covariant LEAst-square Re-fitting with applications to image restoration.{{{{ARTICLE_PARSER}}}}Passive Channel Gain Estimation Between Primary Transceivers in Cognitive Radio Networks.{{{{ARTICLE_PARSER}}}}Alternating Back-Propagation for Generator Network.{{{{ARTICLE_PARSER}}}}Trip-Based Public Transit Routing Using Condensed Search Trees.{{{{ARTICLE_PARSER}}}}Improved Approximation for Weighted Tree Augmentation with Bounded Costs.{{{{ARTICLE_PARSER}}}}PicHunt: Social Media Image Retrieval for Improved Law Enforcement.{{{{ARTICLE_PARSER}}}}Linear-time Kernelization for Feedback Vertex Set.{{{{ARTICLE_PARSER}}}}Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation Algorithm.{{{{ARTICLE_PARSER}}}}Pruning Filters for Efficient ConvNets.{{{{ARTICLE_PARSER}}}}Ten Steps of EM Suffice for Mixtures of Two Gaussians.{{{{ARTICLE_PARSER}}}}Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model.{{{{ARTICLE_PARSER}}}}Consensus of Dependent Opinions.{{{{ARTICLE_PARSER}}}}Congestion-Aware Randomized Routing in Autonomous Mobility-on-Demand Systems.{{{{ARTICLE_PARSER}}}}Blending Entropy: A Term for Addressing Information Density in Mediated Reality.{{{{ARTICLE_PARSER}}}}A Bayesian implementable social choice function may not be truthfully implementable.{{{{ARTICLE_PARSER}}}}A joint-optimization NSAF algorithm based on the first-order Markov model.{{{{ARTICLE_PARSER}}}}Removal Lemmas for Matrices.{{{{ARTICLE_PARSER}}}}Warped Convolutions: Efficient Invariance to Spatial Transformations.{{{{ARTICLE_PARSER}}}}</div>
<div style="visibility:hidden; display: none;" id="abstracts"><p>Central galaxies make up the majority of the galaxy population, including the
majority of the quiescent population at $\mathcal{M}_* &gt;
10^{10}\mathrm{M}_\odot$. Thus, the mechanism(s) responsible for quenching
central galaxies plays a crucial role in galaxy evolution as whole. We combine
a high resolution cosmological $N$-body simulation with observed evolutionary
trends of the \"star formation main sequence,\" quiescent fraction, and stellar
mass function at $z &lt; 1$ to construct a model that statistically tracks the
star formation histories and quenching of central galaxies. Comparing this
model to the distribution of central galaxy star formation rates in a group
catalog of the SDSS Data Release 7, we constrain the timescales over which
physical processes cease star formation in central galaxies. Over the stellar
mass range $10^{9.5}$ to $10^{11} \mathrm{M}_\odot$ we infer quenching
e-folding times that span $1.5$ to $0.5\; \mathrm{Gyr}$ with more massive
central galaxies quenching faster. For $\mathcal{M}_* =
10^{10.5}\mathrm{M}_\odot$, this implies a total migration time of $\sim
4~\mathrm{Gyrs}$ from the star formation main sequence to quiescence. Compared
to satellites, central galaxies take $\sim 2~\mathrm{Gyrs}$ longer to quench
their star formation, suggesting that different mechanisms are responsible for
quenching centrals versus satellites. Finally, the central galaxy quenching
timescale we infer provides key constraints for proposed star formation
quenching mechanisms. Our timescale is generally consistent with gas depletion
timescales predicted by quenching through strangulation. However, the exact
physical mechanism(s) responsible for this still remain unclear.
</p>
{{{{ARTICLE_PARSER}}}}<p>Observations suggest that there is a significant fraction of O-stars in the
field of the Milky Way that appear to have formed in isolation or in low mass
clusters ($&lt;$100 $M_\odot$). The existence of these high-mass stars that
apparently formed in the field challenges the generally accepted paradigm,
which requires star formation to occur in clustered environments. In order to
understand the physical conditions for the formation of these stars, it is
necessary to observe isolated high-mass stars while they are still forming.
With the $Hubble$ $Space$ $Telescope$, we observe the seven most isolated
massive ($&gt;$8 $M_\odot$) young stellar objects (MYSOs) in the Large Magellanic
Cloud (LMC). The observations show that while these MYSOs are remote from other
MYSOs, OB associations, and even from known giant molecular clouds, they are
actually not isolated at all. Imaging reveals $\sim$100 to several hundred
pre--main-sequence (PMS) stars in the vicinity of each MYSO. These previously
undetected PMS stars form prominent compact clusters around the MYSOs, and in
most cases they are also distributed sparsely across the observed regions.
Contrary to what previous high-mass field star studies show, these observations
suggest that high-mass stars may not be able to form in clusters with masses
less than 100 $M_\odot$. If these MYSOs are indeed the best candidates for
isolated high-mass star formation, then the lack of isolation is at odds with
random sampling of the IMF. Moreover, while isolated MYSOs may not exist, we
find evidence that isolated clusters containing O-stars can exist, which in
itself is rare.
</p>
{{{{ARTICLE_PARSER}}}}<p>Semi-numeric methods have made it possible to efficiently model the epoch of
reionisation (EoR). While most implementations involve a reduction to a simple
three-parameter model, we introduce a new mass-dependent ionising efficiency
parameter that folds in physical parameters that are constrained by the latest
numerical simulations. This new parameterization enables the effective modeling
of a broad range of host halo masses containing ionising sources, extending
from the smallest Population III host halos with $M \sim 10^6 M_\odot$, which
are often ignored, to the rarest cosmic peaks with $M \sim 10^{12} M_\odot$
during EoR. We compare the resulting ionising histories with a typical
three-parameter model and also compare with the latest constraints from the
Planck mission. Our model results in a optical depth due to Thomson scattering,
$\tau_{\mathrm{e}}$ = 0.057, that is consistent with Planck. The largest
difference in our model is shown in the resulting bubble size distributions
which peak at lower characteristic sizes and are broadened. We also consider
the uncertainties of the various physical parameters and comparing the
resulting ionising histories broadly disfavors a small contribution from
galaxies. As the smallest haloes cease a meaningful contribution to the
ionising photon budget after $z = 10$, implying they play a role in determining
the start of EoR and little else, an ionising photon escape fraction greater
than 0.6 is also broadly disfavored.
</p>
{{{{ARTICLE_PARSER}}}}<p>Intensity mapping is a promising technique for surveying the large scale
structure of our Universe from $z=0$ to $z \sim 150$, using the brightness
temperature field of spectral lines to directly observe previously unexplored
portions of out cosmic timeline. Examples of targeted lines include the
$21\,\textrm{cm}$ hyperfine transition of neutral hydrogen, rotational lines of
carbon monoxide, and fine structure lines of singly ionized carbon. Recent
efforts have focused on detections of the power spectrum of spatial
fluctuations, but have been hindered by systematics such as foreground
contamination. This has motivated the decomposition of data into Fourier modes
perpendicular and parallel to the line-of-sight, which has been shown to be a
particularly powerful way to diagnose systematics. However, such a method is
well-defined only in the limit of a narrow-field, flat-sky approximation. This
limits the sensitivity of intensity mapping experiments, as it means that wide
surveys must be separately analyzed as a patchwork of smaller fields. In this
paper, we develop a framework for analyzing intensity mapping data in a
spherical Fourier-Bessel basis, which incorporates curved sky effects without
difficulty. We use our framework to generalize a number of techniques in
intensity mapping data analysis from the flat sky to the curved sky. These
include visibility-based estimators for the power spectrum, treatments of
interloper lines, and the \"foreground wedge\" signature of spectrally smooth
foregrounds.
</p>
{{{{ARTICLE_PARSER}}}}<p>Given a galaxy's stellar mass, its host halo mass has a lower limit from the
cosmic baryon fraction and known baryonic physics. At $z&gt;4$, galaxy stellar
mass functions place lower limits on halo number densities that approach
expected $\Lambda$CDM halo mass functions. High-redshift galaxy stellar mass
functions can thus place interesting limits on number densities of massive
haloes, which are otherwise very difficult to measure. While halo mass
functions at $z&lt;8$ are consistent with observed galaxy stellar masses, JWST and
WFIRST will more than double the redshift range over which useful constraints
are available. We calculate galaxy stellar masses as a function of redshift
that, if they existed in sufficient numbers, would either require unusual
baryonic physics or in extreme cases would rule out $\Lambda$CDM entirely.
Extending the calculation to the entire observable Universe, we find that the
existence of a single $10^{11}$ M$_\odot$ galaxy at $z=13$ would rule out
Planck $\Lambda$CDM with &gt;95% confidence. Similar arguments apply to black
holes; using number density constraints alone, the most massive observed black
holes at $z&gt;5$ must lie above the median $z=0$ black hole mass - bulge mass
relation. If their virial mass estimates are accurate, the quasars SDSS
J1044$-$0125 and SDSS J010013.02$+$280225.8 must both have black hole mass -
stellar mass ratios of at least 2%, equaling the recent $z=0$ record for
central galaxies in NGC 1600.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present late-time optical spectroscopy taken with the Large Binocular
Telescope's Multi-Object Double Spectrograph, late-time SWIFT UVOT and XRT
observations, as well as improved ASAS-SN pre-discovery limits on the nearby
(d=90.3 Mpc, z=0.0206) tidal disruption event (TDE) ASASSN-14li. The late-time
optical spectra show H$\alpha$ emission well in excess of that seen in the SDSS
host galaxy spectrum, indicating that the processes powering the luminous
flares associated with TDEs can operate for several hundreds of days. The SWIFT
observations reveal the presence of lingering apparently thermal UV (T$_{\rm
UV} \sim 3.5\times10^4$ K) and X-ray (T$_{\rm X} \sim 7\times10^5$ K) emission.
The characteristic temperatures evolve by, at most, a factor of $\sim2$ over
the 600 day follow-up campaign. The X-ray, UV, and H$\alpha$ luminosities
evolve roughly in tandem and at a rate that is consistent with a power-law
decay at late times. This behavior is in stark contrast with the majority of
optically discovered TDEs, which are X-ray faint and evolve on shorter
timescales. Finally we address how the unique properties of ASASSN-14li can be
used to probe the relationship between the TDE rate and host galaxy properties.
</p>
{{{{ARTICLE_PARSER}}}}<p>Simulations of cosmological filamentary accretion streams into galactic halos
reveal that such flows are warm at T$\sim$10$^4$K, laminar, and provide high
gas accretion efficiency onto galaxies. We present a phenomenological scenario
which suggests that accretion flows are shocked, become thermally unstable,
biphasic, and are, as a result, turbulent. We consider a collimated stream of
warm gas over denser than the hot, virialized halo gas. The post-shock
streaming gas has a higher pressure than the ambient halo gas, expands, and is
thermally unstable and fragments, forming a two phase medium -- a hot phase
with an embedded warm cloudy phase. The thermodynamic evolution of the
post-shock gas is largely determined by the relative timescales of several
processes, namely the cooling, the expansion of the hot phase and turbulent
warm clouds, and the amount of turbulence in clouds, and the halo dynamics. The
cooling is moderated by mixing with the ambient halo gas and heating due to
turbulent dissipation. We consider the evolution of a stream for a single halo
mass, 10$^{13}$ M$_{\odot}$, and redshift, 2. We find that the gas becomes
thermally unstable and fragments into a two-phase medium where the cooler phase
is highly turbulent and has a lower bulk velocity than the initial stream. The
turbulent stream loses coherence in less than a halo dynamical time. Both the
phase separation and \"disruption\" of the stream imply that the accretion
efficiency onto a galaxy in a dynamical time may be less than in simulations
having laminar isothermal flows. De-collimating flows make the direct
interaction between galaxy feedback and accretion streams more likely, thereby
further reducing the overall accretion efficiency. Moderating the gas accretion
efficiency through these mechanisms may help to alleviate a number of
significant challenges in theoretical galaxy formation. [abridged]
</p>
{{{{ARTICLE_PARSER}}}}<p>Together with interstellar turbulence, gravitation is one key player in star
formation. It acts both at galactic scales in the assembly of gas into dense
clouds, and inside those structures for their collapse and the formation of
pre-stellar cores. To understand to what extent the large scale dynamics govern
the star formation activity of galaxies, we present hydrodynamical simulations
in which we generalise the behaviour of gravity to make it differ from
Newtonian dynamics in the low acceleration regime. We focus on the extreme
cases of interacting galaxies, and compare the evolution of galaxy pairs in the
dark matter paradigm to that in the Milgromian Dynamics (MOND) framework.
Following up on the seminal work by Tiret &amp; Combes, this paper documents the
first simulations of galaxy encounters in MOND with a detailed Eulerian
hydrodynamical treatment of baryonic physics, including star formation and
stellar feedback. We show that similar morphologies of the interacting systems
can be produced by both the dark matter and MOND formalisms, but require a much
slower orbital velocity in the MOND case. Furthermore, we find that the star
formation activity and history are significantly more extended in space and
time in MOND interactions, in particular in the tidal debris. Such differences
could be used as observational diagnostics and make interacting galaxies prime
objects in the study of the nature of gravitation at galactic scales.
</p>
{{{{ARTICLE_PARSER}}}}<p>We derive an analytic expression for the transitional column density value
($s_t$) between the lognormal and power-law form of the probability
distribution function (PDF) in star-forming molecular clouds. Our expression
for $s_t$ depends on the mean column density, the variance of the lognormal
portion of the PDF, and the slope of the power-law portion of the PDF. We show
that $s_t$ can be related to physical quantities such as the sonic Mach number
of the flow and the power-law index for a self-gravitating isothermal sphere.
This implies that the transition point between the lognormal and power-law
density/column density PDF represents the critical density where turbulent and
thermal pressure balance, the so-called \"post-shock density.\" We test our
analytic prediction for the transition column density using dust PDF
observations reported in the literature as well as numerical MHD simulations of
self-gravitating supersonic turbulence with the Enzo code. We find excellent
agreement between the analytic $s_t$ and the measured values from the numerical
simulations and observations (to within 1.5 A$_V$). We discuss the utility of
our expression for determining the properties of the PDF from unresolved low
density material in dust observations, for estimating the post-shock density,
and for determining the HI-H$_2$ transition in clouds.
</p>
{{{{ARTICLE_PARSER}}}}<p>The demonstration of efficient single-mode fiber (SMF) coupling is a key
requirement for the development of a compact, ultra-precise radial velocity
(RV) spectrograph. iLocater is a next generation instrument for the Large
Binocular Telescope (LBT) that uses adaptive optics (AO) to inject starlight
into a SMF. In preparation for commissioning iLocater, a prototype SMF
injection system was installed and tested at the LBT in the Y-band (0.970-1.065
$\mu$m). This system was designed to verify the capability of the LBT AO system
as well as characterize on-sky SMF coupling efficiencies. SMF coupling was
measured on stars with variable airmasses, apparent magnitudes, and seeing
conditions for six half-nights using the Large Binocular Telescope
Interferometer. We present the overall optical and mechanical performance of
the SMF injection system, including details of the installation and alignment
procedure. A particular emphasis is placed on analyzing the instrument's
performance as a function of telescope elevation to inform the final design of
the fiber injection system for iLocater.
</p>
{{{{ARTICLE_PARSER}}}}<p>The current generation of precision radial velocity (RV) spectrographs are
seeing-limited instruments. In order to achieve high spectral resolution on 8m
class telescopes, these spectrographs require large optics and in turn, large
instrument volumes. Achieving milli-Kelvin thermal stability for these systems
is challenging but is vital in order to obtain a single measurement RV
precision of better than 1m/s. This precision is crucial to study Earth-like
exoplanets within the habitable zone. iLocater is a next generation RV
instrument being developed for the Large Binocular Telescope. Unlike
seeing-limited RV instruments, iLocater uses adaptive optics (AO) to inject a
diffraction-limited beam into single-mode fibers. These fibers illuminate the
instrument spectrograph, facilitating a diffraction-limited design and a small
instrument volume compared to present-day instruments. This enables intrinsic
instrument stability and facilitates precision thermal control. We present the
current design of the iLocater cryostat which houses the instrument
spectrograph and the strategy for its thermal control. The spectrograph is
situated within a pair of radiation shields mounted inside an MLI lined vacuum
chamber. The outer radiation shield is actively controlled to maintain
instrument stability at the sub-mK level and minimize effects of thermal
changes from the external environment. An inner shield passively dampens any
residual temperature fluctuations and is radiatively coupled to the optical
board. To provide intrinsic stability, the optical board and optic mounts will
be made from Invar and cooled to 58K to benefit from a zero coefficient of
thermal expansion (CTE) value at this temperature. Combined, the small
footprint of the instrument spectrograph, the use of Invar, and precision
thermal control will allow long-term sub-milliKelvin stability to facilitate
precision RV measurements.
</p>
{{{{ARTICLE_PARSER}}}}<p>We are developing a stable and precise spectrograph for the Large Binocular
Telescope (LBT) named \"iLocater.\" The instrument comprises three principal
components: a cross-dispersed echelle spectrograph that operates in the
YJ-bands (0.97-1.30 microns), a fiber-injection acquisition camera system, and
a wavelength calibration unit. iLocater will deliver high spectral resolution
(R~150,000-240,000) measurements that permit novel studies of stellar and
substellar objects in the solar neighborhood including extrasolar planets.
Unlike previous planet-finding instruments, which are seeing-limited, iLocater
operates at the diffraction limit and uses single mode fibers to eliminate the
effects of modal noise entirely. By receiving starlight from two 8.4m diameter
telescopes that each use \"extreme\" adaptive optics (AO), iLocater shows promise
to overcome the limitations that prevent existing instruments from generating
sub-meter-per-second radial velocity (RV) precision. Although optimized for the
characterization of low-mass planets using the Doppler technique, iLocater will
also advance areas of research that involve crowded fields, line-blanketing,
and weak absorption lines.
</p>
{{{{ARTICLE_PARSER}}}}<p>Due to its proximity, SN 1987A offers a unique opportunity to directly
observe the geometry of a stellar explosion as it unfolds. Here we present
spectral and imaging observations of SN 1987A obtained ~10,000 days after the
explosion with HST/STIS and VLT/SINFONI at optical and near-infrared
wavelengths. These observations allow us to produce the most detailed 3D map of
H-alpha to date, the first 3D maps for [Ca II] \lambda \lambda 7292, 7324, [O
I] \lambda \lambda 6300, 6364 and Mg II \lambda \lambda 9218, 9244, as well as
new maps for [Si I]+[Fe II] 1.644 \mu m and He I 2.058 \mu m. A comparison with
previous observations shows that the [Si I]+[Fe II] flux and morphology have
not changed significantly during the past ten years, providing evidence that it
is powered by 44Ti. The time-evolution of H-alpha shows that it is
predominantly powered by X-rays from the ring, in agreement with previous
findings. All lines that have sufficient signal show a similar large-scale 3D
structure, with a north-south asymmetry that resembles a broken dipole. This
structure correlates with early observations of asymmetries, showing that there
is a global asymmetry that extends from the inner core to the outer envelope.
On smaller scales, the two brightest lines, H-alpha and [Si I]+[Fe II] 1.644
\mu m, show substructures at the level of ~ 200 - 1000 km/s and clear
differences in their 3D geometries. We discuss these results in the context of
explosion models and the properties of dust in the ejecta.
</p>
{{{{ARTICLE_PARSER}}}}<p>The exoplanet HD97658b provides a rare opportunity to probe the atmospheric
composition and evolution of moderately irradiated super-Earths. It transits a
bright K star at a moderate orbital distance of 0.08 au. Its low density is
compatible with a massive steam envelope that could photodissociate at high
altitudes and become observable as escaping hydrogen. Our analysis of 3
transits with HST/STIS at Ly-alpha reveals no such signature, suggesting that
the thermosphere is not hydrodynamically expanding and is subjected to a low
escape of neutral hydrogen (&lt;10^8 g/s at 3 sigma). Using HST Ly-alpha and
Chandra &amp; XMM-Newton observations at different epochs, we find that HD97658 is
a weak and soft X-ray source with signs of chromospheric variability in the
Ly-alpha line core. We determine an average reference for the intrinsic
Ly-alpha line and XUV spectrum of the star, and show that HD97658 b is in mild
conditions of irradiation compared to other known evaporating exoplanets with
an XUV irradiation about 3 times lower than the evaporating warm Neptune GJ436
b. This could be why the thermosphere of HD97658b is not expanding: the low XUV
irradiation prevents an efficient photodissociation of any putative steam
envelope. Alternatively, it could be linked to a low hydrogen content or
inefficient conversion of the stellar energy input. The HD97658 system provides
clues for understanding the stability of low-mass planet atmospheres. Our study
of HD97658 b can be seen as a control experiment of our methodology, confirming
that it does not bias detections of atmospheric escape and underlining its
strength and reliability. Our results show that stellar activity can be
efficiently discriminated from absorption signatures by a transiting exospheric
cloud. They also highlight the potential of observing the upper atmosphere of
small transiting planets to probe their physical and chemical properties
</p>
{{{{ARTICLE_PARSER}}}}<p>We report the discovery that the known `changing look' AGN Mrk 1018 has
changed spectral type for a second time. New VLT-MUSE data taken in 2015 as
part of the Close AGN Reference Survey (CARS) shows that the AGN has returned
to its original Seyfert 1.9 classification. The CARS sample is selected to
contain only bright type 1 AGN, but Mrk 1018's broad emission lines and
continuum, typical of type 1 AGN, have almost entirely disappeared. We use
spectral fitting of the MUSE spectrum and previously available spectra to
determine the drop in broad line flux and the Balmer decrement. We find that
the broad line flux has decreased by a factor of 4.75+/- 0.5 in H{\alpha} since
an SDSS spectrum was taken in 2000. The Balmer decrement has not changed
significantly implying no enhanced reddening with time, but the remaining broad
lines are more asymmetric than those present in the type 1 phase. We posit that
the change is due to an intrinsic drop in flux from the accretion disk rather
than variable extinction or a tidal disruption event.
</p>
{{{{ARTICLE_PARSER}}}}<p>We recently discovered that the active galactic nucleus (AGN) of Mrk 1018 has
changed optical type again after 30 years as a type 1 AGN. Here we combine
Chandra, NuStar, Swift, Hubble Space Telescope and ground-based observations to
explore the cause of this change. The 2-10keV flux declines by a factor of ~8
between 2010 and 2016. We show with our X-ray observation that this is not
caused by varying neutral hydrogen absorption along the line-of-sight up to the
Compton-thick level. The optical-UV spectral energy distributions are well fit
with a standard geometrically thin optically thick accretion disc model that
seems to obey the expected $L\sim T^4$ relation. It confirms that a decline in
accretion disc luminosity is the primary origin for the type change. We detect
a new narrow-line absorber in Lya blue-shifted by ~700km/s with respect to the
systemic velocity of the galaxy. This new Lya absorber could be evidence for
the onset of an outflow or a companion black hole with associated gas that
could be related to the accretion rate change. However, the low column density
of the absorber means that it is not the direct cause for Mrk 1018's
changing-look nature.
</p>
{{{{ARTICLE_PARSER}}}}<p>The discovery by the Large Area Telescope on board Fermi of variable
gamma-ray emission from radio-loud narrow-line Seyfert 1 (NLSy1) galaxies
revealed the presence of a possible third class of Active Galactic Nuclei (AGN)
with relativistic jets in addition to blazars and radio galaxies. Considering
that NLSy1 are usually hosted in spiral galaxies, this finding poses intriguing
questions about the nature of these objects and the formation of relativistic
jets. We report on a systematic investigation of the gamma-ray properties of a
sample of radio-loud NLSy1, including the detection of new objects, using 7
years of Fermi-LAT data with the new Pass 8 event-level analysis. In addition
we discuss the radio-to-very-high-energy properties of the gamma-ray emitting
NLSy1, their host galaxy, and black hole mass in the context of the blazar
scenario and the unification of relativistic jets at different scales.
</p>
{{{{ARTICLE_PARSER}}}}<p>SPIRITS---SPitzer InfraRed Intensive Transients Survey---is an ongoing survey
of nearby galaxies searching for infrared (IR) transients with Spitzer/IRAC. We
present the discovery and follow-up observations of one of our most luminous
($M_{[4.5]} = -17.1\pm0.4$ mag, Vega) and red ($[3.6] - [4.5] = 3.0 \pm 0.2$
mag) transients, SPIRITS 15c. The transient was detected in a dusty spiral arm
of IC 2163 ($D\approx35.5$ Mpc). Pre-discovery ground-based imaging revealed an
associated, shorter-duration transient in the optical and near-IR (NIR). NIR
spectroscopy showed a broad ($\approx 8400$ km s$^{-1}$), double-peaked
emission line of He I at $1.083 \mu$m, indicating an explosive origin. The NIR
spectrum of SPIRITS 15c is similar to that of the Type IIb SN 2011dh at a phase
of $\approx 200$ days. Assuming $A_V = 2.2$ mag of extinction in SPIRITS 15c
provides a good match between their optical light curves. The IR light curves
and the extreme $[3.6]-[4.5]$ color cannot be explained using only a standard
extinction law. Another luminous ($M_{4.5} = -16.1\pm0.4$ mag) event, SPIRITS
14buu, was serendipitously discovered in the same galaxy. The source displays
an optical plateau lasting $\gtrsim 80$ days, and we suggest a scenario similar
to the low-luminosity Type IIP SN 2005cs obscured by $A_V \approx 1.5$ mag.
Other classes of IR-luminous transients can likely be ruled out in both cases.
If both events are indeed SNe, this may suggest $\gtrsim 18\%$ of nearby
core-collapse SNe are missed by currently operating optical surveys.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a search for ultra-relativistic magnetic monopoles with the Pierre
Auger Observatory. Such particles, possibly a relic of phase transitions in the
early universe, would deposit a large amount of energy along their path through
the atmosphere, comparable to that of ultrahigh-energy cosmic rays (UHECRs).
The air shower profile of a magnetic monopole can be effectively distinguished
by the fluorescence detector from that of standard UHECRs. No candidate was
found in the data collected between 2004 and 2012, with an expected background
of less than 0.1 event from UHECRs. The corresponding 90% confidence level
(C.L.) upper limits on the flux of ultra-relativistic magnetic monopoles range
from $10^{-19}$ (cm$^{2}$ sr s)$^{-1}$ for a Lorentz factor $\gamma=10^9$ to
$2.5 \times10^{-21}$ (cm$^{2}$ sr s)$^{-1}$ for $\gamma=10^{12}$. These results
- the first obtained with a UHECR detector - improve previously published
limits by up to an order of magnitude.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present initial time-resolved observations of the split comet
332P/Ikeya-Murakami taken using the Hubble Space Telescope. Our images reveal a
dust-bathed cluster of fragments receding from their parent nucleus at
projected speeds in the range 0.06 to 3.5 m s$^{-1}$ from which we estimate
ejection times from October to December 2015. The number of fragments with
effective radii $\gtrsim$20 m follows a differential power law with index
$\gamma$ = -3.6$\pm$0.6, while smaller fragments are less abundant than
expected from an extrapolation of this power-law. We argue that, in addition to
losses due to observational selection, torques from anisotropic outgassing are
capable of destroying the small fragments by driving them quickly to rotational
instability. Specifically, the spin-up times of fragments $\lesssim$20 m in
radius are shorter than the time elapsed since ejection from the parent
nucleus. The effective radius of the parent nucleus is $r_e \le$ 275 m
(geometric albedo 0.04 assumed). This is about seven times smaller than
previous estimates and results in a nucleus mass at least 300 times smaller
than previously thought. The mass in solid pieces, $2\times10^9$ kg, is about
4% of the mass of the parent nucleus. As a result of its small size, the parent
nucleus also has a short spin-up time. Brightness variations in time-resolved
nucleus photometry are consistent with rotational instability playing a role in
the release of fragments.
</p>
{{{{ARTICLE_PARSER}}}}<p>The formation of supermassive stars (SMSs) via rapid mass accretion and their
direct collapse into black holes (BHs) is a promising pathway for sowing seeds
of supermassive BHs in the early universe. We calculate the evolution of
rapidly accreting SMSs by solving the stellar structure equations including
nuclear burning as well as general relativistic (GR) effects up to the onset of
the collapse. We find that such SMSs have less concentrated structure than
fully-convective counterpart, which is often postulated for non-accreting ones.
This effect stabilizes the stars against GR instability even above the
classical upper mass limit $\gtrsim 10^5~M_\odot$ derived for the
fully-convective stars. The accreting SMS begins to collapse at the higher mass
with the higher accretion rate. The collapse occurs when the nuclear fuel is
exhausted only for cases with $\dot M \lesssim 0.1~M_\odot~{\rm yr}^{-1}$. With
$\dot{M} \simeq 0.3 - 1~M_\odot~{\rm yr}^{-1}$, the star becomes GR-unstable
during the helium-burning stage at $M \simeq 2 - 3.5~\times 10^5~M_\odot$. In
an extreme case with $10~M_\odot~{\rm yr}^{-1}$, the star does not collapse
until the mass reaches $\simeq 8.0\times 10^5~M_\odot$, where it is still in
the hydrogen-burning stage. We expect that BHs with roughly the same mass will
be left behind after the collapse in all the cases.
</p>
{{{{ARTICLE_PARSER}}}}<p>We analyze near-infrared UKIDSS observations of a sample of 8325 objects
taken from a catalog of intrinsically red sources in the Galactic plane
selected in the Spitzer-GLIMPSE survey. Given the differences in angular
resolution (factor &gt;2 better in UKIDSS), our aim is to investigate whether
there are multiple UKIDSS sources that might all contribute to the GLIMPSE
flux, or there is only one dominant UKIDSS counterpart. We then study possible
corrections to estimates of the SFR based on counts of GLIMPSE young stellar
objects (YSOs). This represents an exploratory work towards the construction of
a hierarchical YSO catalog. After performing PSF fitting photometry in the
UKIDSS data, we implemented a technique to automatically recognize the dominant
UKIDSS sources by evaluating their match with the spectral energy distribution
(SED) of the associated GLIMPSE red sources. This is a generic method which
could be robustly applied for matching SEDs across gaps at other wavelengths.
We found that most (87.0% +- 1.6%) of the candidate YSOs from the GLIMPSE red
source catalog have only one dominant UKIDSS counterpart which matches the
mid-infrared SED (fainter associated UKIDSS sources might still be present).
Though at first sight this could seem surprising, given that YSOs are typically
in clustered environments, we argue that within the mass range covered by the
GLIMPSE YSO candidates (intermediate to high masses), clustering with objects
with comparable mass is unlikely at the GLIMPSE resolution. Indeed, by
performing simple clustering experiments based on a population synthesis model
of Galactic YSOs, we found that although ~60% of the GLIMPSE YSO enclose at
least two UKIDSS sources, in general only one dominates the flux. No
significant corrections are needed for estimates of the SFR of the Milky Way
based on the assumption that the GLIMPSE YSOs are individual objects.
(Abridged)
</p>
{{{{ARTICLE_PARSER}}}}<p>We present photometry and time-series spectroscopy of the nearby type Ia
supernova (SN Ia) SN 2015F over $-16$ days to $+80$ days relative to maximum
light, obtained as part of the Public ESO Spectroscopic Survey of Transient
Objects (PESSTO). SN 2015F is a slightly sub-luminous SN Ia with a decline rate
of $\Delta m15(B)=1.35 \pm 0.03$ mag, placing it in the region between normal
and SN 1991bg-like events. Our densely-sampled photometric data place tight
constraints on the epoch of first light and form of the early-time light curve.
The spectra exhibit photospheric C II $\lambda 6580$ absorption until $-4$
days, and high-velocity Ca II is particularly strong at $&lt;-10$ days at
expansion velocities of $\simeq$23000\kms. At early times, our spectral
modelling with syn++ shows strong evidence for iron-peak elements (Fe II, Cr
II, Ti II, and V II) expanding at velocities $&gt;14000$ km s$^{-1}$, suggesting
mixing in the outermost layers of the SN ejecta. Although unusual in SN Ia
spectra, including V II in the modelling significantly improves the spectral
fits. Intriguingly, we detect an absorption feature at $\sim$6800 \AA\ that
persists until maximum light. Our favoured explanation for this line is
photospheric Al II, which has never been claimed before in SNe Ia, although
detached high-velocity C II material could also be responsible. In both cases
the absorbing material seems to be confined to a relatively narrow region in
velocity space. The nucleosynthesis of detectable amounts of Al II would argue
against a low-metallicity white dwarf progenitor. We also show that this 6800
\AA\ feature is weakly present in other normal SN Ia events, and common in the
SN 1991bg-like sub-class.
</p>
{{{{ARTICLE_PARSER}}}}<p>Conventional Type Ia supernova (SN Ia) cosmology analyses currently use a
simplistic linear regression of magnitude versus color and light curve shape,
which does not model intrinsic SN Ia variations and host galaxy dust as
physically distinct effects, resulting in low color-magnitude slopes. We
construct a probabilistic generative model for the distribution of dusty
extinguished absolute magnitudes and apparent colors as a convolution of the
intrinsic SN Ia color-magnitude distribution and the host galaxy dust
reddening-extinction distribution. If the intrinsic color-magnitude (M_B vs.
B-V) slope beta_int differs from the host galaxy dust law R_B, this convolution
results in a specific curve of mean extinguished absolute magnitude vs.
apparent color. The derivative of this curve smoothly transitions from beta_int
in the blue tail to R_B in the red tail of the apparent color distribution. The
conventional linear fit approximates this effective curve at this transition
near the average apparent color, resulting in an apparent slope beta_app
between beta_int and R_B. We incorporate these effects into a hierarchical
Bayesian statistical model for SN Ia light curve measurements, and analyze a
dataset of SALT2 optical light curve fits of a compilation of 277 nearby SN Ia
at z &lt; 0.10. The conventional linear fit obtains beta_app = 3. Our model finds
a beta_int = 2.2 +/- 0.3 and a distinct dust law of R_B = 3.7 +/- 0.3,
consistent with the average for Milky Way dust, while correcting a systematic
distance bias of ~0.10 mag in the tails of the apparent color distribution.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study statistical properties of synchrotron polarization emitted from
media with magnetohydrodynamic (MHD) turbulence. We use both synthetic and MHD
turbulence simulation data for our studies. We obtain the spatial spectrum and
its derivative with respect to wavelength of synchrotron polarization arising
from both synchrotron radiation and Faraday rotation fluctuations. In
particular, we investigate how the spectrum changes with frequency. We find
that our simulations agree with the theoretical predication in Lazarian \&amp;
Pogosyan (2016). We conclude that the spectrum of synchrotron polarization and
it derivative can be very informative tools to get detailed information about
the statistical properties of MHD turbulence from radio observations of diffuse
synchrotron polarization. Especially, they are useful to recover the statistics
of turbulent magnetic field as well as turbulent density of electrons. We also
simulate interferometric observations that incorporate the effects of noise and
finite telescope beam size, and demonstrate how we recover statistics of
underlying MHD turbulence.
</p>
{{{{ARTICLE_PARSER}}}}<p>We investigate whether small perturbations can cause relaxation to quantum
equilibrium over very long timescales. We consider in particular a
two-dimensional harmonic oscillator, which can serve as a model of a field mode
on expanding space. We assume an initial wave function with small perturbations
to the ground state. We present evidence that the trajectories are highly
confined so as to preclude relaxation to equilibrium even over very long
timescales. Cosmological implications are briefly discussed.
</p>
{{{{ARTICLE_PARSER}}}}<p>We confirm that the object DDO216-A1 is a substantial globular cluster at the
center of Local Group galaxy DDO216 (the Pegasus dwarf irregular), using Hubble
Space Telescope ACS imaging. By fitting isochrones, we find the cluster
metallicity to be -1.6 +/-0.2, for reddening E(B-V) = 0.16 +/-0.02; the
best-fit age is 12.3 +/-0.8 Gyr. There are ~30 RR Lyrae variables in the
cluster; the magnitude of the fundamental mode pulsators gives a distance
modulus of 24.77 +/-0.08 - identical to the host galaxy. The ratio of overtone
to fundamental mode variables and their mean periods make DDO216-A1 an
Oosterhoff Type I cluster. We find an I-band central surface brightness 20.85
+/-0.17 F814W mag per square arcsecond, half-light radius of 3.1 arcsec (13.4
pc), and absolute magnitude M814 = -7.90 +/-0.16 (approximately 10^5 solar
masses). King models fit to the cluster give the core radius and concentration
index, r_c = 2.1\" +/-0.9\" and c = 1.24 +/-0.39. The cluster is an \"extended\"
cluster somewhat typical of some dwarf galaxies and the outer halo of the Milky
Way. The cluster is projected &lt;30 pc south of the center of DDO216, unusually
central compared to most dwarf galaxy globular clusters. Analytical models of
dynamical friction and tidal destruction suggest that it probably formed at a
larger distance, up to ~1 kpc, and migrated inward. DDO216 has an unexceptional
cluster specific frequency, S_N = 10. DDO216 is the lowest-luminosity Local
Group galaxy to host a 10^5 solar mass globular cluster, and the only
transition-type (dSph/dIrr) in the Local Group with a globular.
</p>
{{{{ARTICLE_PARSER}}}}<p>By monitoring a large number of stars in the Local Group galaxies such as M33
with an 8\,m-class telescope with time integration of $\sim 100\,$sec per shot,
we can detect microlensing events by sub-lunar mass compact objects (SULCOs)
such as primordial black holes (PBHs) and rogue (free-floating) dwarf planets.
For one night observation, we would be able to detect $10^{3-4}$ microlensing
events caused by SULCOs with a mass of $10^{-9}$ to $10^{-7}$ solar mass for
sources with S/N$&gt;5$ if SULCOs constitute all the dark matter components.
Moreover, we expect $10^{1-2}$ events in which sources with S/N$&gt;100$ are
weakly amplified due to lensing by SULCOs with a mass range of $10^{-11}$ to
$10^{-7}$ solar mass. The method would provide a stringent constraint on the
abundance of SULCOs at the distance $0.1-100$ kpc from us.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present long term H$\alpha$ monitoring results of five Be/X-ray binaries
to study the Be disc size variations and their influence on Type II (giant)
X-ray outbursts. The work is done in the context of the viscous decretion disc
model which predicts that Be discs in binary systems are truncated by resonant
torques induced by the neutron star in its orbit. Our observations show that
type II outbursts are not correlated(nor anti-correlated) with the disc size,
as they are seen to occur both at relatively small and large Be disc radii. We
discuss these observations in context of alternate interpretation of Be disc
behaviour, such as precession, elongation and density effects, and with
cognisance of the limitations of our disc size estimates.
</p>
{{{{ARTICLE_PARSER}}}}<p>We describe general features that might be observed in the line spectra of
relic cosmological particles should quantum nonequilibrium be preserved in
their statistics. According to our arguments, these features would represent a
significant departure from those of a conventional origin. Among other
features, we find a possible spectral broadening (for incident photons) that is
proportional to the energy resolution of the recording telescope (and so could
be orders of magnitude larger than any intrinsic broadening). Notably, for a
range of possible initial conditions we find the possibility of spectral line
`narrowing' whereby a telescope could observe a spectral line which is narrower
than it should conventionally be able to resolve. We briefly discuss
implications for the indirect search for dark matter.
</p>
{{{{ARTICLE_PARSER}}}}<p>The determination of the resolution of cosmological N-body simulations, i.e.,
the range of scales in which quantities measured in them represent accurately
the continuum limit, is an important open question. We address it here using
scale-free models, for which self-similarity provides a powerful tool to
control resolution. Such models also provide a robust testing ground for the
so-called stable clustering approximation, which gives simple predictions for
them. Studying large N-body simulations of such models with different force
smoothing, we find that these two issues are in fact very closely related: our
conclusion is that resolution in the non-linear regime extends, in practice,
down to the scale at which stable clustering breaks down. Physically the
association of the two scales is in fact simple to understand: stable
clustering fails to be a good approximation when there are strong interactions
of structures (in particular merging) and it is precisely such non-linear
processes which are sensitive to fluctuations at the smaller scales affected by
discretisation. Resolution may be further degraded if the short distance
gravitational smoothing scale is larger than the scale to which stable
clustering can propagate. We examine in detail the very different conclusions
of a study by Smith et al. (2003), and find that the strong deviations from
stable clustering reported by this work are the results of over-optimistic
assumptions about resolution of the measured power spectra, and the reliance on
Fourier space analysis. We emphasize the much poorer resolution obtained with
the power spectrum compared to the two point correlation function.
</p>
{{{{ARTICLE_PARSER}}}}<p>Time-dependent injection can cause non-linear cooling effects, which lead to
a faster energy loss of the electrons in jets. The most obvious result is the
appearance of unique breaks in the SED, which would normally be attributed to a
complicated electron distribution. The knowledge of the observation time and
duration is important to interpret the observed spectra, because of the
non-trivial evolution of the SED. Intrinsic gamma-gamma absorption processes in
the emission region are only of minor importance.
</p>
{{{{ARTICLE_PARSER}}}}<p>We analyze 15,000 spectra of 29 stellar-mass black hole candidates collected
over the 16-year mission lifetime of RXTE using a simple phenomenological
model. As these black holes vary widely in luminosity and progress through a
sequence of spectral states, which we broadly refer to as hard and soft, we
focus on two spectral components: The Compton power law and the reflection
spectrum it generates by illuminating the accretion disk. Our proxy for the
strength of reflection is the equivalent width of the Fe-K line as measured
with respect to the power law. A key distinction of our work is that for all
states we estimate the continuum under the line by excluding the thermal disk
component and using only the component that is responsible for fluorescing the
Fe-K line, namely the Compton power law. We find that reflection is several
times more pronounced (~3) in soft compared to hard spectral states. This is
most readily caused by the dilution of the Fe line amplitude from Compton
scattering in the corona, which has a higher optical depth in hard states.
Alternatively, this could be explained by a more compact corona in soft
(compared to hard) states, which would result in a higher reflection fraction.
</p>
{{{{ARTICLE_PARSER}}}}<p>Knowing the distribution of stellar rotational velocities is essential for
the understanding stellar evolution. Because we measure the projected
rotational speed vsini, we need to solve an ill-posed problem given by a
Fredholm integral of the first kind to recover the true rotational velocity
distribution. After discretization of the Fredholm integral, we apply the
Tikhonov regularization method to obtain directly the probability distribution
function for stellar rotational velocities. We propose a simple and
straightforward procedure to determine the Tikhonov parameter. We applied Monte
Carlo simulations to prove that Tikhonov method is a consistent estimator and
asymptotically unbiased. This method is applied to a sample of cluster stars.
We obtain confidences intervals using bootstrap method. Our results are in good
agreement with the one obtained using the Lucy method, in recovering the
probability density distribution of rotational velocities. Furthermore, Lucy
estimation lies inside our confidence interval. Tikhonov regularization is a
very robust method that deconvolve the rotational velocity probability density
function from a sample of vsini data straightforward without needing any
convergence criteria.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present an analytical solution for the luminosity distance in spatially
flat cosmology with pressureless matter and the cosmological constant. The
complex analytical solution is made of a real part and a negligible imaginary
part. The real part of the luminosity distance allows finding the two
parameters $H_0$ and $\om$. A simple expression for the distance modulus for
SNs of type Ia is reported in the framework of the minimax approximation.
</p>
{{{{ARTICLE_PARSER}}}}<p>Dense plasma fragments were observed to fall back on the solar surface by the
Solar Dynamics Observatory after an eruption on 7 June 2011, producing strong
EUV brightenings. Previous studies investigated impacts in regions of weak
magnetic field. Here we model the $\sim~300$ km/s impact of fragments
channelled by the magnetic field close to active regions. In the observations,
the magnetic channel brightens before the fragment impact. We use a 3D-MHD
model of spherical blobs downfalling in a magnetized atmosphere. The blob
parameters are constrained from the observation. We run numerical simulations
with different ambient density and magnetic field intensity. We compare the
model emission in the 171\AA~ channel of the Atmospheric Imaging Assembly with
the observed one. We find that a model of downfall channelled in a $\sim~1$MK
coronal loop confined by a magnetic field of $\sim~10-20$G, best explains
qualitatively and quantitatively the observed evolution. The blobs are highly
deformed, further fragmented, when the ram pressure becomes comparable to the
local magnetic pressure and they are deviated to be channelled by the field,
because of the differential stress applied by the perturbed magnetic field.
Ahead of them, in the relatively dense coronal medium, shock fronts propagate,
heat and brighten the channel between the cold falling plasma and the solar
surface. This study shows a new mechanism which brightens downflows channelled
by the magnetic field, such as in accreting young stars, and also works as a
probe of the ambient atmosphere, providing information about the local plasma
density and magnetic field.
</p>
{{{{ARTICLE_PARSER}}}}<p>Almost all SETI searches to date have explicitly targeted stars in the hope
of detecting artificial radio or optical transmissions. It is argued that
extra-terrestrials (ET) might regard sending physical probes to our own Solar
System as a more efficient means for sending large amounts of information to
Earth. Probes are more efficient in terms of energy and time expenditures; may
solve for the vexing problem of Drake's L factor term, namely, that the
civilization wishing to send information may not coexist temporally with the
intended recipient; and they alleviate ET's reasonable fear that the intended
recipient might prove hostile. It is argued that probes may be numerous and
easier to find than interstellar beacons.
</p>
{{{{ARTICLE_PARSER}}}}<p>Stellar activity influences radial velocity (RV) measurements and can also
mimic the presence of orbiting planets. As part of the search for planets
around the components of wide binaries performed with the SARG High Resolution
Spectrograph at the TNG, it was discovered that HD 200466A shows strong
variation in RV that is well correlated with the activity index based on
H$_\alpha$. We used SARG to study the H$_\alpha$ line variations in each
component of the binaries and a few bright stars to test the capability of the
H$_\alpha$ index of revealing the rotation period or activity cycle. We also
analysed the relations between the average activity level and other physical
properties of the stars. We finally tried to reveal signals in the RVs that are
due to the activity. At least in some cases the variation in the observed RVs
is due to the stellar activity. We confirm that H$_\alpha$ can be used as an
activity indicator for solar-type stars and as an age indicator for stars
younger than 1.5 Gyr.
</p>
{{{{ARTICLE_PARSER}}}}<p>We investigate a new method to search for keV-scale sterile neutrinos that
could account for Dark Matter. Neutrinos trapped in our galaxy could be
captured on stable $^{163}$Dy if their mass is greater than 2.83~keV. Two
experimental realizations are studied, an integral counting of $^{163}$Ho atoms
in dysprosium-rich ores and a real-time measurement of the emerging electron
spectrum in a dysprosium-based detector. The capture rates are compared to the
solar neutrino and radioactive backgrounds. An integral counting experiment
using several kilograms of $^{163}$Dy could reach a sensitivity for the
sterile-to-active mixing angle $\sin^2\theta_{e4}$ of $10^{-5}$ significantly
exceeding current laboratory limits. Mixing angles as low as $\sin^2\theta_{e4}
\sim 10^{-7}$ / $\rm m_{^{163}\rm Dy}\rm{(ton)}$ could possibly be explored
with a real-time experiment.
</p>
{{{{ARTICLE_PARSER}}}}<p>On April 23, 2014, the Swift satellite responded to a hard X-ray transient
detected by its Burst Alert Telescope, which turned out to be a stellar flare
from a nearby, young M dwarf binary DG~CVn. We utilize observations at X-ray,
UV, optical, and radio wavelengths to infer the properties of two large flares.
The X-ray spectrum of the primary outburst can be described over the 0.3-100
keV bandpass by either a single very high temperature plasma or a nonthermal
thick-target bremsstrahlung model, and we rule out the nonthermal model based
on energetic grounds. The temperatures were the highest seen spectroscopically
in a stellar flare, at T$_{X}$ of 290 MK. The first event was followed by a
comparably energetic event almost a day later. We constrain the photospheric
area involved in each of the two flares to be $&gt;$10$^{20}$ cm$^{2}$, and find
evidence from flux ratios in the second event of contributions to the white
light flare emission in addition to the usual hot, T$\sim$10$^{4}$K blackbody
emission seen in the impulsive phase of flares. The radiated energy in X-rays
and white light reveal these events to be the two most energetic X-ray flares
observed from an M dwarf, with X-ray radiated energies in the 0.3-10 keV
bandpass of 4$\times$10$^{35}$ and 9$\times$10$^{35}$ erg, and optical flare
energies at E$_{V}$ of 2.8$\times$10$^{34}$ and 5.2$\times$10$^{34}$ erg,
respectively. The results presented here should be integrated into updated
modelling of the astrophysical impact of large stellar flares on close-in
exoplanetary atmospheres.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this work we report the detection of seven Neptune Trojans (NTs) in the
Pan-STARRS 1 (PS1) survey. Five of these are new discoveries, consisting of
four L4 Trojans and one L5 Trojan. Our orbital simulations show that the L5
Trojan stably librates for only several million years. This suggests that the
L5 Trojan must be of recent capture origin. On the other hand, all four new L4
Trojans stably occupy the 1:1 resonance with Neptune for more than 1 Gyr. They
can, therefore, be of primordial origin. Our survey simulation results show
that the inclination width of the Neptune Trojan population should be between
$7^{\circ}$ and $27^{\circ}$ at $&gt;$ 95% confidence, and most likely $\sim
11^{\circ}$. In this paper, we describe the PS1 survey, the Outer Solar System
pipeline, the confirming observations, and the orbital/physical properties of
the new Neptune Trojans.
</p>
{{{{ARTICLE_PARSER}}}}<p>In dense and cold regions of the interstellar medium (ISM), molecules may be
adsorbed onto dust grains to form the ice mantles. Once formed, they can be
processed by ionizing radiation coming from stellar or interstellar medium
leading to formation of several new molecules in the ice. Among the different
kind of ionizing radiation, cosmic rays play an important role in the
solid-phase chemistry because of the large amount of energy deposited in the
ices. The physicochemical changes induced by the energetic processing of
astrophysical ices are recorded in a intrinsic parameter of the matter called
complex refractive index (CRI). In this paper, we present for the first time a
catalogue containing 39 complex refractive indices (n, k) in the infrared from
2.0 - 16.6 micrometer for 13 different water-containing ices processed in
laboratory by cosmic ray analogs. The calculation was done by using the NKABS
(acronym of determination of N and K from ABSorbance data) code, which employs
the Lambert-Beer and Kramers-Kronig equations to calculate the values of n and
k. The results are also available at the website:
<a href=\"http://www1.univap.br/gaa/nkabs-database/data.htm.\">this http URL</a> As test case, a
H2O:NH3:CO2:CH4 ice was employed in a radiative transfer simulation of a
prototoplanetary disk to show that these data are indispensable to reproduce
the spectrum of YSOs containing ices.
</p>
{{{{ARTICLE_PARSER}}}}<p>We investigate the intrinsic properties of a sample of bright (E_iso &gt; 10^52
erg) gamma-ray bursts, comparing those with and without radio afterglows. We
find that the sample of bursts with no radio afterglows has a significantly
shorter mean intrinsic duration of the prompt gamma-ray radiation, and the
distribution of this duration is significantly different from those bursts with
a radio afterglow. Although the sample with no radio afterglow has on average
lower isotropic energy, the lack of radio afterglow does not appear to be a
result of simply energetics of the burst, but a reflection of a separate
physical phenomenon likely related to the circumburst density profile. We also
find a weak correlation between the isotropic $\gamma-$ray energy and intrinsic
duration in the sample with no radio afterglow, but not in the sample which
have observed radio afterglows. We give possible explanations for why there may
exist a sample of GRBs with no radio afterglow depending on whether the radio
emission comes from the forward or reverse shock, and why these bursts appear
to have intrinsically shorter prompt emission durations. We discuss how our
results may have implications for progenitor models of GRBs.
</p>
{{{{ARTICLE_PARSER}}}}<p>The current status of the phenomenology of short-baseline neutrino
oscillations induced by light sterile neutrinos in the framework of 3+1 mixing
is reviewed.
</p>
{{{{ARTICLE_PARSER}}}}<p>Utilizing the all-sky imaging capabilities of the LWA1 radio telescope along
with a host of all-sky optical cameras, we have now observed 44 optical meteor
counterparts to radio afterglows. Combining these observations we have
determined the geographic positions of all 44 afterglows. Comparing the number
of radio detections as a function of altitude above sea level to the number of
expected bright meteors we find a strong altitudinal dependence characterized
by a cutoff below $\sim$ 90 km, below which no radio emission occurs, despite
the fact that many of the observed optical meteors penetrated well below this
altitude. This cutoff suggests that wave damping from electron collisions is an
important factor for the evolution of radio afterglows, which agrees with the
hypothesis that the emission is the result of electron plasma wave emission.
</p>
{{{{ARTICLE_PARSER}}}}<p>We revisit the nonthermal gravitino production at the (p)reheating stage
after inflation. Particular attention is paid to large field inflation models
with a $\mathbb{Z}_2$ symmetry, for which the previous perturbative analysis is
inapplicable; and inflation models with a stabilizer superfield, which have not
been studied non-perturbatively. It is found that in single-superfield
inflation models (without the stabilizer field), nonthermal production of the
transverse gravitino can be cosmologically problematic while the abundance of
the longitudinal gravitino is small enough. In multi-superfield inflation
models (with the stabilizer field), production of the transverse gravitino is
significantly suppressed, and it is cosmologically harmless. We also clarify
the relation between the background field method used in the preheating context
and the standard perturbative decay method to estimate the gravitino abundance.
</p>
{{{{ARTICLE_PARSER}}}}<p>The classical equations of motion for an axion with potential
$V(\phi)=m_a^2f_a^2 [1-\cos (\phi/f_a)]$ possess quasi-stable, localized,
oscillating solutions, which we refer to as \"axion stars\". We study, for the
first time, collapse of axion stars numerically using the full non-linear
Einstein equations of general relativity and the full non-perturbative cosine
potential. We map regions on an \"axion star stability diagram\", parameterized
by the initial ADM mass, $M_{\rm ADM}$, and axion decay constant, $f_a$. We
identify three regions of the parameter space: i) long-lived oscillating axion
star solutions, with a base frequency, $m_a$, modulated by self-interactions,
ii) collapse to a BH and iii) complete dispersal due to gravitational cooling
and interactions. We locate the boundaries of these three regions and an
approximate \"triple point\" $(M_{\rm TP},f_{\rm TP})\sim (2.4 M_{pl}^2/m_a,0.3
M_{pl})$. For $f_a$ below the triple point BH formation proceeds during winding
(in the complex $U(1)$ picture) of the axion field near the dispersal phase.
This could prevent astrophysical BH formation from axion stars with $f_a\ll
M_{pl}$. For larger $f_a\gtrsim f_{\rm TP}$, BH formation occurs through the
stable branch and we estimate the mass ratio of the BH to the stable state at
the phase boundary to be $\mathcal{O}(1)$ within numerical uncertainty. We
discuss the observational relevance of our findings for axion stars as BH
seeds, which are supermassive in the case of ultralight axions. For the QCD
axion, the typical BH mass formed from axion star collapse is $M_{\rm BH}\sim
3.4 (f_a/0.6 M_{pl})^{1.2} M_\odot$.
</p>
{{{{ARTICLE_PARSER}}}}<p>BHR 160 is a virtually unstudied cometary globule within the Sco OB4
association in Scorpius at a distance of 1600pc. It is part of a system of
cometary clouds which face the luminous O star HD155806. BHR 160 is special
because it has an intense bright rim. We attempt to derive physical parameters
for BHR 160 and to understand its structure and the origin of its peculiar
bright rim. BHR 160 was mapped in the $^{12}$CO, $^{13}$CO and C$^{18}$O (2-1)
and (1-0) and CS (3-2) and (2-1) lines. These data, augmented with stellar
photometry derived from the ESO VVV survey, were used to derive the mass and
distribution of molecular material in BHR 160 and its surroundings. Archival
mid-infrared data from the WISE satellite was used to find IR excess stars in
the globule and its neighbourhood. An elongated 1' by 0.6' core lies adjacent
to the globule bright rim. $^{12}$CO emission covers the whole globule, but the
$^{13}$CO, C$^{18}$CO and CS emission is more concentrated to the core. The
$^{12}$CO line profiles indicate the presence of outflowing material near the
core, but the spatial resolution of the mm data is not sufficient for a
detailed spatial analysis. The BHR 160 mass estimated from the C$^{18}$CO
mapping is 100$\pm$50Msun(d/1.6kpc)$^2$ where d is the distance to the globule.
Approximately 70% of the mass lies in the dense core. The total mass of
molecular gas in the direction of BHR 160 is 210$\pm$(d/1.6kpc)$^2$ Msun when
estimated from the more extended VVV NIR photometry. We argue that the bright
rim of BHR 160 is produced by a close-by early B-type star, HD 319648, that was
likely recently born in the globule. This star is likely to have triggered the
formation of a source, IRS 1, that is embedded within the core of the globule
and detected only in Ks and by WISE and IRAS.
</p>
{{{{ARTICLE_PARSER}}}}<p>The chemistry of dense interstellar regions was analyzed using a
time-dependent gas-grain astrochemical simulation and a new chemical network
that incorporates deuterated chemistry taking into account nuclear spin-states
for the hydrogen chemistry and its deuterated isotopologues. With this new
network, the utility of the [HCO$^+$]/[DCO$^+$] abundance ratio as a probe of
the cosmic ray ionization rate has been reexamined, with special attention paid
to the effect of the initial value of the molecular hydrogen ortho-to-para
ratio (OPR). After discussing the use of the probe for cold cores, we then
compare our results with previous theoretical and observational results for a
molecular cloud close to the supernova remnant W51C, which is thought to have
an enhanced cosmic ray ionization rate $\zeta$ caused by the nearby
$\gamma$-ray source. In addition, we attempt to use our approach to estimate
the cosmic ray ionization rate for L1174, a dense core with an embedded star.
Beyond the previously known sensitivity of [HCO$^+$]/[DCO$^+$] to $\zeta$, we
demonstrate its additional dependence on the initial OPR and, secondarily, on
the age of the source, its temperature, and its density. We conclude that the
usefulness of the [HCO$^+$]/[DCO$^+$] abundance ratio to constrain the cosmic
ray ionization rate in dense regions increases with source age and ionization
rate as the ratio becomes far less sensitive to the initial value of the OPR.
</p>
{{{{ARTICLE_PARSER}}}}<p>In the hope of avoiding model dependence of the cosmological observables,
phenomenological parametrizations of Cosmic Inflation have recently been
proposed. Typically, they are expressed in terms of two parameters associated
with an expansion of the inflationary quantities matching the belief that
inflation is characterized by two numbers only, the tensor-to-scalar ratio and
the scalar spectral index. We give different arguments and examples showing
that these new approaches are either not generic or insufficient to make
predictions at the accuracy level needed by the cosmological data. We conclude
that disconnecting inflation from high energy physics and gravity might not be
the most promising way to learn about the physics of the early Universe.
</p>
{{{{ARTICLE_PARSER}}}}<p>Long-term stability of deformable mirrors (DM) is a critical performance
requirement for instruments requiring open-loop corrections. The effects of
temperature changes in the DM performance are equally critical for such
instruments. This paper investigates the long-term stability of three different
Iris AO PTT111 DMs that were calibrated at different times ranging from 13
months to nearly 29 months prior to subsequent testing. Performance testing
showed that only a small increase in positioning errors occurred from the
initial calibration date to the test dates. The increases in errors ranged from
as little as 1.38 nm rms after 18 months to 5.68 nm rms after 29 months. The
paper also studies the effects of small temperature changes, up to 6.2{\deg}C
around room temperature. For three different arrays, the errors ranged from
0.62-1.42 nm rms/{\deg}C. Removing the effects of packaging shows that errors
are $\le$0.50 nm rms/{\deg}C. Finally, measured data showed that individual
segments deformed $\le$0.11 nm rms/{\deg}C when heated.
</p>
{{{{ARTICLE_PARSER}}}}<p>Type III bursts and hard X-rays are both produced by flare energetic electron
beams. The link between both emissions has been investigated in many previous
studies, but no statistical studies have compared both coronal and
interplanetary type III bursts with X-ray flares. Using coronal radio events
above 100 MHz exclusively from type III bursts, we revisited long-standing
questions: Do all coronal type III bursts have X-ray counterparts. What
correlation, if any, occurs between radio and X-ray intensities. What X-ray and
radio signatures above 100 MHz occur in connection with interplanetary type III
bursts below 14 MHz. We analysed data from 2002 to 2011 starting with coronal
type III bursts above 100 MHz. We used RHESSI X-ray data greater than 6 keV to
make a list of 321 events that have associated type III bursts and X-ray
flares, encompassing at least 28 percent of the initial sample of type III
events. We examined the timings, intensities, associated GOES class, and any
interplanetary radio signature. For our 321 events, the X-ray emission at 6 keV
usually lasted longer than type III burst groups at frequencies greater than
100 MHz. A weak correlation was found between the type III radio flux at
frequencies below 327 MHz and the X-ray intensity at 25-50 keV, with an absence
of events at high X-ray intensity and low type III radio flux. Interplanetary
type III bursts less than 14 MHz were observed for 54 percent of the events,
increasing when events were observed with 25-50 keV X-rays. A stronger
interplanetary association was present when 25-50 keV count rates were above
250 counts per second or 170 MHz fluxes were greater than 1000 SFU, relating to
more energetic electrons above 25 keV and events where magnetic flux tubes
extend into the high corona. On average type III bursts increase in flux with
decreasing frequency, the rate varies from event to event.
</p>
{{{{ARTICLE_PARSER}}}}<p>Highly siderophile elements (HSEs) are strongly depleted in the bulk silicate
Earth (BSE) but are present in near-chondritic relative abundances. The
conventional explanation is that the HSEs were stripped from the mantle by the
segregation of metal during core formation but were added back in
near-chondritic proportions by late accretion, after core formation had ceased.
Here we show that metal-silicate equilibration and segregation during Earth's
core formation actually increased HSE mantle concentrations because HSE
partition coefficients are relatively low at the high pressures of core
formation within Earth. The pervasive exsolution and segregation of iron
sulfide liquid from silicate liquid (the \"Hadean matte\") stripped magma oceans
of HSEs during cooling and crystallization, before late accretion, and resulted
in slightly suprachondritic palladium/iridium and ruthenium/iridium ratios.
</p>
{{{{ARTICLE_PARSER}}}}<p>Tidal Disruption Events (TDEs) favor quiescent host galaxies with strong
Balmer absorption lines. Here we study eight hosts of optical/UV-detected TDEs
to determine the duration of the recent star formation episode, the time
elapsed since it ended, and the fraction of stellar mass produced. Most hosts
(6/8) have had short recent starbursts of &lt;200 Myr as opposed to a slower
decline in star formation. TDE host galaxies span a wide range of
post-starburst ages (60-600 Myr for 6/8 galaxies), indicating that TDEs are not
limited to a specific time in their hosts' post-starburst evolution. If the
disrupted star was a main sequence star that formed in the burst or before, the
post-burst ages provide an independent constraint on its likely mass, excluding
O, B and the most massive A stars. If the starburst arose from a galaxy merger,
the time elapsed since the starburst began constrains the coalescence timescale
and thus limits the merger mass ratio to more equal than 12:1 in most (7/8) TDE
hosts. This uncommon ratio, if it also reflects that of the central SMBH
binary, disfavors the scenario in which the TDE rate is boosted by the binary
but is insensitive to its mass ratio. The fraction of stellar mass created in
the burst is 0.5 - 10% for most (7/8) of the TDE hosts, not large enough to
explain the increased TDE rate. If more stars are required to boost the TDE
rate, the stellar concentration in the core must be more important. TDE host
galaxies have stellar masses 10^9.4 - 10^10.3 M$_\odot$, consistent with the
SDSS volume-corrected comparison sample and implying central black hole masses
of 10^5.5 - 10^7.5 M$_\odot$. Subtracting the absorption line spectra, we
uncover hidden emission lines; at least 5 of 8 hosts have ionization sources
inconsistent with star formation. These ionization sources may be related to
circumnuclear gas, merger shocks, or post-AGB stars.
</p>
{{{{ARTICLE_PARSER}}}}<p>The Sun has a steady 11-year cycle in magnetic activity most well-known by
the rising and falling in the occurrence of dark sunspots on the solar disk in
visible bandpasses. The 11-year cycle is also manifest in the variations of
emission in the Ca II H &amp; K line cores, due to non-thermal (i.e. magnetic)
heating in the lower chromosphere. The large variation in Ca II H &amp; K emission
allows for study of the patterns of long-term variability in other stars thanks
to synoptic monitoring with the Mount Wilson Observatory HK photometers
(1966-2003) and Lowell Observatory Solar-Stellar Spectrograph (1994-present).
Overlapping measurements for a set of 27 nearby solar-analog (spectral types
G0-G5) stars were used to calibrate the two instruments and construct time
series of magnetic activity up to 50 years in length. Precise properties of
fundamental importance to the dynamo are available from Hipparcos, the
Geneva-Copenhagen Survey, and CHARA interferometry. Using these long time
series and measurements of fundamental properties, we do a comparative study of
stellar \"twins\" to explore the sensitivity of the stellar dynamo to small
changes to structure, rotation, and composition. We also compare this sample to
the Sun and find hints that the regular periodic variability of the solar cycle
may be rare among its nearest neighbors in parameter space.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study the structure of two-point correlators of the inflationary field
fluctuations in order to improve the accuracy and efficiency of the existing
spectral methods. We present a description motivated by the separation of the
fast and slow evolving components of the spectrum. Our purpose is to rephrase
all the relevant equations of motion in terms of slowly varying quantities.
This is important in order to consider the contribution from high-frequency
modes to the spectrum without affecting computational performance. The
slow-roll approximation is not required to reproduce the main distinctive
features in the power spectrum for each specific model of inflation.
</p>
{{{{ARTICLE_PARSER}}}}<p>Among young binary stars whose magnetospheres are expected to collide, only
two systems have been observed near periastron in the X-ray band: the low-mass
DQ Tau and the older and more massive HD 152404. Both exhibit elevated levels
of X-ray emission at periastron. Our goal is to determine whether colliding
magnetospheres in young high-eccentricity binaries commonly produce elevated
average levels of X-ray activity. This work is based on Chandra snapshots of
multiple periastron and non-periastron passages in four nearby young eccentric
binaries (Parenago 523, RX J1622.7-2325 Nw, UZ Tau E, and HD 152404). We find
that for the merged sample of all 4 binaries the current X-ray data show an
increasing average X-ray flux near periastron (at about 2.5-sigma level).
Further comparison of these data with the X-ray properties of hundreds of young
stars in the Orion Nebula Cluster, produced by the Chandra Orion Ultradeep
Project (COUP), indicates that the X-ray emission from the merged sample of our
binaries can not be explained within the framework of the COUP-like X-ray
activity. However, due to the inhomogeneities of the merged binary sample and
the relatively low statistical significance of the detected flux increase,
these findings are regarded as tentative only. More data are needed to prove
that the flux increase is real and is related to the processes of colliding
magnetospheres.
</p>
{{{{ARTICLE_PARSER}}}}<p>Terrestrial exoplanets in the canonical habitable zone may have a variety of
initial water fractions due to random volatile delivery by planetesimals. If
the total planetary water complement is high, the entire surface may be covered
in water, forming a \"waterworld.\" On a planet with active tectonics, competing
mechanisms act to regulate the abundance of water on the surface by determining
the partitioning of water between interior and surface. Here we explore how the
incorporation of different mechanisms for the degassing and regassing of water
changes the volatile evolution of a planet. For all of the models considered,
volatile cycling reaches an approximate steady-state after $\sim 2 \
\mathrm{Gyr}$. Using these steady-states, we find that if volatile cycling is
either solely dependent on temperature or seafloor pressure, exoplanets require
a high abundance ($\gtrsim 0.3\%$ of total mass) of water to have fully
inundated surfaces. However, if degassing is more dependent on seafloor
pressure and regassing mainly dependent on mantle temperature, the degassing
rate is relatively large at late times and a steady-state between degassing and
regassing is reached with a substantial surface water fraction. If this hybrid
model is physical, super-Earths with a total water fraction similar to that of
the Earth can become waterworlds. As a result, further understanding of the
processes that drive volatile cycling on terrestrial planets is needed to
determine the water fraction at which they are likely to become waterworlds.
</p>
{{{{ARTICLE_PARSER}}}}<p>We conduct an analysis of the Planck 2015 data that is complete in
reionization observables from the large angle polarization $E$-mode spectrum in
the redshift range $6 &lt; z &lt; 30$. Based on 5 principal components, all of which
are constrained by the data, this single analysis can be used to infer
constraints on any model for reionization in the same range; we develop an
effective likelihood approach for applying these constraints to models. By
allowing for an arbitrary ionization history, this technique tests the
robustness of inferences on the total optical depth from the usual step-like
transition assumption, which is important for the interpretation of many other
cosmological parameters such as the dark energy and neutrino mass. The Planck
2015 data not only allow a high redshift $z&gt;15$ component to the optical depth
but prefer it at the $2\sigma$ level. This preference is associated with excess
power in the multipole range $10 \lesssim \ell \lesssim 20$ and may indicate
high redshift ionization sources or unaccounted for systematics and foregrounds
in the 2015 data.
</p>
{{{{ARTICLE_PARSER}}}}<p>We explore whether close-in super-Earths were formed as rocky bodies that
failed to grow fast enough to become the cores of gas giants before the natal
protostellar disk dispersed. We model the failed cores' inward orbital
migration in the low-mass or type I regime, to stopping points at distances
where the tidal interaction with the protostellar disk applies zero net torque.
The three kinds of migration traps considered are those due to the dead zone's
outer edge, the ice line, and the transition from accretion to starlight as the
disk's main heat source. As the disk disperses, the traps move toward final
positions near or just outside 1~au. Planets at this location exceeding about
3~M$_\oplus$ open a gap, decouple from their host trap, and migrate inward in
the high-mass or type II regime to reach the vicinity of the star. We
synthesize the population of planets formed in this scenario, finding that some
fraction of the observed super-Earths can be failed cores. Most super-Earths
formed this way have more than 4~M$_\oplus$, so their orbits when the disk
disperses are governed by type II migration. These planets have solid cores
surrounded by gaseous envelopes. Their subsequent photoevaporative mass loss is
most effective for masses originally below about 6 M$_\oplus$. The failed core
scenario suggests a division of the observed super-Earth mass-radius diagram
into five zones according to the inferred formation history.
</p>
{{{{ARTICLE_PARSER}}}}<p>Condensate clouds fundamentally impact the atmospheric structure and spectra
of exoplanets and brown dwarfs but the connections between surface gravity,
cloud structure, dust in the upper atmosphere, and the red colors of some brown
dwarfs remain poorly understood. Rotational modulations enable the study of
different clouds in the same atmosphere, thereby providing a method to isolate
the effects of clouds. Here we present the discovery of high peak-to-peak
amplitude (8%) rotational modulations in a low-gravity, extremely red
(J-Ks=2.55) L6 dwarf WISEP J004701.06+680352.1 (W0047). Using the Hubble Space
Telescope (HST) time-resolved grism spectroscopy we find a best-fit rotational
period (13.20$\pm$0.14 hours) with a larger amplitude at 1.1 micron than at 1.7
micron. This is the third largest near-infrared variability amplitude measured
in a brown dwarf, demonstrating that large-amplitude variations are not limited
to the L/T transition but are present in some extremely red L-type dwarfs. We
report a tentative trend between the wavelength dependence of relative
amplitude, possibly proxy for small dust grains lofted in the upper atmosphere,
and the likelihood of large-amplitude variability. By assuming forsterite as
haze particle, we successfully explain the wavelength dependent amplitude with
submicron-sized haze particles sizes of around 0.4 {\mu}m. W0047 links the
earlier spectral and later spectral type brown dwarfs in which rotational
modulations have been observed; the large amplitude variations in this object
make this a benchmark brown dwarf for the study of cloud properties close to
the L/T transition.
</p>
{{{{ARTICLE_PARSER}}}}<p>The average star formation rate (SFR) in galaxies has been declining since
redshift of 2. A fraction of galaxies quench and become quiescent. We constrain
two key properties of the quenching process: the quenching time scale and the
quenching rate among galaxies. We achieve this by analyzing the galaxy number
density profile in NUV-u color space and the distribution in NUV-u v.s. u-i
color-color diagram with a simple toy-model framework. We focus on galaxies in
three mass bins between 10 to 10 and 10 to 10.6 solar mass. In the NUV-u v.s.
u-i color-color diagram, the red u-i galaxies exhibit a different slope from
the slope traced by the star-forming galaxies. This angled distribution and the
number density profile of galaxies in NUV-u space strongly suggest that the
decline of the SFR in galaxies has to accelerate before they turn quiescent. We
model this color-color distribution with a two-phase exponential decline star
formation history. The models with an e-folding time in the second phase (the
quenching phase) of 0.5 Gyr best fit the data. We further use the NUV-u number
density profile to constrain the quenching rate among star-forming galaxies as
a function of mass. Adopting an e-folding time of 0.5 Gyr in the second phase
(or the quenching phase), we found the quenching rate to be 19%/Gyr, 25%/Gyr
and 33%/Gyr for the three mass bins. These are upper limits of quenching rate
as the transition zone could also be populated by rejuvenated red-sequence
galaxies.
</p>
{{{{ARTICLE_PARSER}}}}<p>Hot, Dust-Obscured Galaxies (Hot DOGs), selected from the WISE all sky
infrared survey, host some of the most powerful Active Galactic Nuclei (AGN)
known, and might represent an important stage in the evolution of galaxies.
Most known Hot DOGs are at $z&gt; 1.5$, due in part to a strong bias against
identifying them at lower redshift related to the selection criteria. We
present a new selection method that identifies 153 Hot DOG candidates at $z\sim
1$, where they are significantly brighter and easier to study. We validate this
approach by measuring a redshift $z=1.009$, and an SED similar to higher
redshift Hot DOGs for one of these objects, WISE J1036+0449
($L_{\rm\,Bol}\simeq 8\times 10^{46}\rm\,erg\,s^{-1}$), using data from
Keck/LRIS and NIRSPEC, SDSS, and CSO. We find evidence of a broadened component
in MgII, which, if due to the gravitational potential of the supermassive black
hole, would imply a black hole mass of $M_{\rm\,BH}\simeq 2 \times 10^8
M_{\odot}$, and an Eddington ratio of $\lambda_{\rm\,Edd}\simeq 2.7$. WISE
J1036+0449 is the first Hot DOG detected by NuSTAR, and the observations show
that the source is heavily obscured, with a column density of
$N_{\rm\,H}\simeq(2-15)\times10^{23}\rm\,cm^{-2}$. The source has an intrinsic
2-10 keV luminosity of $\sim 6\times 10^{44}\rm\,erg\,s^{-1}$, a value
significantly lower than that expected from the mid-infrared/X-ray correlation.
We also find that the other Hot DOGs observed by X-ray facilities show a
similar deficiency of X-ray flux. We discuss the origin of the X-ray weakness
and the absorption properties of Hot DOGs. Hot DOGs at $z\lesssim1$ could be
excellent laboratories to probe the characteristics of the accretion flow and
of the X-ray emitting plasma at extreme values of the Eddington ratio.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present gravitational N-body simulations of the secular morphological
evolution of disk galaxies induced by density wave modes. In particular, we
address the demands collective effects place on the choice of simulation
parameters, and show that the common practice of the use of a large gravity
softening parameter was responsible for the failure of past simulations to
correctly model the secular evolution process in galaxies, even for those
simulations where the choice of basic state allows an unstable mode to emerge,
a prerequisite for obtaining the coordinated radial mass flow pattern needed
for secular evolution of galaxies along the Hubble sequence. We also
demonstrate that the secular evolution rates measured in our improved
simulations agree to an impressive degree with the corresponding rates
predicted by the recently-advanced theories of dynamically-driven secular
evolution of galaxies. The results of the current work, besides having direct
implications on the cosmological evolution of galaxies, also shed light on the
general question of how irreversibility emerges from a nominally reversible
physical system.
</p>
{{{{ARTICLE_PARSER}}}}<p>Using dark matter simulations we show how halo bias is determined by local
density and not by halo mass. This is not totally surprising, as according to
the peak-background split model, local density is the property that constraints
bias at large scales. Massive haloes have a high clustering because they reside
in high density regions. Small haloes can be found in a wide range of
environments which determine their clustering amplitudes differently. This
contradicts the assumption of standard Halo Occupation Distribution (HOD)
models that the bias and occupation of haloes is determined solely by their
mass. We show that the bias of central galaxies from semi-analytic models of
galaxy formation as a function of luminosity and colour is not correctly
predicted by the standard HOD model. Using local density instead of halo mass
the HOD model correctly predicts galaxy bias. These results indicate the need
to include information about local density and not only mass in order to
correctly apply HOD analysis in these galaxy samples. This new model can be
readily applied to observations and has the advantage that the galaxy density
can be directly observed, in contrast with the dark matter halo mass.
</p>
{{{{ARTICLE_PARSER}}}}<p>Using Hubble Space Telescope Cosmic Origins Spectrograph observations of 89
QSO sightlines through the Sloan Digital Sky Survey footprint, we study the
relationships between C IV absorption systems and the properties of nearby
galaxies as well as large-scale environment. To maintain sensitivity to very
faint galaxies, we restrict our sample to 0.0015 &lt; z &lt; 0.015, which defines a
complete galaxy survey to L &gt; 0.01 L* or stellar mass log M_* &gt; 8 Msun. We
report two principal findings. First, for galaxies with impact parameter rho &lt;
1 rvir, C IV detection strongly depends on the luminosity/stellar mass of the
nearby galaxy. C IV is preferentially associated with galaxies with log M_* &gt;
9.5 Msun; lower mass galaxies rarely exhibit significant C IV absorption
(covering fraction f = 9 +12-6% for 11 galaxies with log M_* &lt; 9.5 Msun).
Second, C IV detection within the log M_* &gt; 9.5 Msun population depends on
environment. Using a fixed-aperture environmental density metric for galaxies
with rho &lt; 160 kpc at z &lt; 0.055, we find that 57+/-12% (8/14) of galaxies in
low-density regions (regions with fewer than seven L &gt; 0.15 L* galaxies within
1.5 Mpc) have affiliated C IV absorption; however, none (0/7) of the galaxies
in denser regions show C IV. Similarly, the C IV detection rate is lower for
galaxies residing in groups with dark-matter halo masses of log Mhalo &gt; 12.5
Msun. In contrast to C IV, H I is pervasive in the CGM without regard to mass
or environment. These results indicate that C IV absorbers with log N(C IV) &gt;
13.5 cm^-2 trace the halos of log M_* &gt; 9.5 Msun galaxies but also reflect
larger scale environmental conditions.
</p>
{{{{ARTICLE_PARSER}}}}<p>We construct spacetimes which provide spherical and nonspherical models of
black hole formation in the flat Friedmann--Lemaitre--Robertson--Walker (FLRW)
universe with the Lemaitre--Tolman--Bondi solution and the Szekeres
quasispherical solution, respectively. These dust solutions may contain both
shell-crossing and shell-focusing naked singularities. These singularities can
be physically regarded as the breakdown of dust description, where strong
pressure gradient force plays a role. We adopt the simultaneous big bang
condition to extract a growing mode of adiabatic perturbation in the flat FLRW
universe. If the density perturbation has a sufficiently homogeneous central
region and a sufficiently sharp transition to the background FLRW universe, its
central shell-focusing singularity is globally covered. If the density
concentration is sufficiently large, no shell-crossing singularity appears and
a black hole is formed. If the density concentration is not sufficiently large,
a shell-crossing singularity appears. In this case, a large dipole moment
significantly advances shell-crossing singularities and they tend to appear
before the black hole formation. In contrast, a shell-crossing singularity
unavoidably appears in the spherical and nonspherical evolution of cosmological
voids. The present analysis is general and applicable to cosmological nonlinear
structure formation described by these dust solutions.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study the evolution of giant clumps in high-z disc galaxies using AMR
cosmological simulations at redshifts z=6-1. Our sample consists of 34
galaxies, of halo masses 10^{11}-10^{12}M_s at z=2, run with and without
radiation pressure (RP) feedback from young stars. While RP has little effect
on the sizes and global stability of discs, it reduces the amount of
star-forming gas by a factor of ~2, leading to a decrease in stellar mass by a
similar factor by z~2. Both samples undergo violent disc instability (VDI) and
form giant clumps of masses 10^7-10^9M_s at a similar rate, though RP
significantly reduces the number of long-lived clumps. When RP is (not)
included, clumps with circular velocity &lt;40(20)km/s, baryonic surface density
&lt;200(100)M_s/pc^2 and baryonic mass &lt;10^{8.2}(10^{7.3})M_s are short-lived,
disrupted in a few free-fall times. The more massive and dense clumps survive
and migrate toward the disc centre over a few disc orbital times. In the RP
simulations, the distribution of clump masses and star-formation rates (SFRs)
normalized to their host disc is very similar at all redshifts. They exhibit a
truncated power-law with a slope slightly shallower than -2. Short-lived clumps
preferentially have young stellar ages, low masses, high gas fractions and
specific SFRs (sSFR), and they tend to populate the outer disc. The sSFR of
massive, long-lived clumps declines with age as they migrate towards the disc
centre, producing gradients in mass, stellar age, gas fraction, sSFR and
metallicity that distinguish them from short-lived clumps. Ex situ mergers make
up ~37% of the mass in clumps and ~29% of the SFR. They are more massive and
with older stellar ages than the in situ clumps, especially near the disc edge.
Roughly half the galaxies at redshifts z=4-1 are clumpy over a wide range of
stellar mass, with clumps accounting for ~3-30% of the SFR but ~0.1-3% of the
stellar mass.
</p>
{{{{ARTICLE_PARSER}}}}<p>It has been recently suggested~\cite{Berezhiani:2015yta} that emerging
tension between cosmological parameter values derived in high-redshift (CMB
anisotropy) and low-redshift (cluster counts, Hubble constant) measurements can
be reconciled in a model which contains subdominant fraction of dark matter
decaying after recombination. We check the model against the CMB Planck data.
We find that lensing of the CMB anisotropies by the large-scale structure gives
strong extra constraints on this model, limiting the fraction as $F&lt;8\%$ at
2\,$\sigma$ confidence level. However, investigating the combined data set of
CMB and conflicting low-$z$ measurements, we obtain that the model with
$F\approx2\!-\!5$\% exhibits better fit (by 1.5-3\,$\sigma$ depending on the
lensing priors) compared to that of the concordance $\Lambda$CDM cosmological
model.
</p>
{{{{ARTICLE_PARSER}}}}<p>We show that the Planck 2015 and BICEP2/KECK measurements of the Cosmic
Microwave Background (CMB) anisotropies provide together an information gain of
0.82 +- 0.13 bits on the reheating history over all slow-roll single-field
models of inflation. This corresponds to a 40% improvement compared to the
Planck 2013 constraints on the reheating. Our method relies on an exhaustive
CMB data analysis performed over nearly 200 models of inflation to derive the
Kullback-Leibler entropy between the prior and the fully marginalized posterior
of the reheating parameter. This number is a weighted average by the Bayesian
evidence of each model to explain the data thereby ensuring its fairness and
robustness.
</p>
{{{{ARTICLE_PARSER}}}}<p>The Ostrogradsky theorem implies that higher-derivative terms of a single
mechanical variable are either trivial or lead to additional, ghost-like
degrees of freedom. In this letter we systematically investigate how the
introduction of additional variables can remedy this situation. Employing a
Lagrangian analysis, we identify conditions on the Lagrangian to ensure the
existence of primary and secondary constraints that together imply the absence
of Ostrogradsky ghosts. We also show the implications of these conditions for
the structure of the equations of motion as well as possible redefinitions of
the variables. We discuss applications to analogous higher-derivative field
theories such as multi-Galileons and beyond Horndeski.
</p>
{{{{ARTICLE_PARSER}}}}<p>The B-mode polarization of the cosmic microwave background (CMB) provides a
unique window into tensor perturbations from inflationary gravitational waves.
Survey effects complicate the estimation and description of the power spectrum
on the largest angular scales. The pixel-space likelihood yields parameter
distributions without the power spectrum as an intermediate step, but it does
not have the large suite of tests available to power spectral methods. Searches
for primordial B-modes must rigorously reject and rule out contamination. Many
forms of contamination vary or are uncorrelated across epochs, frequencies,
surveys, or other data treatment subsets. The cross power and the power
spectrum of the difference of subset maps provide approaches to reject and
isolate excess variance. We develop an analogous joint pixel-space likelihood.
Contamination not modeled in the likelihood produces parameter-dependent bias
and complicates the interpretation of the difference map. We describe a null
test that consistently weights the difference map. Excess variance should
either be explicitly modeled in the covariance or be removed through
reprocessing the data.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present late-time optical spectroscopy taken with the Large Binocular
Telescope's Multi-Object Double Spectrograph, an improved ASAS-SN pre-discovery
non-detection, and late-time SWIFT observations of the nearby ($d=193$ Mpc,
$z=0.0436$) tidal disruption event (TDE) ASASSN-14ae. Our observations span
from $\sim$20 days before to $\sim$750 days after discovery. The proximity of
ASASSN-14ae allows us to study the optical evolution of the flare and the
transition to a host dominated state with exceptionally high precision. We
measure very weak H$\alpha$ emission 300 days after discovery ($L_{\rm H\alpha}
\simeq 4\times 10^{39}$ ergs s$^{-1}$) and the most stringent upper limit to
date on the H$\alpha$ luminosity $\sim$750 days after discovery ($L_{\rm
H\alpha} \lesssim 10^{39}$ ergs s$^{-1}$), suggesting that the optical emission
arising from a TDE can vanish on a timescale as short as 1 year. Our results
have important implications for both spectroscopic detection of TDE candidates
at late times, as well as the nature of TDE host galaxies themselves.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study production of light particles due to oscillation of the Hubble
parameter or the scale factor. Any coherently oscillating scalar field,
irrespective of its energy fraction in the universe, imprints such an
oscillating feature on them. Not only the Einstein gravity but extended gravity
models, such as models with non-minimal (derivative) coupling to gravity and
$f(R)$ gravity, lead to oscillation of the scale factor. We present a
convenient way to estimate the gravitational particle production rate in these
circumstances. Cosmological implications of gravitational particle production,
such as dark matter/radiation and moduli problem, are discussed. For example,
if the theory is described solely by the standard model plus the Peccei-Quinn
sector, the Starobinsky $R^2$ inflation may lead to observable amount of axion
dark radiation.
</p>
{{{{ARTICLE_PARSER}}}}<p>The dependence of gas giant planet occurrence rate on stellar metallicity has
been firmly established. We extend this so-called planet-metallicity
correlation to broader ranges of metallicities and planet masses/radii. In
particular, we assume that the planet-metallicity correlation is a power law
below some critical saturation threshold, and that the probability of hosting
at least one planet is unity for stars with metallicity above the threshold. We
then are able to explain the discrepancy between the tentative detection and
null detection in previous studies regarding the planet-metallicity correlation
for small planets. In particular, we find that the null detection of this
correlation can be attributed to the combination of high planet occurrence rate
and low detection efficiency. Therefore, a planet-metallicity correlation for
small planets cannot be ruled out. We propose that stars with metallicities
lower than the Solar value are better targets for testing the
planet-metallicity correlation for small planets.
</p>
{{{{ARTICLE_PARSER}}}}<p>An analysis of the physics-rich endgame of reionization at $z=5.7$ is
performed, utilizing jointly the observations of the Ly$\alpha$ forest, the
mean free path of ionizing photons, the luminosity function of galaxies and new
physical insight. We find that an upper limit on ${\rm \tau_e}$ provides a
constraint on the minimum mean free path (of ionizing photons) that is
primarily due to dwarf galaxies, which in turn yields a new and yet the
strongest constraint on the matter power spectrum on $10^6-10^9M_\odot$ scales.
With the latest Planck measurements of ${\rm \tau_e = 0.055 \pm 0.009}$, we can
place an upper limit of $(8.9\times 10^6, 3.8\times 10^7, 4.2\times
10^8)M_\odot$ on the lower cutoff mass of the halo mass function, or equivalent
a lower limit on warm dark matter particle mass ${\rm m_x \ge (15.1, 9.8,
4.6)keV}$ or on sterile neutrino mass ${\rm m_s \ge (161, 90, 33)keV}$, at $(1,
1.4, 2.2)\sigma$ confidence level, respectively.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a system of coordinates deriving directly from the so-called
Geodesic Light-Cone (GLC) coordinates and made of two null scalars intersecting
on a 2-dimensional sphere parameterized by two constant angles along geodesics.
These coordinates are shown to be equivalent to the well-known double-null
coordinates. As GLC, they present interesting properties for cosmology and
astrophysics. We discuss this latter topic for static black holes, showing
simple descriptions for the metric or particles and photons trajectories. We
also briefly comment on the time of flight of ultra-relativistic particles.
</p>
{{{{ARTICLE_PARSER}}}}<p>We investigate evolution of an accretion disk in binary black hole (BBH)
systems, the importance of which is now increasing due to its close
relationship to possible electromagnetic counterparts of the gravitational
waves (GWs) from mergers of BBHs. Perna et al. (2016) proposed a novel
evolutionary scenario of an accretion disk in BBHs in which a disk eventually
becomes \"dead\", i.e., the magnetorotational instability (MRI) becomes inactive.
In their scenario, the dead disk survives until a few seconds before the merger
event. We improve the dead disk model and propose another scenario, taking
account of effects of the tidal torque from the companion and the critical
ionization degree for MRI activation more carefully. We find that the mass of
the dead disk is much lower than that in the Perna's scenario. When the binary
separation sufficiently becomes small, the tidal heating reactivates MRI and
mass accretion onto the black hole (BH). We also find that this disk \"revival\"
happens many years before the merger. The mass accretion induced by the tidal
torque increases as the separation decreases, and a relativistic jet could be
launched before the merger. The emissions from these jets are too faint
compared to GRBs, but detectable if the merger events happen within $\lesssim
10$ Mpc or if the masses of BHs are as massive as $\sim 10^5 M_{\odot}$.
</p>
{{{{ARTICLE_PARSER}}}}<p>We report interferometric measurements of [NII] 205 um fine-structure line
emission from a representative sample of three galaxies at z=5-6 using the
Atacama Large (sub)Millimeter Array (ALMA). These galaxies were previously
detected in [CII] and far-infrared continuum emission and span almost two
orders of magnitude in star formation rate (SFR). Our results show at least two
different regimes of ionized inter-stellar medium properties for galaxies in
the first billion years of cosmic time, separated by their L_[CII]/L_[NII]
ratio. We find extremely low [NII] emission compared to [CII] (L_
[CII]/L_[NII]=68 [+200/-28]) from a \"typical\" L*_UV star-forming galaxy, likely
directly or indirectly (by its effect on the radiation field) related to low
dust abundance and low metallicity. The infrared-luminous modestly star-forming
Lyman Break Galaxy (LBG) in our sample is characterized by an ionized-gas
fraction (L_[CII]/L_[NII]&lt;=20) typical of local star-forming galaxies and shows
evidence for spatial variations in its ionized-gas fraction across an extended
gas reservoir. The extreme SFR, warm and compact dusty starburst AzTEC-3 shows
an ionized fraction higher than expected given its star-formation rate surface
density (L_[CII]/L_[NII]=22+/-8) suggesting that [NII] dominantly traces a
diffuse ionized medium rather than star-forming HII regions in this type of
galaxy. This highest redshift sample of [NII] detections provides some of the
first constraints on ionized and neutral gas modeling attempts and on the
structure of the inter-stellar medium at z=5-6 in \"normal\" galaxies and
starbursts.
</p>
{{{{ARTICLE_PARSER}}}}<p>Two energetic hard X-ray bursts have recently triggered the Fermi and Swift
space observatories from the rotation powered pulsar, PSR J1119-6127. We have
performed in depth spectral and temporal analyses of these two events. Our
extensive searches in both observatory data for lower luminosity bursts
uncovered 10 additional events from the source. We report here on the timing
and energetics of the 12 bursts from PSR J1119-6127 during its burst active
phase of 2016 July 26 and 28. We also found a spectral softer X-ray flux
enhancement in a post burst episode, which shows evidence of cooling. We
discuss here the implications of these results on the nature of this unusual
high-field radio pulsar, which firmly place it within the typical magnetar
population.
</p>
{{{{ARTICLE_PARSER}}}}<p>As of 2023, the low-frequency part of the Square Kilometre Array will go
online in Australia. It will constitute the largest and most powerful
low-frequency radio-astronomical observatory to date, and will facilitate a
rich science programme in astronomy and astrophysics. With modest engineering
changes, it will also be able to measure cosmic rays via the radio emission
from extensive air showers. The extreme antenna density and the homogeneous
coverage provided by more than 60,000 antennas within an area of one km$^2$
will push radio detection of cosmic rays in the energy range around 10$^{17}$
eV to ultimate precision, with superior capabilities in the reconstruction of
arrival direction, energy, and an expected depth-of-shower-maximum resolution
of 6~g/cm${^2}$.
</p>
{{{{ARTICLE_PARSER}}}}<p>The connection between nuclear fusion in the Sun's core and solar irradiance
is obscured among other things by uncertainty over the mechanism of coronal
heating. Data for solar wind density and velocity, sunspot number, and EUV flux
suggest that electromagnetic energy from the Sun's convection zone is converted
by induction through the chromosphere into thermal energy. The helium and
hydrogen mixture exhaled by the Sun is then heated by the inverse Joule-Thomson
effect when it expands via the corona into space. The almost complete shutdown
of the solar wind on 10-11 May 1999 demonstrated that its velocity is a more
faithful indicator of solar activity than are sunspots as it reflects
short-term variations in coronal heating rather than quasicyclical fluctuations
in the Sun's magnetism. Its reconstruction from the cosmic ray flux using
isotopes spanning over 800,000 yr should therefore benefit the analysis and
long-term forecasting of Earth and space weather.
</p>
{{{{ARTICLE_PARSER}}}}<p>The national space programs have an historic opportunity to help solve the
global-scale economic and environmental problems of Earth while becoming more
effective at science through the use of space resources. Space programs will be
more cost-effective when they work to establish a supply chain in space, mining
and manufacturing then replicating the assets of the supply chain so it grows
to larger capacity. This has become achievable because of advances in robotics
and artificial intelligence. It is roughly estimated that developing a lunar
outpost that relies upon and also develops the supply chain will cost about 1/3
or less of the existing annual budgets of the national space programs. It will
require a sustained commitment of several decades to complete, during which
time science and exploration become increasingly effective. At the end, this
space industry will capable of addressing global-scale challenges including
limited resources, clean energy, economic development, and preservation of the
environment. Other potential solutions, including nuclear fusion and
terrestrial renewable energy sources, do not address the root problem of our
limited globe and there are real questions whether they will be inadequate or
too late. While industry in space likewise cannot provide perfect assurance, it
is uniquely able to solve the root problem, and it gives us an important chance
that we should grasp. What makes this such an historic opportunity is that the
space-based solution is obtainable as a side-benefit of doing space science and
exploration within their existing budgets. Thinking pragmatically, it may take
some time for policymakers to agree that setting up a complete supply chain is
an achievable goal, so this paper describes a strategy of incremental progress.
</p>
{{{{ARTICLE_PARSER}}}}<p>The mean-field dynamo model is employed to study the non-linear dynamo
regimes in a fully convective star of mass 0.3$M_{\odot}$ rotating with period
of 10 days. For the intermediate value of the parameter of the turbulent
magnetic Prandl number, $Pm_{T}=3$ we found the oscillating dynamo regimes with
period about 40Yr. The higher $Pm_{T}$ results to longer dynamo periods. If the
large-scale flows is fixed we find that the dynamo transits from axisymmetric
to non-axisymmetric regimes for the overcritical parameter of the
$\alpha$effect. The change of dynamo regime occurs because of the
non-axisymmetric non-linear $\alpha$-effect. The situation persists in the
fully non-linear dynamo models with regards of the magnetic feedback on the
angular momentum balance and the heat transport in the star. It is found that
the large-scale magnetic field quenches the latitudinal shear in the bulk of
the star. However, the strong radial shear operates in the subsurface layer of
the star. In the nonlinear case the profile of the angular velocity inside the
star become close to the spherical surfaces. This supports the equator-ward
migration of the axisymmetric magnetic field dynamo waves. It was found that,
the magnetic configuration of the star dominates by the regular
non-axisymmetric mode m=1, forming Yin Yang magnetic polarity pattern with the
strong (&gt;500 G) poloidal magnetic field in polar regions.
</p>
{{{{ARTICLE_PARSER}}}}<p>23 giant flares from 13 active stars (eight RS CVn systems, one Algol system,
three dMe stars and one YSO) were detected during the first two years of our
all-sky X-ray monitoring with the gas propotional counters (GSC) of the Monitor
of All-sky X-ray Image (MAXI). The observed parameters of all of these MAXI/GSC
flares are found to be at the upper ends for stellar flares with the luminosity
of 10^(31-34) ergs s-1 in the 2-20 keV band, the emission measure of 10^(54-57)
cm-3, the e-folding time of 1 hour to 1.5 days, and the total radiative energy
released during the flare of 10^(34-39) ergs. Notably, the peak X-ray
luminosity of 5(3-9)*10^33 ergs s-1 in the 2-20 keV band was detected in one of
the flares on II Peg, which is one of the, or potentially the, largest ever
observed in stellar flares. X-ray flares were detected from GT Mus, V841 Cen,
SZ Psc, and TWA-7 for the first time in this survey. Whereas most of our
detected sources are multiple-star systems, two of them are single stars (YZ
CMi and TWA-7). Among the stellar sources within 100 pc distance, the MAXI/GSC
sources have larger rotation velocities than the other sources. This suggests
that the rapid rotation velocity may play a key role in generating large
flares. Combining the X-ray flare data of nearby stars and the sun, taken from
literature and our own data, we discovered a universal correlation of
tau~L_X^0.2 for the flare duration tau and the intrinsic X-ray luminosity L_X
in the 0.1-100 keV band, which holds for 5 and 12 orders of magnitude in tau
and L_X, respectively. The MAXI/GSC sample is located at the highest ends on
the correlation.
</p>
{{{{ARTICLE_PARSER}}}}<p>The muon content of extensive air showers is an observable sensitive to the
primary composition and to the hadronic interaction properties. The Pierre
Auger Observatory uses water-Cherenkov detectors to measure particle densities
at the ground and therefore is sensitive to the muon content of air showers. We
present here a method which allows us to estimate the muon production depths by
exploiting the measurement of the muon arrival times at the ground recorded
with the Surface Detector of the Pierre Auger Observatory. The analysis is
performed in a large range of zenith angles, thanks to the capability of
estimating and subtracting the electromagnetic component, and for energies
between $10^{19.2}$ and $10^{20}$ eV.
</p>
{{{{ARTICLE_PARSER}}}}<p>Using an effective one body approach we describe in detail gravitational
waves from classical three body problem on a non-rotating straight line and
derive their basic physical characteristics. Special attention is paid to the
irregular motions of such systems and to the significance of double and triple
collisions. The conclusive role of the collinear solutions is also discussed in
short. It is shown that the residuals may contain information about irregular
motion of the source of gravitational waves.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present new, empirical measurements of the radii of 132 stars that host
transiting planets. These stellar radii are determined using only direct
observables---the bolometric flux at Earth, the stellar effective temperature,
and the parallax newly provided by the Gaia first data release---and thus are
virtually model independent, extinction being the only free parameter. We also
determine each star's mass using our newly determined radius and the stellar
density, itself a virtually model independent quantity from the previously
published transit analysis. The newly determined stellar radii and masses are
in turn used to re-determine the transiting planet radii and masses, once again
using only direct observables. The uncertainties on the stellar radii and
masses are typically 8% and 30%, respectively, and the resulting uncertainties
on the planet radii and masses are 9% and 22%, respectively. These accuracies
are generally larger than the previously published model-dependent precisions
of 5% and 6% on the planet radii and masses, respectively, but the newly
determined values are purely empirical. We additionally report stellar radii
for 366 stars that host radial-velocity (non-transiting) planets, with a
typical achieved accuracy in the radii of 2%. Most importantly, the stellar
bolometric fluxes and angular radii reported here---with typical accuracies of
1.7% and 1.8%, respectively---will serve as a fundamental data set to permit
the re-determination of the planet radii and masses with the {\it Gaia\} second
data release to 3% and 5% accuracy, comparable to or better than currently
published precisions, but in an entirely empirical fashion.
</p>
{{{{ARTICLE_PARSER}}}}<p>We use the Gaia data release 1 (DR1) to study the proper motion (PM) fields
of the Large and Small Magellanic Clouds (LMC, SMC). This uses the Tycho-Gaia
Astrometric Solution (TGAS) PMs for 29 Hipparcos stars in the LMC and 8 in the
SMC. The LMC PM in the West and North directions is inferred to be
$(\mu_W,\mu_N) = (-1.874 \pm 0.039, 0.223 \pm 0.049)$ mas/yr, and the SMC PM
$(\mu_W,\mu_N) = (-0.876 \pm 0.060, -1.227 \pm 0.042)$ mas/yr. These results
have similar accuracy and agree to within the uncertainties with existing
Hubble Space Telescope (HST) PM measurements. Since TGAS uses different methods
with different systematics, this provides an external validation of both data
sets and their underlying approaches. Residual DR1 systematics may affect the
TGAS results, but the HST agreement implies this must be below the random
errors. Also in agreement with prior HST studies, the TGAS LMC PM field clearly
shows the clockwise rotation of the disk, even though it takes the LMC disk in
excess of $10^8$ years to complete one revolution. The implied rotation curve
amplitude for young LMC stars is consistent with that inferred from
line-of-sight (LOS) velocity measurements. Comparison of the PM and LOS
rotation curves implies a kinematic LMC distance modulus $m-M = 18.53 \pm
0.42$, consistent but not yet competitive with photometric methods. These first
results from Gaia on the topic of Local Group (LG) dynamics provide an
indication of how its future data releases will revolutionize this field.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present spatially-resolved two-dimensional stellar kinematics for the 41
most massive early-type galaxies (MK &lt;~ -25.7 mag, stellar mass M* &gt;~ 10^11.8
Msun) of the volume-limited (D &lt; 108 Mpc) MASSIVE survey. For each galaxy, we
obtain high-quality spectra in the wavelength range of 3650 to 5850 A from the
246-fiber Mitchell integral-field spectrograph (IFS) at McDonald Observatory,
covering a 107\" x 107\" field of view (often reaching 2 to 3 effective radii).
We measure the 2-D spatial distribution of each galaxy's angular momentum
(lambda and fast or slow rotator status), velocity dispersion (sigma), and
higher-order non-Gaussian velocity features (Gauss-Hermite moments h3 to h6).
Our sample contains a high fraction (~80% ) of slow and non-rotators with
lambda &lt;~ 0.2. When combined with the lower-mass ETGs in the ATLAS3D survey, we
find the fraction of slow-rotators to increase dramatically with galaxy mass,
reaching ~50% at MK ~ -25.5 mag and ~90% at MK &lt;~ -26 mag. All of our fast
rotators show a clear anti-correlation between h3 and V/sigma, and the slope of
the anti-correlation is steeper in more round galaxies. The radial profiles of
sigma show a clear luminosity and environmental dependence: the 12 most
luminous galaxies in our sample (MK &lt;~ -26 mag) are all brightest cluster/group
galaxies (except NGC 4874) and all have rising or nearly flat sigma profiles,
whereas five of the seven \"isolated\" galaxies are all fainter than MK = -25.8
mag and have falling sigma. All of our galaxies have positive average h4; the
most luminous galaxies have average h4 ~ 0.05 while less luminous galaxies have
a range of values between 0 and 0.05. Most of our galaxies show positive radial
gradients in h4, and those galaxies also tend to have rising sigma profiles. We
discuss the implications for the relationship among dynamical mass, sigma, h4,
and velocity anisotropy for these massive galaxies.
</p>
{{{{ARTICLE_PARSER}}}}<p>We have resolved the scatter-broadened image of PSR B0329+54 and detected
substructure within it. These results are not influenced by any extended
structure of a source but instead are directly attributed to the interstellar
medium. We obtained these results at 324 MHz with the ground-space
interferometer RadioAstron which included the space radio telescope (SRT),
ground-based Westerbork Synthesis Radio Telescope and 64-m Kalyazin Radio
Telescope on baseline projections up to 330,000 km in 2013 November 22 and 2014
January 1 to 2. At short 15,000 to 35,000 km ground-space baseline projections
the visibility amplitude decreases with baseline length providing a direct
measurement of the size of the scattering disk of 4.8$\pm$0.8 mas. At longer
baselines no visibility detections from the scattering disk would be expected.
However, significant detections were obtained with visibility amplitudes of 3
to 5% of the maximum scattered around a mean and approximately constant up to
330,000 km. These visibilities reflect substructure from scattering in the
interstellar medium and offer a new probe of ionized interstellar material. The
size of the diffraction spot near Earth is 17,000$\pm$3,000 km. With the
assumption of turbulent irregularities in the plasma of the interstellar
medium, we estimate that the effective scattering screen is located 0.6$\pm$0.1
of the distance from Earth toward the pulsar.
</p>
{{{{ARTICLE_PARSER}}}}<p>Anisotropic flow is a sensitive probe of the initial conditions and the
transport properties of the Quark Gluon Plasma (QGP) produced in heavy-ion
collisions. In these proceedings, we present the first results of elliptic
($v_2$), triangular ($v_3$) and quadrangular flow ($v_4$) of charged particles
in Pb--Pb collisions at $\sqrt{s_{_{\rm NN}}}=$ 5.02 TeV with the ALICE
detector. In addition, the comparison of experimental measurements to various
theoretical calculations will be discussed. This provides a unique opportunity
to test the validity of the hydrodynamic picture and discriminates between
various possibilities for the temperature dependence of shear viscosity to
entropy density ratio of the produced QGP.
</p>
{{{{ARTICLE_PARSER}}}}<p>The Dalitz decays eta -&gt; e^+e^-g and omega -&gt; pi^0 e^+e^- have been measured
in the g p -&gt; eta p and g p -&gt; omega p reactions, respectively, with the A2
tagged-photon facility at the Mainz Microtron, MAMI. The value obtained for the
slope parameter of the electromagnetic transition form factor of eta,
Lambda^{-2}_eta=(1.97+/-0.13_tot) GeV^{-2}, is in good agreement with previous
measurements of the eta -&gt; e^+e^-g and eta -&gt; mu^+mu^-g decays. The uncertainty
obtained in the value of Lambda^{-2}_eta is lower than in previous results
based on the eta -&gt; e^+e^-g decay. The value obtained for the omega slope
parameter, Lambda^{-2}_omega_pi^0 = (1.99+/-0.22_tot) GeV^{-2}, is somewhat
lower than previous measurements based on omega -&gt; pi^0 mu^+mu^-, but the
results for the omega transition form factor are in better agreement with
theoretical calculations, compared to earlier experiments.
</p>
{{{{ARTICLE_PARSER}}}}<p>Several topics presented and discussed at MESON2016 are highlighted,
including pentaquarks, dibaryons and meson-nuclear bound states.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this review, we summarize the theoretical development on the electric
dipole moment of light nuclei. We first describe the nucleon level CP violation
and its parametrization. We then present the results of calculations of the EDM
of light nuclei in the ab initio approach and in the cluster model. The
analysis of the effect of several models beyond standard model is presented,
together with the prospects for its discovery. The advantage of the electric
dipole moment of light nuclei is focused in the point of view of the many-body
physics. The evaluations of the nuclear electric dipole moment generated by the
$\theta$-term and by the CP phase of the Cabibbo-Kobayashi-Maskawa matrix are
also reviewed.
</p>
{{{{ARTICLE_PARSER}}}}<p>Dihadron and isolated direct photon-hadron angular correlations are measured
in $p$$+$$p$ collisions at $\sqrt{s}=510$ GeV. Correlations of charged hadrons
of $0.7&lt;p_T&lt;10$ GeV/$c$ with $\pi^0$ mesons of $4&lt;p_T&lt;15$ GeV/$c$ or isolated
direct photons of $7&lt;p_T&lt;15$ GeV/$c$ are used to study nonperturbative effects
generated by initial-state partonic transverse momentum and final-state
transverse momentum from fragmentation. The nonperturbative behavior is
characterized by measuring the out-of-plane transverse momentum component
$p_{\rm out}$ perpendicular to the axis of the trigger particle, which is the
high-$p_T$ direct photon or $\pi^0$. Nonperturbative evolution effects are
extracted from Gaussian fits to the away-side inclusive-charged-hadron yields
for different trigger-particle transverse momenta ($p_T^{\rm trig}$). The
Gaussian widths and root mean square of $p_{\rm out}$ are reported as a
function of the interaction hard scale $p_T^{\rm trig}$ to investigate possible
transverse-momentum-dependent evolution differences between the $\pi^0$-h$^\pm$
and direct photon-h$^\pm$ correlations and factorization breaking effects. The
widths are found to decrease with $p_T^{\rm trig}$, which indicates that the
Collins-Soper-Sterman soft factor is not driving the evolution with the hard
scale in nearly back-to-back dihadron and direct photon-hadron production in
$p$$+$$p$ collisions. This behavior is in contrast to Drell-Yan and
semi-inclusive deep-inelastic scattering measurements.
</p>
{{{{ARTICLE_PARSER}}}}<p>There exist four pion nucleon coupling constants, $f_{\pi^0, pp}$,
$-f_{\pi^0, nn}$, $f_{\pi^+, pn} /\sqrt{2}$ and $ f_{\pi^-, np} /\sqrt{2}$
which coincide when up and down quark masses are identical and the electron
charge is zero. While there is no reason why the pion-nucleon-nucleon coupling
constants should be identical in the real world, one expects that the small
differences might be pinned down from a sufficiently large number of
independent and mutually consistent data. Our discussion provides a rationale
for our recent determination $$f_p^2 = 0.0759(4) \, , \quad f_{0}^2 = 0.079(1)
\,, \quad f_{c}^2 = 0.0763(6) \, , $$ based on a partial wave analysis of the
$3\sigma$ self-consistent nucleon-nucleon Granada-2013 database comprising 6713
published data in the period 1950-2013.
</p>
{{{{ARTICLE_PARSER}}}}<p>Currently, radiobiology experiments using heavy ions at GANIL(Grand
Accelerateur National d Ions Lourds) are conducted under the supervision of the
CIMAP (Center for research on Ions, MAterials and Photonics). In this context,
a new beam monitoring equipment named DOSION has been developed. It allows to
perform measurements of accurate fluence and dose maps in near real time for
each biological sample irradiated. In this paper, we present the detection
system, its design, performances, calibration protocol and measurements
performed during radiobiology experiments. This setup is currently available
for any radiobiology experiments if one wishes to correlate one s own sample
analysis to state of the art dosimetric references.
</p>
{{{{ARTICLE_PARSER}}}}<p>Static properties of hadrons such as their radii and other moments of the
electric and magnetic distributions can only be extracted using theoretical
methods and not directly measured from experiments. As a result, discrepancies
between the extracted values from different precision measurements can exist.
The proton charge radius, $r_p$, which is either extracted from electron proton
elastic scattering data or from hydrogen atom spectroscopy seems to be no
exception. The value $r_p = 0.84087(39)$ fm extracted from muonic hydrogen
spectroscopy is about 4% smaller than that obtained from electron proton
scattering or standard hydrogen spectroscopy. The resolution of this so called
proton radius puzzle has been attempted in many different ways over the past
six years. The present article reviews these attempts with a focus on the
methods of extracting the radius.
</p>
{{{{ARTICLE_PARSER}}}}<p>Anisotropic flow is a sensitive probe of the initial conditions and the
transport properties of the Quark Gluon Plasma (QGP) produced in heavy-ion
collisions. In these proceedings, we present the first results of elliptic
($v_2$), triangular ($v_3$) and quadrangular flow ($v_4$) of charged particles
in Pb--Pb collisions at $\sqrt{s_{_{\rm NN}}}=$ 5.02 TeV with the ALICE
detector. In addition, the comparison of experimental measurements to various
theoretical calculations will be discussed. This provides a unique opportunity
to test the validity of the hydrodynamic picture and discriminates between
various possibilities for the temperature dependence of shear viscosity to
entropy density ratio of the produced QGP.
</p>
{{{{ARTICLE_PARSER}}}}<p>Due to its dominance in the low energy eta-nucleon interaction, the S11
N$^*$(1535) resonance enters as an important ingredient in the analyses of
experiments aimed at finding evidence for the existence of eta-mesic nuclei.
The static properties of the resonance get modified inside the nucleus and its
momentum distribution is used in deciding these properties as well as the
kinematics in the analyses. Here we show that given the possibility for the
existence of an N$^*$-$^3$He quasibound state, the relative momentum
distribution of an N$^*$ and $^3$He inside such a $^4$He is narrower than that
of neutron-$^3$He in $^4$He. Results for the N$^*$-$^{24}$Mg system are also
presented. The present exploratory work could be useful in motivating searches
of exotic N$^*$-nucleus quasibound states as well as in performing analyses of
eta meson production data.
</p>
{{{{ARTICLE_PARSER}}}}<p>We compute (pseudo)critical temperature, $T_c$, of chiral symmetry
restoration for quark matter in the background of parallel electric and
magnetic fields. This field configuration leads to the production of a chiral
medium on a time scale $\tau$, characterized by a nonvanishing value of the
chiral density that equilibrates due to microscopic processes in the thermal
bath. We estimate the relaxation time $\tau$ to be about $\approx 0.1-1$ fm/c
around the chiral crossover; then we compute the effect of the fields and of
the chiral medium on~$T_c$. We find $T_c$ to be lowered by the external fields
in the chiral medium.
</p>
{{{{ARTICLE_PARSER}}}}<p>Several topics presented and discussed at MESON2016 are highlighted,
including pentaquarks, dibaryons and meson-nuclear bound states.
</p>
{{{{ARTICLE_PARSER}}}}<p>We propose a practical effective model by introducing temperature ($T$)
dependence to the coupling strengths of four-quark and six-quark
Kobayashi-Maskawa-'t Hooft interactions in the 2+1 flavor Polyakov-loop
extended Nambu-Jona-Lasinio model. The $T$ dependence is determined from LQCD
data on the renormalized chiral condensate around the pseudocritical
temperature $T_c^{\chi}$ of chiral crossover and the screening-mass difference
between $\pi$ and $a_0$ mesons in $T &gt; 1.1T_c^\chi$ where only the $U(1)_{\rm
A}$-symmetry breaking survives. The model well reproduces LQCD data on
screening masses $M_{\xi}^{\rm scr}(T)$ for both scalar and pseudoscalar
mesons, particularly in $T \ge T_c^{\chi}$. Using this effective model, we
predict meson pole masses $M_{\xi}^{\rm pole}(T)$ for scalar and pseudoscalar
mesons. For $\eta'$ meson, the prediction is consistent with the experimental
value at finite $T$ measured in heavy-ion collisions. We point out that the
relation $M_{\xi}^{\rm scr}(T)-M_{\xi}^{\rm pole}(T) \approx M_{\xi'}^{\rm
scr}(T)-M_{\xi'}^{\rm pole}(T)$ is pretty good when $\xi$ and $\xi'$ are scalar
mesons, and show that the relation $M_{\xi}^{\rm scr}(T)/M_{\xi'}^{\rm scr}(T)
\approx M_{\xi}^{\rm pole}(T)/M_{\xi'}^{\rm pole}(T)$ is well satisfied within
20% error when $\xi$ and $\xi'$ are pseudoscalar mesons and also when $\xi$ and
$\xi'$ are scalar mesons.
</p>
{{{{ARTICLE_PARSER}}}}<p>We determine the strong couplings of three mesons that involve, at least, one
$\eta_c$ or $J/\psi$ meson, within the framework of a constituent-quark model
by means of relativistic dispersion formulations. For strong couplings of
$J/\psi$ mesons to two charmed mesons, our approach leads to predictions
roughly twice as large as those arising from QCD sum rules.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this review, we summarize the theoretical development on the electric
dipole moment of light nuclei. We first describe the nucleon level CP violation
and its parametrization. We then present the results of calculations of the EDM
of light nuclei in the ab initio approach and in the cluster model. The
analysis of the effect of several models beyond standard model is presented,
together with the prospects for its discovery. The advantage of the electric
dipole moment of light nuclei is focused in the point of view of the many-body
physics. The evaluations of the nuclear electric dipole moment generated by the
$\theta$-term and by the CP phase of the Cabibbo-Kobayashi-Maskawa matrix are
also reviewed.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study the transition to hydrodynamics in a weakly-coupled model of
quark-gluon plasma given by kinetic theory in the relaxation time
approximation. Our studies uncover qualitative similarities to the results on
hydrodynamization in strongly coupled gauge theories. In particular, we
demonstrate that the gradient expansion in this model has vanishing radius of
convergence. The asymptotic character of the hydrodynamic gradient expansion is
crucial for the recently discovered applicability of hydrodynamics at large
gradients. Furthermore, the analysis of the resurgent properties of the series
provides, quite remarkably, indication for the existence of a novel transient,
damped oscillatory mode of expanding plasmas in kinetic theory.
</p>
{{{{ARTICLE_PARSER}}}}<p>The $\eta^\prime$ transition form factor is reanalyzed in view of the recent
BESIII first observation of the Dalitz decay $\eta^\prime\to\gamma e^+e^-$ in
both space- and time-like regions at low and intermediate energies using the
Pad\'e approximants method. The present analysis provides a suitable
parameterization for reproducing the measured form factor in the whole energy
region and allows to extract the corresponding low-energy parameters together
with a prediction of its values at the origin, related to
$\Gamma_{\eta^\prime\to\gamma\gamma}$, and the asymptotic limit. The
$\eta$-$\eta^\prime$ mixing is reassessed within a mixing scheme compatible
with the large-$N_c$ chiral perturbation theory at next-to-leading order, with
particular attention to the OZI-rule--violating parameters. The $J/\psi$,
$Z\to\eta^{(\prime)}\gamma$ decays are also considered and predictions
reported.
</p>
{{{{ARTICLE_PARSER}}}}<p>We rederive relativistic hydrodynamics as a Lagrangian effective theory using
the doubled coordinates technique, allowing us to include dissipative terms. We
include Navier-Stokes shear and bulk terms, as well as Israel-Stewart
relaxation time terms, within this formalism. We show how the inclusion of
shear viscosity, and the requirement of a bounded energy-momentum \"vacuum\",
forces the inclusion of the Israel-Stewart term into the theory, thereby
providing a justification for the origin and uniqueness of these terms.
</p>
{{{{ARTICLE_PARSER}}}}<p>There exist four pion nucleon coupling constants, $f_{\pi^0, pp}$,
$-f_{\pi^0, nn}$, $f_{\pi^+, pn} /\sqrt{2}$ and $ f_{\pi^-, np} /\sqrt{2}$
which coincide when up and down quark masses are identical and the electron
charge is zero. While there is no reason why the pion-nucleon-nucleon coupling
constants should be identical in the real world, one expects that the small
differences might be pinned down from a sufficiently large number of
independent and mutually consistent data. Our discussion provides a rationale
for our recent determination $$f_p^2 = 0.0759(4) \, , \quad f_{0}^2 = 0.079(1)
\,, \quad f_{c}^2 = 0.0763(6) \, , $$ based on a partial wave analysis of the
$3\sigma$ self-consistent nucleon-nucleon Granada-2013 database comprising 6713
published data in the period 1950-2013.
</p>
{{{{ARTICLE_PARSER}}}}<p>Coherent states are required to form a complete set of vectors in the Hilbert
space by providing the resolution of identity. We study the completeness of
coherent states for two different models in a noncommutative space associated
with the generalised uncertainty relation by finding the resolution of unity
with a positive definite weight function. The weight function, which is
sometimes known as the Borel measure, is obtained through explicit analytic
solutions of the Stieltjes and Hausdorff moment problem with the help of the
standard techniques of inverse Mellin transform.
</p>
{{{{ARTICLE_PARSER}}}}<p>We establish a Minkowski measurability criterion for a large class of
relative fractal drums (or, in short, RFDs), in Euclidean spaces of arbitrary
dimension in terms of their complex dimensions, which are defined as the poles
of their associated fractal zeta functions. Relative fractal drums represent a
far-reaching generalization of bounded subsets of Euclidean spaces as well as
of fractal strings studied extensively by the first author and his
collaborators. In fact, the Minkowski measurability criterion established here
is a generalization of the corresponding one obtained for fractal strings by
the first author and M.\ van Frankenhuijsen. Similarly as in the case of
fractal strings, the criterion established here is formulated in terms of the
locations of the principal complex dimensions associated with the relative drum
under consideration. These complex dimensions are defined as poles or, more
generally, singularities of the corresponding distance (or tube) zeta function.
We also reflect on the notion of gauge-Minkowski measurability of RFDs and
establish several results connecting it to the nature and location of the
complex dimensions. (This is especially useful when the underlying scaling does
not follow a classic power law.) We illustrate our results and their
applications by means of a number of interesting examples.
</p>
{{{{ARTICLE_PARSER}}}}<p>We give a non-perturbative construction of a distinguished state for the
quantized Dirac field in Minkowski space in the presence of a time-dependent
external field of the form of a plane electromagnetic wave. By explicit
computation of the fermionic signature operator, it is shown that the Dirac
operator has the strong mass oscillation property. We prove that the resulting
fermionic projector state is a Hadamard state.
</p>
{{{{ARTICLE_PARSER}}}}<p>In the paper, we make use of Manton's analytical method to investigate the
force between kink and the anti-kink with large distance in $1+1$ dimensional
field theory. The related potential has infinite order corrections of
exponential pattern, and coefficients for each order are determined. These
coefficients can also be obtained by solving the equation of the fluctuation
around the vacuum. At the lowest order, the kink lattice represents the Toda
lattice. With higher order correction terms, the kink lattice can represent one
kind of the generic Toda lattice. With only two sites, the kink lattice is
classically integrable. If the number of sites of the lattice is larger than
two, the kink lattice is not integrable but a near integrable system. We take
use of the Flaschka's variables to study the Lax pair of the kink lattice.
These Flaschka's variables have interesting algebraic relations and the
non-integrability can be manifested. We also discussed the higher Hamiltonians
for the deformed open Toda lattice, which has a similar result as the ordinary
deformed Toda.
</p>
{{{{ARTICLE_PARSER}}}}<p>Random motions on the line and on the plane with space-varying velocities are
considered and analyzed in this paper.
</p>
<p>On the line we investigate symmetric and asymmetric telegraph processes with
space-dependent velocities and we are able to present the explicit distribution
of the position $\mathcal{T}(t)$, $t&gt;0$, of the moving particle.
</p>
<p>Also the case of a non-homogeneous Poisson process (with rate $\lambda =
\lambda(t)$) governing the changes of direction is analyzed in three specific
cases. For the special case $\lambda(t)= \alpha/t$ we obtain a random motion
related to the Euler-Poisson-Darboux (EPD) equation which generalizes the
well-known case treated e.g. in Foong and Van Kolck (1992), Garra and Orsingher
(2016) and Rosencrans (1973).
</p>
<p>A EPD--type fractional equation is also considered and a parabolic solution
(which in dimension $d=1$ has the structure of a probability density) is
obtained.
</p>
<p>Planar random motions with space--varying velocities and infinite directions
are finally analyzed in Section 5. We are able to present their explicit
distributions and for polynomial-type velocity structures we obtain the hyper
and hypo-elliptic form of their support (of which we provide a picture).
</p>
{{{{ARTICLE_PARSER}}}}<p>A relationship between the discrete Dirac-K\\"{a}hler equation and discrete
analogues of some Dirac type equations in the geometric spacetime algebra is
discussed. We show that a solution of the discrete Dirac-K\\"{a}hler equation
can be represented as the sum and difference of solutions of discrete Dirac
type equations with the corresponding sign of the mass term.
</p>
{{{{ARTICLE_PARSER}}}}<p>We extend the well-known Sacks-Uhlenbeck energy gap result (1981) for
harmonic maps from closed Riemann surfaces into closed Riemannian manifolds
from the case of maps with small energy (thus near a constant map), to the case
of harmonic maps with high absolute energy but small energy relative to a
reference harmonic map.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider the many-body dynamics of fermions with Coulomb interaction in a
mean-field scaling limit where the kinetic and potential energy are of the same
order for large particle numbers. In the considered limit the spatial variation
of the mean-field is small. We prove two results about this scaling limit.
First, due to the small variation, i.e., small forces, we show that the
many-body dynamics can be approximated by the free dynamics with an appropriate
phase factor with the optimal error term. Second, we show that the Hartree-Fock
dynamics gives a better approximation with a smaller error term. Thus, the
Hartree-Fock equations can indeed be derived from the many-body dynamics with
Coulomb interaction in a mean-field scaling limit.
</p>
{{{{ARTICLE_PARSER}}}}<p>We show that when observing the range of a chordal SLE$_\kappa$ curve for
$\kappa \in (4,8)$, it is not possible to recover the order in which the points
have been visited. We also derive related results about conformal loop
ensembles (CLE): (i) The loops in a CLE$_\kappa$ for $\kappa \in (4,8)$ are not
determined by the CLE$_\kappa$ gasket. (ii) The continuum percolation
interfaces defined in the fractal carpets of conformal loop ensembles
CLE$_{\kappa}$ for $\kappa \in (8/3, 4)$ (we defined these percolation
interfaces in previous work, and showed there that they are SLE$_{16/\kappa}$
curves) are not determined by the CLE$_{\kappa}$ carpet that they are defined
in.
</p>
{{{{ARTICLE_PARSER}}}}<p>We determine the exact strong converse exponent of classical-quantum channel
coding, for every rate above the Holevo capacity. Our form of the exponent is
an exact analogue of Arimoto's, given as a transform of the Renyi capacities
with parameters alpha&gt;1. It is important to note that, unlike in the classical
case, there are many inequivalent ways to define the Renyi divergence of
states, and hence the R\'enyi capacities of channels. Our exponent is in terms
of the Renyi capacities corresponding to a version of the Renyi divergences
that has been introduced recently in [M\\"uller-Lennert, Dupuis, Szehr, Fehr and
Tomamichel, J. Math. Phys. 54, 122203, (2013)], and [Wilde, Winter, Yang,
Commun. Math. Phys. 331, (2014)]. Our result adds to the growing body of
evidence that this new version is the natural definition for the purposes of
strong converse problems.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this work we investigate a phase field model for damage processes in
two-dimensional viscoelastic media with nonhomogeneous Neumann data describing
external boundary forces. In the first part we establish global-in-time
existence, uniqueness, a priori estimates and continuous dependence of strong
solutions on the data. The main difficulty is caused by the irreversibility of
the phase field variable which results in a constrained PDE system. In the last
part we consider an optimal control problem where a cost functional penalizes
maximal deviations from prescribed damage profiles. The goal is to minimize the
cost functional with respect to exterior forces acting on the boundary which
play the role of the control variable in the considered model. To this end, we
prove existence of minimizers and study a family of \"local\" approximations via
adapted cost functionals.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study meromorphic extensions of distance and tube zeta functions, as well
as of geometric zeta functions of fractal strings. The distance zeta function
$\zeta_A(s):=\int_{A_\delta} d(x,A)^{s-N}\mathrm{d}x$, where $\delta&gt;0$ is
fixed and $d(x,A)$ denotes the Euclidean distance from $x$ to $A$ extends the
definition of the zeta function associated with bounded fractal strings to
arbitrary bounded subsets $A$ of $\mathbb{R}^N$. The abscissa of Lebesgue
convergence $D(\zeta_A)$ coincides with $D:=\overline\dim_BA$, the upper box
dimension of $A$. The complex dimensions of $A$ are the poles of the
meromorphic continuation of the fractal zeta function of $A$ to a suitable
connected neighborhood of the \"critical line\" $\{\Re(s)=D\}$. We establish
several meromorphic extension results, assuming some suitable information about
the second term of the asymptotic expansion of the tube function $|A_t|$ as
$t\to0^+$, where $A_t$ is the Euclidean $t$-neighborhood of $A$. We pay
particular attention to a class of Minkowski measurable sets, such that
$|A_t|=t^{N-D}(\mathcal M+O(t^\gamma))$ as $t\to0^+$, with $\gamma&gt;0$, and to a
class of Minkowski nonmeasurable sets, such that $|A_t|=t^{N-D}(G(\log
t^{-1})+O(t^\gamma))$ as $t\to0^+$, where $G$ is a nonconstant periodic
function and $\gamma&gt;0$. In both cases, we show that $\zeta_A$ can be
meromorphically extended (at least) to the open right half-plane
$\{\Re(s)&gt;D-\gamma\}$. Furthermore, up to a multiplicative constant, the
residue of $\zeta_A$ evaluated at $s=D$ is shown to be equal to $\mathcal M$
(the Minkowski content of $A$) and to the mean value of $G$ (the average
Minkowski content of $A$), respectively. Moreover, we construct a class of
fractal strings with principal complex dimensions of any prescribed order, as
well as with an infinite number of essential singularities on the critical line
$\{\Re(s)=D\}$.
</p>
{{{{ARTICLE_PARSER}}}}<p>The paper is devoted to hyperbolic (generally speaking, non-Lagrangian and
nonlinear) partial differential systems possessing a full set of differential
operators that map any function of one independent variable into a symmetry of
the corresponding system. We demonstrate that a system has the above property
if and only if this system admits a full set of formal integrals (i.e.
differential operators which map symmetries into integrals of the system). As a
consequence, such systems possess both direct and inverse Noether operators (in
the terminology of a work by B. Fuchssteiner and A.S. Fokas who have used these
terms for operators that map cosymmetries into symmetries and back). Systems
admitting Noether operators are not exhausted by Euler-Lagrange systems and the
systems with formal integrals. In particular, a hyperbolic system admits an
inverse Noether operator if a differential substitution maps this system into a
system possessing an inverse Noether operator.
</p>
{{{{ARTICLE_PARSER}}}}<p>We prove that the Kontsevich tetrahedral flow $\dot{\mathcal{P}} =
\mathcal{Q}_{a:b} (\mathcal{P})$, the right-hand side of which is a linear
combination of two differential monomials of degree four in a bi-vector
$\mathcal{P}$ on an affine real Poisson manifold $N^n$, does infinitesimally
preserve the space of Poisson bi-vectors on $N^n$ if and only if the two
monomials in $\mathcal{Q}_{a:b} (\mathcal{P})$ are balanced by the ratio
$a:b=1:6$.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this work we introduce the contact Heisenberg algebra which is the
restriction of the Jacobi algebra on contact manifolds to the linear and
constant functions. We give the exact expression of its corresponding
Baker-Campbell-Hausdorff formula. We argue that this result is relevant to the
quantization of contact systems.
</p>
{{{{ARTICLE_PARSER}}}}<p>We introduce a new family of $N\times N$ random real symmetric matrix
ensembles, the $k$-checkerboard matrices, whose limiting spectral measure has
two components which can be determined explicitly. All but $k$ eigenvalues are
in the bulk, and their behavior, appropriately normalized, converges to the
semi-circle as $N\to\infty$; the remaining $k$ are tightly constrained near
$N/k$ and their distribution converges to the $k \times k$ hollow GOE ensemble
(this is the density arising by modifying the GOE ensemble by forcing all
entries on the main diagonal to be zero). Similar results hold for complex and
quaternionic analogues. We isolate the two regimes by using matrix perturbation
results and a nonstandard weight function for the eigenvalues, then derive
their limiting distributions using a modification of the method of moments and
analysis of the resulting combinatorics.
</p>
{{{{ARTICLE_PARSER}}}}<p>It has recently been shown that some integrable spin chains possess a set of
quasilocal conserved charges, with the classic example being the
spin-$\frac{1}{2}$ XXZ Heisenberg chain. These charges have been proven to be
essential for properly describing stationary states after a quantum quench, and
must be included in the generalized Gibbs ensemble (GGE). We find that similar
charges are also necessary for the GGE description of integrable quantum field
theories with non-diagonal scattering. A stationary state in a non-diagonal
scattering theory is completely specified by fixing the mode-ocuppation density
distributions of physical particles, as well auxiliary particles which carry no
energy or momentum. We show that the set of conserved charges with integer
Lorentz spin, related to the integrability of the model, are unable to fix the
distributions of these auxiliary particles, since these charges can only fix
kinematical properties of physical particles. The field theory analogs of
quasilocal lattice charges are therefore necessary. As a concrete example, we
find the complete set of charges needed in the sine-Gordon model, by using the
fact that this field theory is recovered as the continuum limit of a spatially
inhomogeneous version of the XXZ chain. The set of quasilocal charges of the
lattice theory are shown to become a set local charges with fractional spin in
the field theory.
</p>
{{{{ARTICLE_PARSER}}}}<p>Simulating the stochastic evolution of real quantities on a digital computer
requires a trade-off between the precision to which these quantities are
approximated, and the memory required to store them. The statistical accuracy
of the simulation is thus generally limited by the internal memory available to
the simulator. Here, using tools from computational mechanics, we show that
quantum processors with a fixed finite memory can simulate stochastic processes
of real variables to arbitrarily high precision. This demonstrates a provable,
unbounded memory advantage that a quantum simulator can exhibit over its best
possible classical counterpart.
</p>
{{{{ARTICLE_PARSER}}}}<p>The emergence of a diverging length scale in many-body systems at a quantum
phase transition implies that total entanglement has to reach its maximum
there. In order to fully characterize this one has to consider multipartite
entanglement as, for instance, bipartite entanglement between individual
particles fails to signal this effect. However, quantification of multipartite
entanglement is very hard and detecting it may not be possible due to the lack
accessibility to all individual particles. For these reasons it will be more
sensible to partition the system into relevant subsystems, each containing few
to many spins, and study entanglement between those constituents as a coarse
grain picture of multipartite entanglement between individual particles. In
impurity systems, famously exemplified by two impurity and two channel Kondo
models, it is natural to divide the system into three parts, namely, impurities
and the left and right bulks. By exploiting two tripartite entanglement
measures, based on negativity, we show that at impurity quantum phase
transitions the tripartite entanglement diverges and shows scaling behavior.
While the critical exponents are different for each tripartite entanglement
measure they both provide very similar critical exponents for the two impurity
and the two channel Kondo models suggesting that they belong to the same
universality class.
</p>
{{{{ARTICLE_PARSER}}}}<p>By considering (non-relativistic) quantum mechanics as it is done in practice
in particular in condensed-matter physics, it is argued that a deterministic,
unitary time evolution within a chosen Hilbert space always has a limited
scope, leaving a lot of room for embedding the quantum-classical transition
into our current theories without recurring to difficult-to-accept
interpretations of quantum mechanics. Nonunitary projections to initial and
final states, the breaking of time-reversal symmetry, a change of Hilbert
space, and the introduction of classical concepts such as external potentials
or localized atomic nuclei are widespread in quantum mechanical calculations.
Furthermore, quantum systems require classical environments that enable the
symmetry breaking that is necessary for creating the atomic configurations of
molecules and crystals. This paper argues that such classical environments are
provided by finite-temperature macroscopic systems in which the range of
quantum correlations and entanglement is limited. This leads to classical
behavior on larger scales, and to collapse-like events in all dynamical
processes that become coupled to the thermalized degrees of freedom.
</p>
{{{{ARTICLE_PARSER}}}}<p>Coherent states are required to form a complete set of vectors in the Hilbert
space by providing the resolution of identity. We study the completeness of
coherent states for two different models in a noncommutative space associated
with the generalised uncertainty relation by finding the resolution of unity
with a positive definite weight function. The weight function, which is
sometimes known as the Borel measure, is obtained through explicit analytic
solutions of the Stieltjes and Hausdorff moment problem with the help of the
standard techniques of inverse Mellin transform.
</p>
{{{{ARTICLE_PARSER}}}}<p>We investigate whether small perturbations can cause relaxation to quantum
equilibrium over very long timescales. We consider in particular a
two-dimensional harmonic oscillator, which can serve as a model of a field mode
on expanding space. We assume an initial wave function with small perturbations
to the ground state. We present evidence that the trajectories are highly
confined so as to preclude relaxation to equilibrium even over very long
timescales. Cosmological implications are briefly discussed.
</p>
{{{{ARTICLE_PARSER}}}}<p>We report on a planar microwave resonator providing arbitrarily polarized
oscillating magnetic fields that enable selective excitation of the electronic
spins of nitrogen-vacancy (NV) centers in diamond. The polarization plane is
parallel to the surface of diamond, which makes the resonator fully compatible
with (111)-oriented diamond. The field distribution is spatially uniform in a
circular area with a diameter of 4 mm, and a near-perfect circular polarization
is achieved. We also demonstrate that the original resonance frequency of 2.8
GHz can be varied in the range of 2-3.2 GHz by introducing varactor diodes that
serve as variable capacitors.
</p>
{{{{ARTICLE_PARSER}}}}<p>We have verified a mechanism for Raman excitation of atoms through continuum
levels previously obtained by quantum optimal control using the
multi-configurational time-dependent Hartree-Fock (MCTDHF) method. This
mechanism, which was obtained at the time-dependent configuration interaction
singles (TDCIS) level of theory, involves sequentially exciting an atom from
the ground state to an intermediate core-hole state using a long pump pulse,
and then transferring this population to the target Raman state with a shorter
Stokes pulse. This process represents the first step in a multidimensional
x-ray spectroscopy scheme that will provide a local probe of valence electronic
correlations. Although at the optimal pulse intensities at the TDCIS level of
theory the MCTDHF method predicts multiple ionization of the atom, at slightly
lower intensities (reduced by a factor of about 4) the TDCIS mechanism is shown
to hold qualitatively. Quantitatively, the MCTDHF populations are reduced from
the TDCIS calculations by a factor of 4.
</p>
{{{{ARTICLE_PARSER}}}}<p>Engineering a sensor system for detecting an extremely tiny signal such as
the gravitational-wave force is a very important subject in quantum physics. A
major obstacle to this goal is that, in a simple detection setup, the
measurement noise is lower bounded by the so-called standard quantum limit
(SQL), which is originated from the intrinsic mechanical back-action noise.
Hence, the sensor system has to be carefully engineered so that it evades the
back-action noise and eventually beats the SQL. In this paper, based on the
well-developed geometric control theory for classical disturbance decoupling
problem, we provide a general method for designing an auxiliary (coherent
feedback or direct interaction) controller for the sensor system to achieve the
above-mentioned goal. This general theory is applied to a typical
opto-mechanical sensor system. Also, we demonstrate a controller design for a
practical situation where several experimental imperfections are present.
</p>
{{{{ARTICLE_PARSER}}}}<p>It is universally accepted that noise and decoherence affecting the
performance of superconducting quantum circuits are consistent with the
presence of spurious two-level systems (TLS). In recent years bulk defects have
been generally ruled out as the dominant source, and the search has focused on
surfaces and interfaces. Despite a wide range of theoretical models and
experimental efforts, the origin of these surface TLS still remains largely
unknown, making further mitigation of TLS induced decoherence extremely
challenging. Here we use a recently developed on-chip electron spin resonance
(ESR) technique that allows us to detect spins with a very low surface
coverage. We combine this technique with various surface treatments
specifically to reveal the nature of native surface spins on Al$_2$O$_3$ -- the
mainstay of almost all solid state quantum devices. On a large number of
samples we resolve three ESR peaks with the measured total paramagnetic spin
density $n=2.2\times 10^{17}$m$^{-2}$, which matches the density inferred from
the flux noise in SQUIDs. We show that two of these peaks originate from
physisorbed atomic hydrogen which appears on the surface as a by-product of
water dissociation. We suggest that the third peak is due to molecular oxygen
on the Al$_2$O$_3$ surface captured at strong Lewis base defect sites,
producing charged O$_2^-$. These results provide important information towards
the origin of charge and flux noise in quantum circuits. Our findings open up a
whole new approach to identification and controlled reduction of paramagnetic
sources of noise in solid state quantum devices.
</p>
{{{{ARTICLE_PARSER}}}}<p>We describe general features that might be observed in the line spectra of
relic cosmological particles should quantum nonequilibrium be preserved in
their statistics. According to our arguments, these features would represent a
significant departure from those of a conventional origin. Among other
features, we find a possible spectral broadening (for incident photons) that is
proportional to the energy resolution of the recording telescope (and so could
be orders of magnitude larger than any intrinsic broadening). Notably, for a
range of possible initial conditions we find the possibility of spectral line
`narrowing' whereby a telescope could observe a spectral line which is narrower
than it should conventionally be able to resolve. We briefly discuss
implications for the indirect search for dark matter.
</p>
{{{{ARTICLE_PARSER}}}}<p>I report a theoretical study of collective coherent quantum-mechanical
oscillations in disordered superconducting quantum metamaterials (SQMs), i.e
artificially fabricated arrays of qubits (two-levels system) embedded in a
low-dissipative resonator. An unavoidable disorder in qubits parameters results
in a substantial spread of qubits frequencies, and in the absence of
electromagnetic interaction between qubits these individual quantum-mechanical
oscillations manifest themselves by a large number of small resonant drops in
the frequency dependent transmission of electromagnetic waves propagating
through disordered SQM, $D(\omega)$. We show that even a weak electromagnetic
interaction between adjacent qubits can overcome the disorder and establish
completely or partially \emph{synchronized} quantum-mechanical dynamic state in
the disordered SQM. In such a state a large amount of qubits displays the
collective quantum mechanical oscillations, and this collective behavior
manifests itself by a few giant resonant drops in the $D(\omega)$ dependence.
The size of a system $r_0$ showing the collective (synchronized)
quantum-mechanical behavior is determined as $r_0 \simeq a [K/\delta
\Delta]^2$, where $K$, $\delta \Delta$, $a$ are the energy of nearest-neighbor
interaction, the spread of qubits energy splitting, and the distance between
qubits, accordingly. Our analysis is in a good accord with recent experiments
on the electrodynamics of the disordered SQMs.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we discuss tensor network descriptions of AdS/CFT from two
different viewpoints. First, we start with an Euclidean path-integral
computation of ground state wave functions with a UV cut off. We consider its
efficient optimization by making its UV cut off position dependent and define a
quantum state at each length scale. We conjecture that this path-integral
corresponds to a time slice of AdS. Next, we derive a flow of quantum states by
rewriting the action of Killing vectors of AdS3 in terms of the dual 2d CFT.
Both approaches support a correspondence between the hyperbolic time slice H2
in AdS3 and a version of continuous MERA (cMERA). We also give a heuristic
argument why we can expect a sub-AdS scale bulk locality for holographic CFTs.
</p>
{{{{ARTICLE_PARSER}}}}<p>Achieving effectively adiabatic dynamics is a ubiquitous goal in almost all
areas of quantum physics. Here, we study the speed with which a quantum system
can be driven when employing transitionless quantum driving. As a main result,
we establish a rigorous link between this speed, the quantum speed limit, and
the (energetic) cost of implementing such a shortcut to adiabaticity.
Interestingly, this link elucidates a trade-off between speed and cost, namely
that instantaneous manipulation is impossible as it requires an infinite cost.
These findings are illustrated for two experimentally relevant systems - the
parametric oscillator and the Landau-Zener model - which reveal that the
spectral gap governs the quantum speed limit as well as the cost for realizing
the shortcut.
</p>
{{{{ARTICLE_PARSER}}}}<p>We introduce a new kind of spontaneous four wave mixing process for the
generation of photon pairs, in which the four waves involved counter-propagate
in a guided-wave \chi^{(3)} medium; we refer to this process as
counter-propagating spontaneous four wave mixing (CP-SFWM). We show that for
the simplest CP-SFWM source, in which all waves propagate in the same
polarization and transverse mode and in which self- and cross-phase modulation
effects are negligible, phasematching is attained automatically regardless of
dispersion in the fiber or waveguide. Furthermore, we show that in two distinct
versions of this source (both pumps pulsed, or one pump pulsed and the
remaining one monochromatic), the two-photon state is automatically factorable
provided that the length of the nonlinear medium exceeds a certain threshold,
easily achievable in practice since this threshold length tends to be in the
range of mm to cm. We also show that if one of the pumps approaches the
monochromatic limit, and for a sufficient nonlinear medium length, the
bandwidth of one of the two photons in a given pair may be reduced to the level
of MHz, compatible with electronic transitions for the implementation of
atom-photon interfaces, without the use of optical cavities.
</p>
{{{{ARTICLE_PARSER}}}}<p>The surface code is currently the leading proposal to achieve fault-tolerant
quantum computation. Among its strengths are the plethora of known ways in
which fault-tolerant Clifford operations can be performed, namely, by deforming
the topology of the surface, by the fusion and splitting of codes and even by
braiding engineered Majorana modes using twist defects. Here we present a
unified framework to describe these methods, which is required to better
compare different schemes, and to facilitate the design of hybrid schemes.
Novel to our unification is the identification of twist defects with the
corners of the planar code. This identification enables us to find new ways of
performing single-qubit Clifford gates by exchanging the corners of the planar
code via code deformation. We analyse the ways in which all of the different
schemes can be combined, and propose a novel hybrid encoding. We also show how
all of the Clifford gates can be implemented on a single code without loss of
distance, thus offering an attractive alternative to ancilla-mediated schemes
to complete the Clifford group with lattice surgery.
</p>
{{{{ARTICLE_PARSER}}}}<p>Motivated by numerous experiments on Bose-Einstein condensed atoms which have
been performed in tight trapping potentials of various geometries (elongated
and/or toroidal/annular), we develop a general method which allows us to reduce
the corresponding three-dimensional Gross-Pitaevskii equation for the order
parameter into an effectively one-dimensional equation, taking into account the
interactions (i.e., treating the width of the transverse profile variationally)
and the curvature of the trapping potential. As an application of our model we
consider atoms which rotate in a toroidal trapping potential. We evaluate the
state of lowest energy for a fixed value of the angular momentum within various
approximations of the effectively one-dimensional model and compare our results
with the full solution of the three-dimensional problem, thus getting evidence
for the accuracy of our model.
</p>
{{{{ARTICLE_PARSER}}}}<p>A proposed phase-estimation protocol based on measuring the parity of a
two-mode squeezed-vacuum state at the output of a Mach-Zehnder interferometer
shows that the Cram\'{e}r-Rao sensitivity is sub-Heisenberg [Phys.\ Rev.\
Lett.\ {\bf104}, 103602 (2010)]. However, these measurements are problematic,
making it unclear if this sensitivity can be obtained with a finite number of
measurements. This sensitivity is only for phase near zero, and in this region
there is a problem with ambiguity because measurements cannot distinguish the
sign of the phase. Here, we consider a finite number of parity measurements,
and show that an adaptive technique gives a highly accurate phase estimate
regardless of the phase. We show that the Heisenberg limit is reachable, where
the number of trials needed for mean photon number $\bar{n}=1$ is approximately
one hundred. We show that the Cram\'{e}r-Rao sensitivity can be achieved
approximately, and the estimation is unambiguous in the interval ($-\pi/2,
\pi/2$).
</p>
{{{{ARTICLE_PARSER}}}}<p>Nitrogen-vacancy (NV) centers in diamonds are interesting due to their
remarkable characteristics that are well suited to applications in
quantum-information processing and magnetic field sensing, as well as
representing stable fluorescent sources. Multiple NV centers in nanodiamonds
(NDs) are especially useful as biological fluorophores due to their chemical
neutrality, brightness and room-temperature photostability. Furthermore, NDs
containing multiple NV centers also have potential in high-precision magnetic
field and temperature sensing. Coupling NV centers to propagating surface
plasmon polariton (SPP) modes gives a base for lab-on-a-chip sensing devices,
allows enhanced fluorescence emission and collection which can further enhance
the precision of NV-based sensors. Here, we investigate coupling of multiple NV
centers in individual NDs to the SPP modes supported by silver surfaces
protected by thin dielectric layers and by gold V-grooves (VGs) produced via
the self-terminated silicon etching. In the first case, we concentrate on
monitoring differences in fluorescence spectra obtained from a source ND, which
is illuminated by a pump laser, and from a scattering ND illuminated only by
the fluorescence-excited SPP radiation. In the second case, we observe changes
in the average NV lifetime when the same ND is characterized outside and inside
a VG. Fluorescence emission from the VG terminations is also observed, which
confirms the NV coupling to the VG-supported SPP modes.
</p>
{{{{ARTICLE_PARSER}}}}<p>We derive a formal connection between quantum data hiding and quantum
privacy, confirming the intuition behind the construction of bound entangled
states from which secret bits can be extracted. We present three main results.
First, we obtain a bound on the distillable entanglement of private states in
terms of restricted relative entropy measures, which is tight in many cases.
Second, we find that protocols for key distillation lead to the distillation of
data hiding states. Third, we consider the problem of extending the distance of
quantum key distribution with help of intermediate stations. In analogy to the
quantum repeater, this paradigm has been called the quantum key repeater. We
show that when extending private states, the resulting rate is bounded by the
distillable entanglement. In order to swap perfect secrecy it is thus
essentially optimal to use entanglement swapping.
</p>
{{{{ARTICLE_PARSER}}}}<p>Surface plasmon polaritons (SPPs) are locally excited at silver surfaces
using (~100) nm-sized nanodiamonds (NDs) with multiple nitrogen-vacancy (NV)
centers (~400). The fluorescence from an externally illuminated (at 532 nm) ND
and from nearby NDs, which are not illuminated but produce out-of-plane
scattering of SPPs excited by the illuminated ND, exhibit distinctly different
wavelength spectra, showing short-wavelength filtering due to the SPP
propagation loss. The results indicate that NDs with multiple NV centers can be
used as efficient sub-wavelength SPP sources in planar integrated plasmonics
for various applications.
</p>
{{{{ARTICLE_PARSER}}}}<p>The notion of objective probability or chance, as a physical trait of the
world, has proved elusive; the identification of chances with actual
frequencies does not succeed. An adequate theory of chance should explain not
only the connection of chance with statistics, but with degrees of belief, and
more broadly the entire phenomenology of (seemingly) chance events and their
measurement. Branching structure in the decoherence-based many worlds theory
provides an account of what chance is that satisfies all these desiderata,
including the requirement that chance involves uncertainty.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider the many-body dynamics of fermions with Coulomb interaction in a
mean-field scaling limit where the kinetic and potential energy are of the same
order for large particle numbers. In the considered limit the spatial variation
of the mean-field is small. We prove two results about this scaling limit.
First, due to the small variation, i.e., small forces, we show that the
many-body dynamics can be approximated by the free dynamics with an appropriate
phase factor with the optimal error term. Second, we show that the Hartree-Fock
dynamics gives a better approximation with a smaller error term. Thus, the
Hartree-Fock equations can indeed be derived from the many-body dynamics with
Coulomb interaction in a mean-field scaling limit.
</p>
{{{{ARTICLE_PARSER}}}}<p>Bose-Einstein condensation is a unique phase transition in that it is not
driven by inter-particle interactions, but can theoretically occur in an ideal
gas, purely as a consequence of quantum statistics. This chapter addresses the
question \emph{`How is this ideal Bose gas condensation modified in the
presence of interactions between the particles?' } This seemingly simple
question turns out to be surprisingly difficult to answer. Here we outline the
theoretical background to this question and discuss some recent measurements on
ultracold atomic Bose gases that have sought to provide some answers.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this note, we study Cabello's nonlocality argument (CNA) for three-qubit
systems configured in the generalized GHZ state. For this class of states, we
show that CNA runs for almost all entangled ones, and that the maximum
probability of success of CNA is $14\%$ (approx.), which is attained for the
maximally entangled GHZ state. This maximum probability is slightly higher than
that achieved for the standard Hardy's nonlocality argument (HNA) for three
qubits, namely $12.5\%$. Also, we show that the success probability of both HNA
and CNA for three-qubit systems can reach a maximum of $50\%$ in the framework
of generalized no-signaling theory.
</p>
{{{{ARTICLE_PARSER}}}}<p>The theory of composite bosons (cobosons) made of two fermions [Phys. Rev. A
71, 034306 (2005), Phys. Rev. Lett. 109, 260403 (2012)] converges to ordinary
structureless bosons in the limit of infinitely strong entanglement between the
fermionic constituents. For finite entanglement, the annihilation operator
$\hat c$ of a composite boson couples the $N$-coboson Fock-state not only to
the $(N-1)$-coboson state -- as for ordinary bosons --, but also to a component
which is orthogonal to the Fock-state ladder of cobosons. Coupling with states
orthogonal to the Fock ladder arises also in dynamical processes of cobosons.
Here, with a Gedanken-experiment involving both mode-splitting and collective
Hong-Ou-Mandel-like interference, we derive the characteristic physical
signature of the states orthogonal to the Fock ladder generated in the
splitting process. This allows to extract microscopic properties of
many-fermion-wave functions from the collective coboson behavior. We show that
consecutive beam-splitter dynamics increases the deviation from the ideal
bosonic behavior pattern, which opens up a rigorous approach to the
falsification of coboson theory.
</p>
{{{{ARTICLE_PARSER}}}}<p>We demonstrate a novel way of synthesizing spin-orbit interactions in
ultracold quantum gases, based on a single-photon optical clock transition
coupling two long-lived electronic states of two-electron $^{173}$Yb atoms. By
mapping the electronic states onto effective sites along a synthetic
\"electronic\" dimension, we have engineered synthetic fermionic ladders with
tunable magnetic fluxes. We have detected the spin-orbit coupling with
fiber-link-enhanced clock spectroscopy and directly measured the emergence of
chiral edge currents, probing them as a function of the magnetic field flux.
These results open new directions for the investigation of topological states
of matter with ultracold atomic gases.
</p>
{{{{ARTICLE_PARSER}}}}<p>Geometric quantum computation is the idea that geometric phases can be used
to implement quantum gates, i.e., the basic elements of the Boolean network
that forms a quantum computer. Although originally thought to be limited to
adiabatic evolution, controlled by slowly changing parameters, this form of
quantum computation can as well be realized at high speed by using nonadiabatic
schemes. Recent advances in quantum gate technology have allowed for
experimental demonstrations of different types of geometric gates in adiabatic
and nonadiabatic evolution. Here, we address some conceptual issues that arise
in the realizations of geometric gates. We examine the appearance of dynamical
phases in quantum evolution and point out that not all dynamical phases need to
be compensated for in geometric quantum computation. We delineate the relation
between Abelian and non-Abelian geometric gates and find an explicit physical
example where the two types of gates coincide. We identify differences and
similarities between adiabatic and nonadiabatic realizations of quantum
computation based on non-Abelian geometric phases.
</p>
{{{{ARTICLE_PARSER}}}}<p>We determine the exact strong converse exponent of classical-quantum channel
coding, for every rate above the Holevo capacity. Our form of the exponent is
an exact analogue of Arimoto's, given as a transform of the Renyi capacities
with parameters alpha&gt;1. It is important to note that, unlike in the classical
case, there are many inequivalent ways to define the Renyi divergence of
states, and hence the R\'enyi capacities of channels. Our exponent is in terms
of the Renyi capacities corresponding to a version of the Renyi divergences
that has been introduced recently in [M\\"uller-Lennert, Dupuis, Szehr, Fehr and
Tomamichel, J. Math. Phys. 54, 122203, (2013)], and [Wilde, Winter, Yang,
Commun. Math. Phys. 331, (2014)]. Our result adds to the growing body of
evidence that this new version is the natural definition for the purposes of
strong converse problems.
</p>
{{{{ARTICLE_PARSER}}}}<p>We describe a general method for turning quantum circuits into sparse quantum
subsystem codes. The idea is to turn each circuit element into a set of
low-weight gauge generators that enforce the input-output relations of that
circuit element. Using this prescription, we can map an arbitrary stabilizer
code into a new subsystem code with the same distance and number of encoded
qubits but where all the generators have constant weight, at the cost of adding
some ancilla qubits. With an additional overhead of ancilla qubits, the new
code can also be made spatially local.
</p>
<p>Applying our construction to certain concatenated stabilizer codes yields
families of subsystem codes with constant-weight generators and with minimum
distance $d = n^{1-\epsilon}$, where $\epsilon = O(1/\sqrt{\log n})$. For
spatially local codes in $D$ dimensions we nearly saturate a bound due to
Bravyi and Terhal and achieve $d = n^{1-\epsilon-1/D}$. Previously the best
code distance achievable with constant-weight generators in any dimension, due
to Freedman, Meyer and Luo, was $O(\sqrt{n\log n})$ for a stabilizer code.
</p>
{{{{ARTICLE_PARSER}}}}<p>Complex mesoscopic systems play increasingly important roles in modern
science -- from understanding biological functions at the molecular level, to
designing solid-state information processing devices. The operation of these
systems typically depends on their energetic structure, yet probing their
energy-landscape can be extremely challenging; they have many degrees of
freedom, which may be hard to isolate and measure independently. Here we show
that a qubit (a two-level quantum system) with a biased energy-splitting can
directly probe the spectral properties of a complex system, without knowledge
of how they couple. Our work is based on the completely-positive and
trace-preserving map formalism, which treats any unknown dynamics as a
`black-box' process. This black box contains information about the system with
which the probe interacts, which we access by measuring the survival
probability of the initial state of the probe as function of the
energy-splitting and the process time. Fourier transforming the results yields
the energy spectrum of the complex system. Without making assumptions about the
strength or form of its coupling, our probe could determine aspects of a
complex molecule's energy landscape as well as, in many cases, test for
coherent superposition of its energy eigenstates.
</p>
{{{{ARTICLE_PARSER}}}}<p>We demonstrate that the quantum corrections to the classical arrival time for
a quantum object in a potential free region of space, as computed by Galapon
[Phys. Rev. A {\bf 80}, 030102(R) (2009)], can be eliminated up to a given
order of $\hbar$ by choosing an appropriate position-dependent phase for the
object's wavefunction. This then implies that we can make the quantum arrival
time of the object as close as possible to its corresponding classical arrival
time, allowing us to synchronize a classical and quantum clock which tells time
using the classical and quantum arrival time of the object, respectively. We
provide an example for synchronizing such a clock by making use of a quantum
object with a position-dependent phase imprinted on the object's initial
wavefunction with the use of an impulsive potential.
</p>
{{{{ARTICLE_PARSER}}}}<p>To exploit a given physical system for quantum information processing, it is
critical to understand the different types of noise affecting quantum control.
Distinguishing coherent and incoherent errors is extremely useful as they can
be reduced in different ways. Coherent errors are generally easier to reduce at
the hardware level, e.g. by improving calibration, whereas some sources of
incoherent errors, e.g. T2* processes, can be reduced by engineering robust
pulses. In this work, we illustrate how purity benchmarking and randomized
benchmarking can be used together to distinguish between coherent and
incoherent errors and to quantify the reduction in both of them due to using
optimal control pulses and accounting for the transfer function in an electron
spin resonance system. We also prove that purity benchmarking provides bounds
on the optimal fidelity and diamond norm that can be achieved by correcting the
coherent errors through improving calibration.
</p>
{{{{ARTICLE_PARSER}}}}<p>We investigate multi-\"photon\" interband excitation processes in an optical
lattice that is driven periodically in time by a modulation of the lattice
depth. Assuming the system to be prepared in the lowest band, we compute the
excitation spectrum numerically. Moreover, we estimate the effective coupling
parameters for resonant interband excitation processes analytically, employing
degenerate perturbation theory in Floquet space. We find that below a threshold
driving strength, interband excitations are suppressed exponentially with
respect to the inverse driving frequency. For sufficiently low frequencies,
this leads to a rather sudden onset of interband heating, once the driving
strength reaches the threshold. We argue that this behavior is rather generic
and should also be found in lattice systems that are driven by other forms of
periodic forcing. Our results are relevant for Floquet engineering, where a
lattice system is driven periodically in time in order to endow it with novel
properties like the emergence of a strong artificial magnetic field or a
topological band structure. In this context, interband excitation processes
correspond to detrimental heating.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study the influence of narrowed distributions of the nuclear Overhauser
field on the decoherence of a central electron spin in quantum dots. We
describe the spin dynamics in quantum dots by the central spin model. We use
analytic solutions for uniform couplings and the time dependent density-matrix
renormalization group (tDMRG) for nonuniform couplings. With these tools we
calculate the dynamics of the central spin for large baths of nuclear spins
with or without external magnetic field applied to the central spin. The focus
of our study is the influence of initial mixtures with narrowed distributions
of the Overhauser field and of applied magnetic fields on the decoherence of
the central spin.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a proposal of a set-up to measure the work distribution due to an
arbitrary unitary process acting on the spatial transverse degrees of freedom
of a light beam. Hermite-Gaussian optical modes representing a quantum harmonic
oscillator are prepared in a thermal state and sent through an interferometer.
We show that the Fourier transform of the work distribution, or the
characteristic function, can be obtained by measuring the intensity at the
output of the interferometer. The usefulness of the approach is illustrated by
calculating the work distribution for a unitary operation that displaces the
linear momentum of the oscillator. We discuss the feasibility of the
experiment, which can be realized with simple linear optical components.
</p>
{{{{ARTICLE_PARSER}}}}<p>We determine the ultimate potential of quantum imaging for boosting the
resolution of a far-field, diffraction-limited, linear imaging device within
the paraxial approximation. First we show that the problem of estimating the
separation between two point-like sources is equivalent to the estimation of
the loss parameters of two lossy bosonic channels, i.e., the transmissivities
of two beam splitters. Using this representation, we establish the ultimate
precision bound for resolving two point-like sources in an arbitrary quantum
state, with a simple formula for the specific case of two thermal sources. We
find that the precision bound scales with the number of collected photons
according to the standard quantum limit. Then we determine the sources whose
separation can be estimated optimally, finding that quantum-correlated sources
(entangled or discordant) can be super-resolved at the sub-Rayleigh scale. Our
results set the upper bounds on any present or future imaging technology, from
astronomical observation to microscopy, which is based on quantum detection as
well as source engineering.
</p>
{{{{ARTICLE_PARSER}}}}<p>Polarized photons from a symmetrical mixture are in the measured state prior
to a measurement. Accordingly, photons in singlet state do not exhibit action
at a distance. After detecting particle 1 we certainly know the polarization
state of particle 2. This is the element of physical reality demanded by EPR.
It was shown why quantum mechanics infringes Bells inequality.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study quantum mechanical tunneling using complex solutions of the
classical field equations. Simple visualization techniques allow us to unify
and generalize previous treatments, and straightforwardly show the connection
to the standard approach using Euclidean instanton solutions. We demonstrate
that the negative modes of solutions along various contours in the complex time
plane reveal which paths give the leading contribution to tunneling and which
do not, and we provide a criterion for identifying the negative modes. Central
to our approach is the solution of the background and perturbation equations
not only along a single path, but over an extended region of the complex time
plane. Our approach allows for a fully continuous and coherent treatment of
classical evolution interspersed by quantum tunneling events, and is applicable
in situations where singularities are present and also where Euclidean
solutions might not exist.
</p>
{{{{ARTICLE_PARSER}}}}<p>Cat states are coherent quantum superpositions of macroscopically distinct
states and are useful for understanding the boundary between the classical and
the quantum world. Due to their macroscopic nature, cat states are difficult to
prepare in physical systems. We propose a method to create cat states in
one-dimensional quantum walks using delocalized initial states of the walker.
Since the quantum walks can be performed on any quantum system, our proposal
enables a platform-independent realization of the cat states. We further show
that the linear dispersion relation of the effective quantum walk Hamiltonian,
which governs the dynamics of the delocalized states, is responsible for the
formation of the cat states. We analyze the robustness of these states against
environmental interactions and present methods to control and manipulate the
cat states in the photonic implementation of quantum walks.
</p>
{{{{ARTICLE_PARSER}}}}<p>We experimentally demonstrate the violation of classical physics in a
many-atom system using a recently derived criterion [E. Kot et al., Phys. Rev.
Lett. 108, 233601 (2013)] that explicitly does not make use of quantum
mechanics. We thereby show that the magnetic moment distribution measured by
McConnell et al. [R. McConnell et al., Nature 519, 439 (2015)] in a system with
a total mass of $2.6\times 10^5$ atomic mass units is inconsistent with
classical physics. Notably, the violation of classical physics affects an area
in phase space $10^3$ times larger than the Planck quantum $\hbar$.
</p>
{{{{ARTICLE_PARSER}}}}<p>Time-energy high-dimensional quantum key distribution (HD-QKD) leverages the
high-dimensional nature of time-energy entangled biphotons and the loss
tolerance of single-photon detection to achieve long-distance key distribution
with high photon information efficiency. To date, the general-attack security
of HD-QKD has only been proven in the asymptotic regime, while HD-QKD's
finite-key security has only been established for a limited set of attacks.
Here we fill this gap by providing a rigorous HD-QKD security proof for general
attacks in the finite-key regime. Our proof relies on a novel entropic
uncertainty relation that we derive for time and conjugate-time measurements
using dispersive optics, and our analysis includes an efficient decoy-state
protocol in its parameter estimation. We present numerically-evaluated
secret-key rates illustrating the feasibility of secure and composable HD-QKD
over metropolitan-area distances when the system is subjected to the most
powerful eavesdropping attack.
</p>
{{{{ARTICLE_PARSER}}}}<p>Known entropy bounds, and the Generalized Second Law, were recently shown to
imply bounds on the information arriving at future null infinity. We complete
this derivation by including the contribution from gravitons. We test the
bounds in classical settings with gravity and no matter. In Minkowski space,
the bounds vanish on any subregion of the future boundary, independently of
coordinate choices. More generally, the bounds vanish in regions where no
gravitational radiation arrives. In regions that do contain Bondi news, the
bounds are compatible with the presence of information, including the
information stored in gravitational memory. All of our results are consistent
with the equivalence principle, which states that empty Riemann-flat spacetime
regions contain no classical information. We also discuss the possibility that
Minkowski space has an infinite vacuum degeneracy labeled by a choice of Bondi
coordinates (a classical parameter, if physical). We argue that this degeneracy
cannot have any observational consequences if the equivalence principle holds.
Our bounds are consistent with this conclusion.
</p>
{{{{ARTICLE_PARSER}}}}<p>A Gibbs operator $e^{-\beta H}$ for a 2D lattice system with a Hamiltonian
$H$ can be represented by a 3D tensor network, the third dimension being the
imaginary time (inverse temperature) $\beta$. Coarse-graining the network along
$\beta$ results in an accurate 2D projected entangled-pair operator (PEPO) with
a finite bond dimension. The coarse-graining is performed by a tree tensor
network of isometries that are optimized variationally to maximize the accuracy
of the PEPO. The algorithm is applied to the two-dimensional Hubbard model on
an infinite square lattice. Benchmark results are obtained that are consistent
with the best cluster dynamical mean-field theory and power series expansion in
the regime of parameters where they yield mutually consistent results.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study almost quantum correlations in tripartite Bell scenarios. In the
simplest case with two dichotomic measurements per party we find that there
exist tight Bell inequalities that witness almost quantum correlations beyond
quantum theory, which is in contrast to its bipartite counterpart. Finally, we
study the relation between the almost quantum set and the NPA hierarchy. While
the former lies between the first and third level of the hierarchy, we find
that the second level does not contain and is not contained within the almost
quantum set.
</p>
{{{{ARTICLE_PARSER}}}}<p>Quantum-statistical effects occur during the propagation of electromagnetic
(EM) waves inside the dielectric media or metamaterials, which include a large
class of nanophotonic and plasmonic waveguides with dissipation and noise.
Exploiting the formal analogy between the Schrodinger equation and the Maxwell
equations for dielectric linear media, we rigorously derive the effective
Hamiltonian operator which describes such propagation. This operator turns out
to be essentially non-Hermitian in general, and pseudo-Hermitian in some
special cases. Using the density operator approach for general non-Hermitian
Hamiltonians, we derive a master equation that describes the statistical
ensembles of EM wave modes. The method also describes the quantum dissipative
and decoherence processes which happen during the wave's propagation, and,
among other things, it reveals the conditions that are necessary to control the
energy and information loss inside the above-mentioned materials.
</p>
{{{{ARTICLE_PARSER}}}}<p>There has been recent interest in identifying entanglement as the fundamental
concept from which space may emerge. We note that the particular way that a
Hilbert space is decomposed into tensor factors is important in what the
resulting geometry looks like. We then propose that time may be regarded as a
variable that parameterizes a family of such decompositions, thus giving rise
to a family of spatial geometries. As a proof of concept, this idea is
demonstrated in two toy models based on Kitaev's toric code, which feature a
dynamical change of dimension and topology.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this work we introduce the contact Heisenberg algebra which is the
restriction of the Jacobi algebra on contact manifolds to the linear and
constant functions. We give the exact expression of its corresponding
Baker-Campbell-Hausdorff formula. We argue that this result is relevant to the
quantization of contact systems.
</p>
{{{{ARTICLE_PARSER}}}}<p>Presented is a quantum lattice gas algorithm to efficiently model a system of
Dirac particles interacting through an intermediary gauge field. The algorithm
uses a fixed qubit array to represent both the spacetime and the particles
contained in the spacetime. Despite being a lattice based algorithm, Lorentz
invariance is preserved down to the grid scale, with the continuum Dirac
Hamiltonian generating the local unitary evolution even at that scale: there is
nonlinear scaling between the smallest observable time and that time measured
in the quantum field theory limit, a kind of time dilation effect that emerges
on small scales but has no effect on large scales. The quantum lattice gas
algorithm correctly accounts for the anticommutative braiding of
indistinguishable fermions---it does not suffer the Fermi-sign problem. It
provides a highly convergent numerical simulation for strongly-correlated
fermions equal to a covariant path integral, presented here for the case when a
Dirac particle's Compton wavelength is large compared to the grid scale of the
qubit array.
</p>
{{{{ARTICLE_PARSER}}}}<p>A new way of constructing unextendible maximally entangled basis (UMEB) from
maximally entangled basis (MEB) is proposed. Consequently, it is shown that if
there is an $N$-member UMEB in $\mathbb{C}^d\otimes \mathbb{C}^d$, then there
exists a $(qd)^2-q(d^2-N)$-member UMEB in $\mathbb{C}^{qd}\otimes
\mathbb{C}^{qd}$ for any $q\in\mathbb{N}$. This improves the results in [Phys.
Rev. A 90, 034301(2014)], which shows that there exists a
$(qd)^2-(d^2-N)$-member UMEB in $\mathbb{C}^{qd}\otimes \mathbb{C}^{qd}$
provided that an $N$-member UMEB exsits in $\mathbb{C}^d\otimes \mathbb{C}^d$.
In addition, a very easy way of constructing UMEB in $\mathbb{C}^d\otimes
\mathbb{C}^{d'}$ with $d&lt;d'$ is presented, which promotes and covers all the
previous related work.
</p>
{{{{ARTICLE_PARSER}}}}<p>Compared with automatic speech recognition (ASR), the human auditory system
is more adept at handling noise-adverse situations, including environmental
noise and channel distortion. To mimic this adeptness, auditory models have
been widely incorporated in ASR systems to improve their robustness. This paper
proposes a novel auditory model which incorporates psychoacoustics and
otoacoustic emissions (OAEs) into ASR. In particular, we successfully implement
the frequency-dependent property of psychoacoustic models and effectively
improve resulting system performance. We also present a novel double-transform
spectrum-analysis technique, which can qualitatively predict ASR performance
for different noise types. Detailed theoretical analysis is provided to show
the effectiveness of the proposed algorithm. Experiments are carried out on the
AURORA2 database and show that the word recognition rate using our proposed
feature extraction method is significantly increased over the baseline. Given
models trained with clean speech, our proposed method achieves up to 85.39%
word recognition accuracy on noisy data.
</p>
{{{{ARTICLE_PARSER}}}}<p>Bayesian methods for machine learning have been widely investigated, yielding
principled methods for incorporating prior information into inference
algorithms. In this survey, we provide an in-depth review of the role of
Bayesian methods for the reinforcement learning (RL) paradigm. The major
incentives for incorporating Bayesian reasoning in RL are: 1) it provides an
elegant approach to action-selection (exploration/exploitation) as a function
of the uncertainty in learning; and 2) it provides a machinery to incorporate
prior knowledge into the algorithms. We first discuss models and methods for
Bayesian inference in the simple single-step Bandit model. We then review the
extensive recent literature on Bayesian methods for model-based RL, where prior
information can be expressed on the parameters of the Markov model. We also
present Bayesian methods for model-free RL, where priors are expressed over the
value function or policy class. The objective of the paper is to provide a
comprehensive survey on Bayesian RL algorithms and their theoretical and
empirical properties.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study the state complexity of binary operations on regular languages over
different alphabets. It is known that if $L'_m$ and $L_n$ are languages of
state complexities $m$ and $n$, respectively, and restricted to the same
alphabet, the state complexity of any binary boolean operation on $L'_m$ and
$L_n$ is $mn$, and that of product (concatenation) is $m 2^n - 2^{n-1}$. In
contrast to this, we show that if $L'_m$ and $L_n$ are over different
alphabets, the state complexity of union and symmetric difference is
$(m+1)(n+1)$, that of difference is $mn+m$, that of intersection is $mn$, and
that of product is $m2^n+2^{n-1}$. We also study unrestricted complexity of
binary operations in the classes of regular right, left, and two-sided ideals,
and derive tight upper bounds. The bounds for product of the unrestricted cases
(with the bounds for the restricted cases in parentheses) are as follows: right
ideals $m+2^{n-2}+2^{n-1}$ ($m+2^{n-2}$); left ideals $mn+m+n$ ($m+n-1$);
two-sided ideals $m+2n$ ($m+n-1$). The state complexities of boolean operations
on all three types of ideals are the same as those of arbitrary regular
languages, whereas that is not the case if the alphabets of the arguments are
the same. Finally, we update the known results about most complex regular,
right-ideal, left-ideal, and two-sided-ideal languages to include the
unrestricted cases.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider the computational complexity of finding a legal black pebbling of
a DAG $G=(V,E)$ with minimum cumulative cost. A black pebbling is a sequence
$P_0,\ldots, P_t \subseteq V$ of sets of nodes which must satisfy the following
properties: $P_0 = \emptyset$ (we start off with no pebbles on $G$),
$\mathsf{sinks}(G) \subseteq \bigcup_{j \leq t} P_j$ (every sink node was
pebbled at some point) and $\mathsf{parents}\big(P_{i+1}\backslash P_i\big)
\subseteq P_i$ (we can only place a new pebble on a node $v$ if all of $v$'s
parents had a pebble during the last round). The cumulative cost of a pebbling
$P_0,P_1,\ldots, P_t \subseteq V$ is $\mathsf{cc}(P) = | P_1| + \ldots + |
P_t|$. The cumulative pebbling cost is an especially important security metric
for data-independent memory hard functions, an important primitive for password
hashing. Thus, an efficient (approximation) algorithm would be an invaluable
tool for the cryptanalysis of password hash functions as it would provide an
automated tool to establish tight bounds on the amortized space-time cost of
computing the function. We show that such a tool is unlikely to exist. In
particular, we prove the following results. (1) It is
$\texttt{NP}\mbox{-}\texttt{Hard}$ to find a pebbling minimizing cumulative
cost. (2) The natural linear program relaxation for the problem has integrality
gap $\tilde{O}(n)$, where $n$ is the number of nodes in $G$. We conjecture that
the problem is hard to approximate. (3) We show that a related problem, find
the minimum size subset $S\subseteq V$ such that $\textsf{depth}(G-S) \leq d$,
is also $\texttt{NP}\mbox{-}\texttt{Hard}$. In fact, under the unique games
conjecture there is no $(2-\epsilon)$-approximation algorithm.
</p>
{{{{ARTICLE_PARSER}}}}<p>We have created a large diverse set of cars from overhead images, which are
useful for training a deep learner to binary classify, detect and count them.
The dataset and all related material will be made publically available. The set
contains contextual matter to aid in identification of difficult targets. We
demonstrate classification and detection on this dataset using a neural network
we call ResCeption. This network combines residual learning with
Inception-style layers and is used to count cars in one look. This is a new way
to count objects rather than by localization or density estimation. It is
fairly accurate, fast and easy to implement. Additionally, the counting method
is not car or scene specific. It would be easy to train this method to count
other kinds of objects and counting over new scenes requires no extra set up or
assumptions about object locations.
</p>
{{{{ARTICLE_PARSER}}}}<p>We introduce several techniques for sampling and visualizing the latent
spaces of generative models. Replacing linear interpolation with spherical
linear interpolation prevents diverging from a models prior distribution and
produces sharper samples. J Diagrams and MINE grids are introduced as
visualizations of manifolds created by analogies and nearest neighbors. We
demonstrate two new techniques for deriving attribute vectors - bias-correct
vectors with data replication and synthetic vectors with data augmentation.
Most techniques are intended to be independent of model type and examples are
shown on both Variational Autoencoders and Generative Adversarial Networks.
</p>
{{{{ARTICLE_PARSER}}}}<p>Sorting is one of the oldest computing problems and is still very important
in the age of big data. Various algorithms and implementation techniques have
been proposed. In this study, we focus on comparison based, internal sorting
algorithms. We created 12 data types of various sizes for experiments and
tested extensively various implementations in a single setting. Using some
effective techniques, we discovered that quicksort is adaptive to nearly sorted
inputs and is still the best overall sorting algorithm. We also identified
which techniques are effective in timsort, one of the most popular and
efficient sorting method based on natural mergesort, and created our version of
mergesort, which runs faster than timsort on nearly sorted instances. Our
implementations of quicksort and mergesort are different from other
implementations reported in all textbooks or research articles, faster than any
version of the C library qsort functions, not only for randomly generated data,
but also for various types of nearly sorted data. This experiment can help the
user to choose the best sorting algorithm for the hard sorting job at hand.
This work provides a platform for anyone to test their own sorting algorithm
against the best in the field.
</p>
{{{{ARTICLE_PARSER}}}}<p>We propose a new parallel framework for fast computation of inverse and
forward dynamics of articulated robots based on prefix sums (scans). We
re-investigate the well-known recursive Newton-Euler formulation of robot
dynamics and show that the forward-backward propagation process for robot
inverse dynamics is equivalent to two scan operations on certain semigroups. We
show that the state-of-the-art forward dynamics algorithms may almost
completely be cast into a sequence of scan operations, with unscannable parts
clearly identified. This suggests a serial-parallel hybrid approach for systems
with a moderate number of links. We implement our scan based algorithms on
Nvidia CUDA platform with performance compared with multithreading CPU-based
recursive algorithms; a significant level of acceleration is demonstrated.
</p>
{{{{ARTICLE_PARSER}}}}<p>Optimal transport is a powerful framework for computing distances between
probability distributions. We unify the two main approaches to optimal
transport, namely Monge-Kantorovitch and Sinkhorn-Cuturi, into what we define
as Tsallis regularized optimal transport (\trot). \trot~interpolates a rich
family of distortions from Wasserstein to Kullback-Leibler, encompassing as
well Pearson, Neyman and Hellinger divergences, to name a few. We show that
metric properties known for Sinkhorn-Cuturi generalize to \trot, and provide
efficient algorithms for finding the optimal transportation plan with formal
convergence proofs. We also present the first application of optimal transport
to the problem of ecological inference, that is, the reconstruction of joint
distributions from their marginals, a problem of large interest in the social
sciences. \trot~provides a convenient framework for ecological inference by
allowing to compute the joint distribution --- that is, the optimal
transportation plan itself --- when side information is available, which is
\textit{e.g.} typically what census represents in political science.
Experiments on data from the 2012 US presidential elections display the
potential of \trot~in delivering a faithful reconstruction of the joint
distribution of ethnic groups and voter preferences.
</p>
{{{{ARTICLE_PARSER}}}}<p>Millimeter wave (mmWave) vehicular communica tion systems have the potential
to improve traffic efficiency and safety. Lack of secure communication links,
however, may lead to a formidable set of abuses and attacks. To secure
communication links, a physical layer precoding technique for mmWave vehicular
communication systems is proposed in this paper. The proposed technique
exploits the large dimensional antenna arrays available at mmWave systems to
produce direction dependent transmission. This results in coherent transmission
to the legitimate receiver and artificial noise that jams eavesdroppers with
sensitive receivers. Theoretical and numerical results demonstrate the validity
and effectiveness of the proposed technique and show that the proposed
technique provides high secrecy throughput when compared to conventional array
and switched array transmission techniques.
</p>
{{{{ARTICLE_PARSER}}}}<p>Inference on time series data is a common requirement in many scientific
disciplines and internet of things (IoT) applications, yet there are few
resources available to domain scientists to easily, robustly, and repeatably
build such complex inference workflows: traditional statistical models of time
series are often too rigid to explain complex time domain behavior, while
popular machine learning packages require already-featurized dataset inputs.
Moreover, the software engineering tasks required to instantiate the
computational platform are daunting. cesium is an end-to-end time series
analysis framework, consisting of a Python library as well as a web front-end
interface, that allows researchers to featurize raw data and apply modern
machine learning techniques in a simple, reproducible, and extensible way.
Users can apply out-of-the-box feature engineering workflows as well as save
and replay their own analyses. Any steps taken in the front end can also be
exported to a Jupyter notebook, so users can iterate between possible models
within the front end and then fine-tune their analysis using the additional
capabilities of the back-end library. The open-source packages make us of many
use modern Python toolkits, including xarray, dask, Celery, Flask, and
scikit-learn.
</p>
{{{{ARTICLE_PARSER}}}}<p>Relational learning deals with data that are characterized by relational
structures. An important task is collective classification, which is to jointly
classify networked objects. While it holds a great promise to produce a better
accuracy than non-collective classifiers, collective classification is
computational challenging and has not leveraged on the recent breakthroughs of
deep learning. We present Column Network (CLN), a novel deep learning model for
collective classification in multi-relational domains. CLN has many desirable
theoretical properties: (i) it encodes multi-relations between any two
instances; (ii) it is deep and compact, allowing complex functions to be
approximated at the network level with a small set of free parameters; (iii)
local and relational features are learned simultaneously; (iv) long-range,
higher-order dependencies between instances are supported naturally; and (v)
crucially, learning and inference are efficient, linear in the size of the
network and the number of relations. We evaluate CLN on multiple real-world
applications: (a) delay prediction in software projects, (b) PubMed Diabetes
publication classification and (c) film genre classification. In all
applications, CLN demonstrates a higher accuracy than state-of-the-art rivals.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study various versions of the problem of scheduling platoons of autonomous
vehicles through an unregulated intersection, where an algorithm must schedule
which platoons should wait so that others can go through, so as to minimize the
maximum delay for any vehicle. We provide polynomial-time algorithms for
constructing such schedules for a $k$-way merge intersection, for constant $k$,
and for a crossing intersection involving two-way traffic. We also show that
the more general problem of scheduling autonomous platoons through an
intersection that includes both a $k$-way merge, for non-constant $k$, and a
crossing of two-way traffic is NP-complete.
</p>
{{{{ARTICLE_PARSER}}}}<p>Security researchers have stated that the core concept behind current
implementations of access control predates the Internet. These assertions are
made to pinpoint that there is a foundational gap in this field, and one should
consider revisiting the concepts from the ground up. Moreover, Insider threats,
which are an increasing threat vector against organizations are also associated
with the failure of access control. Access control models derived from access
control matrix encompass three sets of entities, Subjects, Objects and
Operations. Typically, objects are considered to be files and operations are
regarded as Read, Write, and Execute. This implies an `open sesame' approach
when granting access to data, i.e. once access is granted, there is no
restriction on command executions. Inspired by Functional Encryption, we
propose applying access authorizations at a much finer granularity, but instead
of an ad-hoc or computationally hard cryptographic approach, we postulate a
foundational transformation to access control. From an abstract viewpoint, we
suggest storing access authorizations as a three-dimensional tensor, which we
call Access Control Tensor (ACT). In Function-based Access Control (FBAC),
applications do not give blind folded execution right and can only invoke
commands that have been authorized for data segments. In other words, one might
be authorized to use a certain command on one object, while being forbidden to
use exactly the same command on another object. The theoretical foundations of
FBAC are presented along with Policy, Enforcement and Implementation (PEI)
requirements of it. A critical analysis of the advantages of deploying FBAC,
how it will result in developing a new generation of applications, and
compatibility with existing models and systems is also included. Finally, a
proof of concept implementation of FBAC is presented.
</p>
{{{{ARTICLE_PARSER}}}}<p>Integrating optical circuit switches in data-centers is an on-going research
challenge. In recent years, state-of-the-art solutions introduce hybrid
packet/circuit architectures for different optical circuit switch technologies,
control techniques, and traffic rerouting methods. These solutions are based on
separated packet and circuit planes which do not have the ability to utilize an
optical circuit with flows that do not arrive from or delivered to switches
directly connected to the circuit's end-points. Moreover, current SDN-based
elephant flow rerouting methods require a forwarding rule for each flow, which
raise scalability issues. In this paper, we present C-Share -- a practical,
scalable SDN-based circuit sharing solution for data center networks. C-Share
inherently enable elephant flows to share optical circuits by exploiting a flat
upper tier network topology. C-Share is based on a scalable and decoupled
SDN-based elephant flow rerouting method comprised of elephant flow detection,
tagging and identification, which is utilized by using a prevalent network
sampling method (e.g., sFlow). C-Share requires only a single OpenFlow rule for
each optical circuit, and therefore significantly reduces the required OpenFlow
rule entry footprint and setup rule rate. It also mitigates the OpenFlow
outbound latency for subsequent elephant flows. We implement a proof-of-concept
system for C-Share based on Mininet, and test the scalability of C-Share by
using an event driven simulation. Our results show a consistent increase in the
mice/elephant flow separation in the network which, in turn, improves both
network throughput and flow completion time.
</p>
{{{{ARTICLE_PARSER}}}}<p>The implementation of device-to-device (D2D) underlaying or overlaying
pre-existing cellular networks has received much attention due to the potential
of enhancing the total cell throughput, reducing power consumption and
increasing the instantaneous data rate. In this paper we propose a distributed
power allocation scheme for D2D OFDMA communications and, in particular, we
consider the two operating modes amenable to a distributed implementation:
dedicated and reuse modes. The proposed schemes address the problem of
maximizing the users' sum rate subject to power constraints, which is known to
be nonconvex and, as such, extremely difficult to be solved exactly. We propose
here a fresh approach to this well-known problem, capitalizing on the fact that
the power allocation problem can be modeled as a potential game. Exploiting the
potential games property of converging under better response dynamics, we
propose two fully distributed iterative algorithms, one for each operation mode
considered, where each user updates sequentially and autonomously its power
allocation. Numerical results, computed for several different user scenarios,
show that the proposed methods, which converge to one of the local maxima of
the objective function, exhibit performance close to the maximum achievable
optimum and outperform other schemes presented in the literature.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper addresses channel estimation and data equalization on
frequency-selective 1-bit quantized Multiple Input-Multiple Output (MIMO)
systems. No joint processing or Channel State Information is assumed at the
transmitter, and therefore our findings are also applicable to the uplink of
Multi-User MIMO systems. System models for both Orthogonal Division Frequency
Multiplexing (OFDM) and single-carrier schemes are developed. A Cram\'er-Rao
Lower Bound for the estimation problems is derived. The two nonlinear
algorithms Expectation Maximization (EM) and Generalized Approximate Message
Passing (GAMP) are adapted to the problems, and a linear method based on the
Bussgang theorem is proposed. In the OFDM case, the linear method enables
subcarrier-wise estimation, greatly reducing computational complexity.
Simulations are carried out to compare the algorithms with different settings.
The results turn out to be close to the Cram\'er-Rao bound in the low Signal to
Noise Ratio (SNR) region. The OFDM setting is more suitable for the nonlinear
algorithms, and that the linear methods incur a performance loss with respect
to the nonlinear approaches. In the relevant low and medium SNR regions, the
loss amounts to 2-3 dB and might well be justified in exchange for the reduced
computational effort, especially in Massive MIMO settings.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper introduces matrix product state (MPS) decomposition as a new and
systematic method to compress multidimensional data represented by higher-order
tensors. It solves two major bottlenecks in tensor compression: computation and
compression quality. Regardless of tensor order, MPS compresses tensors to
matrices of moderate dimension which can be used for classification. Mainly
based on a successive sequence of singular value decompositions (SVD), MPS is
quite simple to implement and arrives at the global optimal matrix, bypassing
local alternating optimization, which is not only computationally expensive but
cannot yield the global solution. Benchmark results show that MPS can achieve
better classification performance with favorable computation cost compared to
other tensor compression methods.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper we consider the dyadic effect introduced in complex networks
when nodes are distinguished by a binary characteristic. Under these
circumstances two independent parameters, namely dyadicity and heterophilicity,
are able to measure how much the assigned characteristic affects the network
topology. All possible configurations can be represented in a phase diagram
lying in a two-dimensional space that represents the feasible region of the
dyadic effect, which is bound by two upper bounds on dyadicity and
heterophilicity. Using some network's structural arguments, we are able to
improve such upper bounds and introduce two new lower bounds, providing a
reduction of the feasible region of the dyadic effect as well as constraining
dyadicity and heterophilicity within a specific range. Some computational
experiences show the bounds' effectiveness and their usefulness with regards to
different classes of networks.
</p>
{{{{ARTICLE_PARSER}}}}<p>We propose a novel multiple-access technique to overcome the shortcomings of
the current proposals for the future releases of Long-Term Evolution (LTE). We
provide a unified radio access system that efficiently and flexibly integrates
both traditional cellular services and machine-to-machine (M2M) connections
arising from Internet-of-Things (IoT) applications. The proposed solution,
referred to as multi-service oriented multiple access (MOMA), is based on a)
establishing separate classes of users using relevant criteria that go beyond
the simple handheld-IoT device split, b) service-dependent hierarchical
spreading of the data signals and c) a mix of multiuser and single-user
detection schemes at the receiver. Signal spreading in MOMA allows to handle
densely connected devices with different quality-of-service (QoS) profiles and
at the same time its flexible receiver structure allows to allocate the
receiver computational resources to the connections that need it most. This
yields a scalable and efficient use of the available radio resources and a
better service integration. While providing significant advantages for key
future communications scenarios, MOMA can be incorporated into LTE with a
limited impact on the protocol structure and the signaling overhead.
</p>
{{{{ARTICLE_PARSER}}}}<p>The Stream Control Transmission Protocol (SCTP) is a connection and message
oriented transport protocol. It supports multiple uni-directional streams in
each direction allowing user message sequence preservation within each stream.
This minimizes the re-sequencing delay at the receiver side in case of message
loss. The base protocol, although being optimized for small messages, supports
arbitrary large user messages by using fragmentation and reassembly at the cost
of adding delays at the sender side. To overcome this limitation, a protocol
extension called User Message Interleaving is currently being specified by the
Internet Engineering Task Force (IETF). This paper describes the new extension,
its integration and validation in the context of the INET framework.
</p>
{{{{ARTICLE_PARSER}}}}<p>Switching connectivity over multiple wireless interfaces of a mobile node is
essential when performing handover between heterogeneous networks. In such a
communication scenario, session continuity as experienced by the user can be
enhanced if soft handover is enabled through the concept of make-before-break.
However, some of the available networking protocols need to be modified to
support this feature. This work gives an insight into the supplementary modules
we implemented and the modification conducted in MIPv6 model of OMNeT++ to
facilitate soft handover and data offloading for mobile nodes.
</p>
{{{{ARTICLE_PARSER}}}}<p>Software Defined Networking (SDN) has been recently introduced as a new
communication paradigm in computer networks. By separating the control plane
from the data plane and entrusting packet forwarding to straightforward
switches, SDN makes it possible to deploy and run networks which are more
flexible to manage and easier to configure. This paper describes a set of
extensions for the INET framework, which allow researchers and network
designers to simulate SDN architectures and evaluate their performance and
security at design time. Together with performance evaluation and design
optimization of SDN networks, our extensions enable the simulation of SDN-based
anomaly detection and mitigation techniques, as well as the quantitative
evaluation of cyber-physical attacks and their impact on the network and
application. This work is an ongoing research activity, and we plan to propose
it for an official contribution to the INET framework.
</p>
{{{{ARTICLE_PARSER}}}}<p>A publicly available dataset for federated search reflecting a real web
environment has long been absent, making it difficult for researchers to test
the validity of their federated search algorithms for the web setting. We
present several experiments and analyses on resource selection on the web using
a recently released test collection containing the results from more than a
hundred real search engines, ranging from large general web search engines such
as Google, Bing and Yahoo to small domain-specific engines. First, we
experiment with estimating the size of uncooperative search engines on the web
using query based sampling and propose a new method using the ClueWeb09
dataset. We find the size estimates to be highly effective in resource
selection. Second, we show that an optimized federated search system based on
smaller web search engines can be an alternative to a system using large web
search engines. Third, we provide an empirical comparison of several popular
resource selection methods and find that these methods are not readily suitable
for resource selection on the web. Challenges include the sparse resource
descriptions and extremely skewed sizes of collections.
</p>
{{{{ARTICLE_PARSER}}}}<p>Many success stories involving deep neural networks are instances of
supervised learning, where available labels power gradient-based learning
methods. Creating such labels, however, can be expensive and thus there is
increasing interest in weak labels which only provide coarse information, with
uncertainty regarding time, location or value. Using such labels often leads to
considerable challenges for the learning process. Current methods for
weak-label training often employ standard supervised approaches that
additionally reassign or prune labels during the learning process. The
information gain, however, is often limited as only the importance of labels
where the network already yields reasonable results is boosted. We propose
treating weak-label training as an unsupervised problem and use the labels to
guide the representation learning to induce structure. To this end, we propose
two autoencoder extensions: class activity penalties and structured dropout. We
demonstrate the capabilities of our approach in the context of score-informed
source separation of music.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a cognitive model of opinion dynamics which studies the behavior
of a population of interacting individuals in the context of risk of natural
disaster. In particular, we investigate the response of the individuals to the
information received by institutional sources about the correct behaviors for
prevention and harm reduction. The results of our study show that alarmist
opinions are more likely to be adopted by populations, since worried people
</p>
{{{{ARTICLE_PARSER}}}}<p>We advocate the Loop-of-stencil-reduce pattern as a means of simplifying the
implementation of data-parallel programs on heterogeneous multi-core platforms.
Loop-of-stencil-reduce is general enough to subsume map, reduce, map-reduce,
stencil, stencil-reduce, and, crucially, their usage in a loop in both
data-parallel and streaming applications, or a combination of both. The pattern
makes it possible to deploy a single stencil computation kernel on different
GPUs. We discuss the implementation of Loop-of-stencil-reduce in FastFlow, a
framework for the implementation of applications based on the parallel
patterns. Experiments are presented to illustrate the use of
Loop-of-stencil-reduce in developing data-parallel kernels running on
heterogeneous systems.
</p>
{{{{ARTICLE_PARSER}}}}<p>Network-on-Chip (NoC) architecture has been proposed to solve the global
communication problem of complex Systems-on-Chips (SoCs). However, NoC testing
is a main challenging problem yet. In this article, we propose novel test
architecture for NoC router testing. The proposed test architecture uses both
advantages of Software Based Self-Testing (SBST) and Built in Self-Testing
(BIST) methodologies. In this methodology, we propose custom test instructions
with regarding the ISA of NoC processors. These custom instructions are
responsible for applying test patterns and collecting their responses. In the
proposed approach, the processor cores are used to manage the whole test
operation of their corresponding router. So there is no need for expensive
Automated Test Equipment (ATE) to access internal circuit of NoC, while
bringing an efficient at-speed testing paradigm and also No need to packet
transmission between NoC nodes to test the communication blocks or router. As a
case study the Heracles architecture is used and the experimental results show
the efficiency of the proposed test methodology over functional test strategy
in the term of test time and fault coverage. With only 7.2% hardware overhead
in router circuit, the proposed architecture reveals 74% Percentage decreases
in test time and 4% percentage increases in fault coverage.
</p>
{{{{ARTICLE_PARSER}}}}<p>Processing of sparse matrices in blocks often yields higher efficiency of
matrix operations performed within computer programs. On the other hand,
partitioning of matrices into blocks represents itself an additional runtime
overhead. The presented study analyses 563 representative sparse matrices
arising from multiple application problems types %from the University of
Florida Sparse Matrix Collection with respect to their partitioning into
uniformly-sized blocks. Different block sizes and different ways of storing
blocks in memory are considered and statistically evaluated using the matrix
memory footprint criterion. Memory footprints of matrices are additionally
compared with their lower bounds and with the CSR storage format. Based on the
obtained results, we provide generic suggestions for reduction of partitioning
overhead and for efficient storage of partitioned sparse matrices in a computer
memory.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we show the equivalency of weak and strong secrecy conditions
for a large class of secure network coding problems. When we restrict to linear
operations, we show the equivalency of \"perfect secrecy and zero-error
constraints\" with \"weak secrecy and $\epsilon$-error constraints\".
</p>
{{{{ARTICLE_PARSER}}}}<p>The Minimum Eccentricity Shortest Path (MESP) Problem consists in determining
a shortest path (a path whose length is the distance between its extremities)
of minimum eccentricity in a graph. It was introduced by Dragan and Leitert [9]
who described a linear-time algorithm which is an 8-approximation of the
problem. In this paper, we study deeper the double-BFS procedure used in that
algorithm and extend it to obtain a linear-time 3-approximation algorithm. We
moreover study the link between the MESP problem and the notion of laminarity,
introduced by V{\\"o}lkel et al [12], corresponding to its restriction to a
diameter (i.e. a shortest path of maximum length), and show tight bounds
between MESP and laminarity parameters.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a Model-Predictive Controller (MPC) for multi-contact locomotion
where predictive optimizations are realized by Time-Optimal Path
Parameterization (TOPP). The key feature of this design is that, contrary to
existing planners where step timings are provided as inputs, here the timing
between contact switches is computed as output to a linear optimization problem
based on a dynamic model of the robot. This is particularly appealing to
multi-contact locomotion, where proper timings depend on the terrain topology
and suitable heuristics are unknown. Thanks to recent advances in multi-contact
stability computations, we improve the performance of TOPP for COM
trajectories, which allows us to integrate it into a fast control loop. We
implement the complete control pipeline and showcase it in simulations where a
model of the HRP-4 humanoid climbs up and down a series of hills.
</p>
{{{{ARTICLE_PARSER}}}}<p>The study of MDS self-dual codes has attracted lots of attention in recent
years.
</p>
<p>There are many papers on determining existence of $q-$ary MDS self-dual codes
for various lengths.
</p>
<p>There are not existence of $q-$ary MDS self-dual codes of some lengths, even
these lengths $&lt; q$.
</p>
<p>We generalize MDS Euclidean self-dual codes to near MDS Euclidean self-dual
codes and near MDS isodual codes.
</p>
<p>And we obtain many new near MDS isodual codes from extended negacyclic duadic
codes and we obtain many new MDS Euclidean self-dual codes from MDS Euclidean
self-dual codes.
</p>
<p>We generalize MDS Hermitian self-dual codes to near MDS Hermitian self-dual
codes.
</p>
<p>We obtain near MDS Hermitian self-dual codes from extended negacyclic duadic
codes and from MDS Hermitian self-dual codes.
</p>
{{{{ARTICLE_PARSER}}}}<p>Simulation is widely adopted in the study of modern computer networks. In
this context, OMNeT++ provides a set of very effective tools that span from the
definition of the network, to the automation of simulation execution and quick
result representation. However, as network models become more and more complex
to cope with the evolution of network systems, the amount of simulation
factors, the number of simulated nodes and the size of results grow
consequently, leading to simulations with larger scale. In this work, we
perform a critical analysis of the tools provided by OMNeT++ in case of such
large-scale simulations. We then propose a unified and flexible software
architecture to support simulation automation.
</p>
{{{{ARTICLE_PARSER}}}}<p>Wi-Fi Direct is a popular wireless technology which is integrated in most of
today's smartphones and tablets. This technology allows a set of devices to
dynamically negotiate and select a group owner which plays the role access
point. This important feature is the strength of Wi-Fi Direct and makes it more
and more widely used in telecommunications networks. In this paper, we present
the implementation of Wi-Fi Direct in the INET framework of OMNeT++. We have
implemented the main procedures of Wi-Fi Direct such as discovery, negotiation
and group formation. The implementation has been validated by two test
scenarios which show the conformity of the implementation to the protocol
specification.
</p>
{{{{ARTICLE_PARSER}}}}<p>We show that clustered planarity with overlapping clusters as introduced by
Didimo et al. can be solved in polynomial time if each cluster induces a
connected subgraph. It can be solved in linear time if the set of clusters is
the union of two partitions of the vertex set such that, for each cluster, both
the cluster and its complement, induce connected subgraphs. Clustered planarity
with overlapping clusters is NP-complete, even if restricted to instances where
the underlying graph is 2-connected, the set of clusters is the union of two
partitions and each cluster contains at most two connected components while
their complements contain at most three connected components.
</p>
{{{{ARTICLE_PARSER}}}}<p>-In this work, we revisit fast dimension reduction approaches, as with random
projections and random sampling. Our goal is to summarize the data to decrease
computational costs and memory footprint of subsequent analysis. Such dimension
reduction can be very efficient when the signals of interest have a strong
structure, such as with images. We focus on this setting and investigate
feature clustering schemes for data reductions that capture this structure. An
impediment to fast dimension reduction is that good clustering comes with large
algorithmic costs. We address it by contributing a linear-time agglomerative
clustering scheme, Recursive Nearest Agglomeration (ReNA). Unlike existing fast
agglomerative schemes, it avoids the creation of giant clusters. We empirically
validate that it approximates the data as well as traditional
variance-minimizing clustering schemes that have a quadratic complexity. In
addition, we analyze signal approximation with feature clustering and show that
it can remove noise, improving subsequent analysis steps. As a consequence,
data reduction by clustering features with ReNA yields very fast and accurate
models, enabling to process large datasets on budget. Our theoretical analysis
is backed by extensive experiments on publicly-available data that illustrate
the computation efficiency and the denoising properties of the resulting
dimension reduction scheme.
</p>
{{{{ARTICLE_PARSER}}}}<p>Recently, Holt and McMillan [Bionformatics 2014, ACM-BCB 2014] have proposed
a simple and elegant algorithm to merge the Burrows-Wheeler transforms of a
family of strings. In this paper we show that the H&amp;M algorithm can be improved
so that, in addition to merging the BWTs, it can also merge the Longest Common
Prefix (LCP) arrays. The new algorithm, called Gap because of how it operates,
has the same asymptotic cost as the H&amp;M algorithm and requires additional space
only for storing the LCP values.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a new approach for neural machine translation (NMT) using the
morphological and grammatical decomposition of the words (factors) in the
output side of the neural network. This architecture addresses two main
problems occurring in MT, namely dealing with a large target language
vocabulary and the out of vocabulary (OOV) words. By the means of factors, we
are able to handle larger vocabulary and reduce the training time (for systems
with equivalent target language vocabulary size). In addition, we can produce
new words that are not in the vocabulary. We use a morphological analyser to
get a factored representation of each word (lemmas, Part of Speech tag, tense,
person, gender and number). We have extended the NMT approach with attention
mechanism in order to have two different outputs, one for the lemmas and the
other for the rest of the factors. The final translation is built using some
\textit{a priori} linguistic information. We compare our extension with a
word-based NMT system. The experiments, performed on the IWSLT'15 dataset
translating from English to French, show that while the performance do not
always increase, the system can manage a much larger vocabulary and
consistently reduce the OOV rate. We observe up to 2% BLEU point improvement in
a simulated out of domain translation setup.
</p>
{{{{ARTICLE_PARSER}}}}<p>We propose a decentralized Maximum Likelihood solution for estimating the
stochastic renewable power generation and demand in single bus Direct Current
(DC) MicroGrids (MGs), with high penetration of droop controlled power
electronic converters. The solution relies on the fact that the primary control
parameters are set in accordance with the local power generation status of the
generators. Therefore, the steady state voltage is inherently dependent on the
generation capacities and the load, through a non-linear parametric model,
which can be estimated. To have a well conditioned estimation problem, our
solution avoids the use of an external communication interface and utilizes
controlled voltage disturbances to perform distributed training. Using this
tool, we develop an efficient, decentralized Maximum Likelihood Estimator (MLE)
and formulate the sufficient condition for the existence of the globally
optimal solution. The numerical results illustrate the promising performance of
our MLE algorithm.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this article we propose a method to refine the clustering results obtained
with the nonnegative matrix factorization (NMF) technique, imposing consistency
constraints on the final labeling of the data. The research community focused
its effort on the initialization and on the optimization part of this method,
without paying attention to the final cluster assignments. We propose a game
theoretic framework in which each object to be clustered is represented as a
player, which has to choose its cluster membership. The information obtained
with NMF is used to initialize the strategy space of the players and a weighted
graph is used to model the interactions among the players. These interactions
allow the players to choose a cluster which is coherent with the clusters
chosen by similar players, a property which is not guaranteed by NMF, since it
produces a soft clustering of the data. The results on common benchmarks show
that our model is able to improve the performances of many NMF formulations.
</p>
{{{{ARTICLE_PARSER}}}}<p>We seek to deepen understanding of the micro-foundations of
institutionalization while contributing to a sociological theory of markets by
investigating the puzzle of price bubbles in financial markets. We find that
such markets, despite textbook conditions of high efficiency -- perfect
information, atomistic agents, no uncertainty -- quickly develop patterns
consistent with institutionalization processes.
</p>
{{{{ARTICLE_PARSER}}}}<p>We analyze the structure of the state space of chess by means of transition
path sampling Monte Carlo simulation. Based on the typical number of moves
required to transpose a given configuration of chess pieces into another, we
conclude that the state space consists of several pockets between which
transitions are rare. Skilled players explore an even smaller subset of
positions that populate some of these pockets only very sparsely. These results
suggest that the usual measures to estimate both, the size of the state space
and the size of the tree of legal moves, are not unique indicators of the
complexity of the game, but that topological considerations are equally
important.
</p>
{{{{ARTICLE_PARSER}}}}<p>The main aim of our study was to analyse the effects of a virtual environment
on social conformity, with particular attention to the effects of different
types of task and psychological variables on social influence, on one side, and
to the neural correlates related to conformity, measured by means of an Emotiv
EPOC device on the other. For our purpose, we replicated the famous Asch's
visual task and created two new tasks of increasing ambiguity, assessed through
the calculation of the item's entropy. We also administered five scales in
order to assess different psychological traits. From the experiment, conducted
on 181 university students, emerged that conformity grows according to the
ambiguity of the task, but normative influence is significantly weaker in
virtual environments, if compared to face-to-face experiments. The analysed
psycho-logical traits, however, result not to be relatable to conformity, and
they only affect the subjects' response times. From the ERP (Event-related
potentials) analysis, we detected N200 and P300 components comparing the plots
of conformist and non-conformist subjects, alongside with the detection of
their Late Positive Potential, Readiness Potential, and Error-Related
Negativity, which appear consistently different for the two typologies.
</p>
{{{{ARTICLE_PARSER}}}}<p>Detecting small obstacles on the road ahead is a critical part of the driving
task which has to be mastered by fully autonomous cars. In this paper, we
present a method based on stereo vision to reliably detect such obstacles from
a moving vehicle. The proposed algorithm performs statistical hypothesis tests
in disparity space directly on stereo image data, assessing freespace and
obstacle hypotheses on independent local patches. This detection approach does
not depend on a global road model and handles both static and moving obstacles.
For evaluation, we employ a novel lost-cargo image sequence dataset comprising
more than two thousand frames with pixelwise annotations of obstacle and
free-space and provide a thorough comparison to several stereo-based baseline
methods. The dataset will be made available to the community to foster further
research on this important topic. The proposed approach outperforms all
considered baselines in our evaluations on both pixel and object level and runs
at frame rates of up to 20 Hz on 2 mega-pixel stereo imagery. Small obstacles
down to the height of 5 cm can successfully be detected at 20 m distance at low
false positive rates.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper we show the results of our studies carried out in the framework
of the European Project SciCafe2.0 in the area of Participatory Engagement
models. We present a methodological approach built on participative engagements
models and holistic framework for problem situation clarification and solution
impacts assessment. Several online platforms for social engagement have been
analysed to extract the main patterns of participative engagement. We present
our own experiments through the SciCafe2.0 Platform and our insights from
requirements elicitation.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper we provide two introductory analyses of CAPs, based exclusively
on the analysis of documents found on the Internet. The first analysis allowed
us to investigate the world of CAPs, in particular for what concerned their
status (dead or alive), the scope of those platforms and the typology of users.
In order to develop a more accurate model of CAPs, and to understand more
deeply the motivation of the users and the type of expected payoff, we analysed
those CAPs from the above list that are still alive and we used two models
developed for what concerned the virtual community and the collective
intelligence.
</p>
{{{{ARTICLE_PARSER}}}}<p>Reputation systems are currently used, often with success, to ensure the
functioning of online services as well as of e-commerce sites. Despite the
relationship between reputation and material cooperative behaviours is quite
supported, less obvious appears the relationship with informative behaviours,
which are crucial for the transmission of reputational information and
therefore for the maintenance of cooperation among individuals. The purpose of
this study was to verify how reputation affects cooperation dynamics in virtual
environment, within a social dilemma situation (i.e., where there are
incentives to act selfishly). The results confirm that reputation can activate
prosocial conducts, however it highlights also the limitations and distortions
that reputation can create.
</p>
{{{{ARTICLE_PARSER}}}}<p>X-ray Computed Tomography (CT) reconstruction from sparse number of views is
becoming a powerful way to reduce either the radiation dose or the acquisition
time in CT systems but still requires a huge computational time. This paper
introduces an approximate Bayesian inference framework for CT reconstruction
based on a family of denoising approximate message passing (DCT-AMP) algorithms
able to improve both the convergence speed and the reconstruction quality.
Approximate Message Passing for Compressed Sensing has been extensively
analysed for random linear measurements but there are still not clear solutions
on how AMP should be modified and how it performs with real world problems. In
particular to overcome the convergence issues of DCT-AMP with structured
measurement matrices, we propose a disjoint preconditioned version of the
algorithm tailored for both the geometric system model and the noise model. In
addition the Bayesian DCT-AMP formulation allows to measure how the current
estimate is close to the prediction by analysing the state evolution. This work
aims to provide a proof of concept to demonstrate that approximate Bayesian
reconstruction algorithms can be tailored for ill conditioned, underdetermined
real problems such CT imaging. Experiments with simulated and real CT baggage
scans confirm that the performance of the proposed algorithms are comparable
and can even outperform traditional statistical X-ray CT reconstruction
optimization solvers.
</p>
{{{{ARTICLE_PARSER}}}}<p>The main intreest of this study was to investigate the phenomenon of
collective intelligence in an anonymous virtual environment developed for this
purpose. In particular, we were interested in studiyng how dividing a fixed
community in different group size, which, in different phases of the
experiment, works to solve tasks of different complexity, influences the social
problem solving process. The experiments, which have involved 216 university
students, showed that the cooperative behaviour is stronger in small groups
facing complex tasks: the cooperation probability negatively correlated with
both the group size and easiness of task. Individuals seem to activate a
collective intelligence heuristics when the problem is too complex. Some
psychosocial variables were considered in order to check how they affect the
cooperative behaviour of participants, but they do not seem to have a
significant impact on individual cooperation probability, supporting the idea
that a partial de-individualization operates in virtual environments.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we address a class of distributed optimization problems in the
presence of inter-agent communication delays based on passivity. We first focus
on unconstrained distributed optimization and provide a passivity-based
perspective for distributed optimization algorithms. This perspective allows us
to handle communication delays while using scattering transformation. Moreover,
we extend the results to constrained distributed optimization, where it is
shown that the problem is solved by just adding one more feedback loop of a
passive system to the solution of the unconstrained ones. We also show that
delays can be incorporated in the same way as the unconstrained problems.
Finally, the algorithm is applied to a visual human localization problem using
a pedestrian detection algorithm.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this briefing report, we introduce a new concept (war algorithms) that
elevates algorithmically-derived choices and decisions to a, and perhaps the,
central concern regarding technical autonomy in war. We thereby aim to shed
light on and recast the discussion regarding autonomous weapon systems. We
define war algorithm as any algorithm that is expressed in computer code, that
is effectuated through a constructed system, and that is capable of operating
in relation to armed conflict. In introducing this concept, our foundational
technological concern is the capability of a constructed system, without
further human intervention, to help make and effectuate a decision or choice of
a war algorithm. Distilled, the two core ingredients are an algorithm expressed
in computer code and a suitably capable constructed system. Through that lens,
we link international law and related accountability architectures to relevant
technologies. We sketch a three-part (non-exhaustive) approach that highlights
traditional and unconventional accountability avenues. We focus largely on
international law because it is the only normative regime that purports, in key
respects but with important caveats, to be both universal and uniform. By not
limiting our inquiry only to weapon systems, we take an expansive view, showing
how the broad concept of war algorithms might be susceptible to regulation, and
how those algorithms might already fit within the existing regulatory system
established by international law.
</p>
{{{{ARTICLE_PARSER}}}}<p>Maximal repetition of a string is the maximal length of a repeated substring.
This paper investigates maximal repetition of strings drawn from stationary
processes. Strengthening previous results, two new bounds for the almost sure
growth rate of maximal repetition are identified: an upper bound in terms of
conditional Renyi entropy and a lower bound in terms of Shannon entropy. The
proof of the upper bound applies an inequality for the distribution of
recurrence time, whereas the proof of the lower bound makes use of an
inequality for the expectation of subword complexity. In particular, it is
shown that a hyperlogarithmic growth of maximal repetition with respect to the
string length, recently observed for texts in natural language, may hold only
if the conditional Renyi entropy rate equals zero. According to this
observation, natural language cannot be faithfully modeled by a typical hidden
Markov process, which is a class of heuristic models widely used in
computational linguistics.
</p>
{{{{ARTICLE_PARSER}}}}<p>We show that the proof nets introduced in [Hughes &amp; van Glabbeek 2003, 2005]
for MALL (Multiplicative Additive Linear Logic, without units) identify
cut-free proofs modulo rule commutation: two cut-free proofs translate to the
same proof net if and only if one can be obtained from the other by a
succession of rule commutations. This result holds with and without the mix
rule, and we extend it with cut.
</p>
{{{{ARTICLE_PARSER}}}}<p>In the absence of reliable and accurate GPS, visual odometry (VO) has emerged
as an effective means of estimating the egomotion of robotic vehicles. Like any
dead-reckoning technique, VO suffers from unbounded accumulation of drift error
over time, but this accumulation can be limited by incorporating absolute
orientation information from, for example, a sun sensor. In this paper, we
leverage recent work on visual outdoor illumination estimation to show that
estimation error in a stereo VO pipeline can be reduced by inferring the sun
position from the same image stream used to compute VO, thereby gaining the
benefits of sun sensing without requiring a dedicated sun sensor or the sun to
be visible to the camera. We compare sun estimation methods based on
hand-crafted visual cues and Convolutional Neural Networks (CNNs) and
demonstrate our approach on a combined 7.8 km of urban driving from the popular
KITTI dataset, achieving up to a 43% reduction in translational average root
mean squared error (ARMSE) and a 59% reduction in final translational drift
error compared to pure VO alone.
</p>
{{{{ARTICLE_PARSER}}}}<p>Android is the most widely used smartphone OS with 82.8% market share in
2015. It is therefore the most widely targeted system by malware authors.
Researchers rely on dynamic analysis to extract malware behaviors and often use
emulators to do so. However, using emulators lead to new issues. Malware may
detect emulation and as a result it does not execute the payload to prevent the
analysis. Dealing with virtual device evasion is a never-ending war and comes
with a non-negligible computation cost. To overcome this state of affairs, we
propose a system that does not use virtual devices for analysing malware
behavior. Glassbox is a functional prototype for the dynamic analysis of
malware applications. It executes applications on real devices in a monitored
and controlled environment. It is a fully automated system that installs, tests
and extracts features from the application for further analysis. We present the
architecture of the platform and we compare it with existing Android dynamic
analysis platforms. Lastly, we evaluate the capacity of Glassbox to trigger
application behaviors by measuring the average coverage of basic blocks on the
AndroCoverage dataset. We show that it executes on average 13.52% more basic
blocks than the Monkey program.
</p>
{{{{ARTICLE_PARSER}}}}<p>Preference orderings are orderings of a set of items according to the
preferences (of judges). Such orderings arise in a variety of domains,
including group decision making, consumer marketing, voting and machine
learning. Measuring the mutual information and extracting the common patterns
in a set of preference orderings are key to these areas. In this paper we deal
with the representation of sets of preference orderings, the quantification of
the degree to which judges agree on their ordering of the items (i.e. the
concordance), and the efficient, meaningful description of such sets.
</p>
<p>We propose to represent the orderings in a subsequence-based feature space
and present a new algorithm to calculate the size of the set of all common
subsequences - the basis of a quantification of concordance, not only for pairs
of orderings but also for sets of orderings. The new algorithm is fast and
storage efficient with a time complexity of only $O(Nn^2)$ for the orderings of
$n$ items by $N$ judges and a space complexity of only $O(\min\{Nn,n^2\})$.
</p>
<p>Also, we propose to represent the set of all $N$ orderings through a smallest
set of covering preferences and present an algorithm to construct this smallest
covering set.
</p>
{{{{ARTICLE_PARSER}}}}<p>We start by comparing two popular k-medoids algorithms, clarans (Ng et al.
2005) and vik (Park and Jun, 2009) and find that clarans consistently results
in better clusterings. We provide an explanation for this finding in terms of
local minima of their respective loss functions. We then discuss how clarans
can be accelerated by using the triangle inequality and early proposal
rejection, making it a viable algorithm for large datasets.
</p>
<p>The observation that clarans finds better minima than vik, along with the
similarity between vik and the standard k-means algorithm, suggests that
clarans may be an effective k-means initialiser. We show that this is indeed
the case, with clarans outperforming other popular seeding algorithms such as
k-means++ (Arthur and Vassilvitskii, 2007) on 20/20 datasets, with a mean
decrease in energy of 2.2%.
</p>
{{{{ARTICLE_PARSER}}}}<p>We propose a theoretical framework to study the eigenvalue spectra of the
controllability Gramian of systems with random state matrices, such as
networked systems with a random graph structure. Using random matrix theory, we
provide expressions for the moments of the eigenvalue distribution of the
controllability Gramian. These moments can then be used to derive useful
properties of the eigenvalue distribution of the Gramian (in some cases, even
closed-form expressions for the distribution). We illustrate this framework by
considering system matrices derived from common random graph and matrix
ensembles, such as the Wigner ensemble, the Gaussian Orthogonal Ensemble (GOE),
and random regular graphs. Subsequently, we illustrate how the eigenvalue
distribution of the Gramian can be used to draw conclusions about the energy
required to control random system.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper describes the Robotarium -- a remotely accessible, multi-robot
research facility. The impetus behind the Robotarium is that multi-robot
testbeds constitute an integral and essential part of the multi-robot research
cycle, yet they are expensive, complex, and time-consuming to develop, operate,
and maintain. These resource constraints, in turn, limit access for large
groups of researchers and students, which is what the Robotarium is remedying
by providing users with remote access to a state-of-the-art multi-robot test
facility. This paper details the design and operation of the Robotarium and
discusses the considerations one must take when making complex hardware
remotely accessible. In particular, safety must be built into the system
already at the design phase without overly constraining what coordinated
control programs users can upload and execute, which calls for minimally
invasive safety routines with provable performance guarantees.
</p>
{{{{ARTICLE_PARSER}}}}<p>Due to the enormous variety of application scenarios and ubiquity,Internet of
Things (IoT) brought a new perspective of applications for the current and
future Internet. The Wireless Sensor Networks provide key devices for
developing the IoT communication paradigm, such as the sensors collecting
various kind of information and the routing and MAC protocols. However, this
type of network has strong power consumption and transmission capacity
restrictions (low speed wireless links and subject to interference). In this
context, it is necessary to develop solutions that enable a more efficient
communication based on the optimized utilization of the network resources. This
papers aims to present a multi-objective routing algorithm, named Routing-Aware
of path Length, Link quality, and traffic Load (RALL), that seeks to balance
three objectives: to minimize bottlenecks, to minimize path length, and to
avoid links with low quality. RALL results in good performance when taking into
consideration delivery rate, overhead, delay, and power consumption.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper we present a novel dataset for a critical aspect of autonomous
driving, the joint attention that must occur between drivers and of
pedestrians, cyclists or other drivers. This dataset is produced with the
intention of demonstrating the behavioral variability of traffic participants.
We also show how visual complexity of the behaviors and scene understanding is
affected by various factors such as different weather conditions, geographical
locations, traffic and demographics of the people involved. The ground truth
data conveys information regarding the location of participants (bounding
boxes), the physical conditions (e.g. lighting and speed) and the behavior of
the parties involved.
</p>
{{{{ARTICLE_PARSER}}}}<p>MICROMVP is an affordable, portable, and open source micro-scale mobile robot
platform designed for robotics research and education. As a complete and unique
multi-vehicle platform enabled by 3D printing and the maker culture, MICROMVP
can be easily reproduced and requires little maintenance: a set of six micro
vehicles, each measuring 8x5x6 cubic centimeters and weighing under 100 grams,
and the accompanying tracking platform can be fully assembled in under two
hours, all from readily available components. In this paper, we describe
MICROMVP's hardware and software architecture, and the design thoughts that go
into the making of the platform. The capabilities of MICROMVP APIs are then
demonstrated with several single- and multi-robot path and motion planning
algorithms. MICROMVP supports all common operation systems.
</p>
{{{{ARTICLE_PARSER}}}}<p>Gradient descent optimization algorithms, while increasingly popular, are
often used as black-box optimizers, as practical explanations of their
strengths and weaknesses are hard to come by. This article aims to provide the
reader with intuitions with regard to the behaviour of different algorithms
that will allow her to put them to use. In the course of this overview, we look
at different variants of gradient descent, summarize challenges, introduce the
most common optimization algorithms, review architectures in a parallel and
distributed setting, and investigate additional strategies for optimizing
gradient descent.
</p>
{{{{ARTICLE_PARSER}}}}<p>Current top-performing blind perceptual image quality prediction models are
generally trained on legacy databases of human quality opinion scores on
synthetically distorted images. Therefore they learn image features that
effectively predict human visual quality judgments of inauthentic, and usually
isolated (single) distortions. However, real-world images usually contain
complex, composite mixtures of multiple distortions. We study the perceptually
relevant natural scene statistics of such authentically distorted images, in
different color spaces and transform domains. We propose a bag of feature-maps
approach which avoids assumptions about the type of distortion(s) contained in
an image, focusing instead on capturing consistencies, or departures therefrom,
of the statistics of real world images. Using a large database of authentically
distorted images, human opinions of them, and bags of features computed on
them, we train a regressor to conduct image quality prediction. We demonstrate
the competence of the features towards improving automatic perceptual quality
prediction by testing a learned algorithm using them on a benchmark legacy
database as well as on a newly introduced distortion-realistic resource called
the LIVE In the Wild Image Quality Challenge Database. We extensively evaluate
the perceptual quality prediction model and algorithm and show that it is able
to achieve good quality prediction power that is better than other leading
models.
</p>
{{{{ARTICLE_PARSER}}}}<p>Transport-based techniques for signal and data analysis have received
increased attention recently. Given their abilities to provide accurate
generative models for signal intensities and other data distributions, they
have been used in a variety of applications including content-based retrieval,
cancer detection, image super-resolution, and statistical machine learning, to
name a few, and shown to produce state of the art in several applications.
Moreover, the geometric characteristics of transport-related metrics have
inspired new kinds of algorithms for interpreting the meaning of data
distributions. Here we provide an overview of the mathematical underpinnings of
mass transport-related methods, including numerical implementation, as well as
a review, with demonstrations, of several applications.
</p>
{{{{ARTICLE_PARSER}}}}<p>This work investigates style and topic aspects of language in online
communities: looking at both utility as an identifier of the community and
correlation with community reception of content. Style is characterized using a
hybrid word and part-of-speech tag n-gram language model, while topic is
represented using Latent Dirichlet Allocation. Experiments with several Reddit
forums show that style is a better indicator of community identity than topic,
even for communities organized around specific topics. Further, there is a
positive correlation between the community reception to a contribution and the
style similarity to that community, but not so for topic similarity.
</p>
{{{{ARTICLE_PARSER}}}}<p>As OS-level virtualization technology usually imposes little overhead on
virtual machine start-up and running, it provides an excellent choice for
building intrusion/fault tolerant applications that require redundancy and
frequent invocation. When developing Windows OS-level virtual machine, however,
people will inevitably face the challenge of confining Windows Inter-Process
Communications (IPC). As IPC on Windows platform is more complex than UNIX
style OS and most of the programs on Windows are not open-source, it is
difficult to discover all of the performed IPCs and confine them. In this
paper, we propose three general principles to confine IPC on Windows OS and a
novel IPC confinement mechanism based on the principles. With the mechanism,
for the first time from the literature, we successfully virtualized RPC System
Service (RPCSS) and Internet Information Server (IIS) on Feather-weight Virtual
Machine (FVM). Experimental results demonstrate that multiple IIS web server
instances can simultaneously run on single Windows OS with much less
performance overhead than other popular VM technology, offering a good basis
for constructing dependable system.
</p>
{{{{ARTICLE_PARSER}}}}<p>We investigate mechanism design without payments when agents have different
types of preferences. Contrary to most settings in the literature where agents
have the same preference, e.g. in the facility location games all agents would
like to stay close to (or away from) the facility, we demonstrate the
limitation of mechanism design without payments when agents have different
preferences by introducing exchanging phases. We consider two types of
exchanging phases. The first model is called central exchanges where the
exchanges are performed by a central authority. The other model is called
individual exchanges where agents exchange their outcomes by themselves. By
using facility location games as an example, we provide a truthful mechanism
that optimizes social welfare in central exchanges. We also provide a
universally truthful randomized mechanism that achieves at least a half of the
optimal social welfare in individual exchanges.
</p>
{{{{ARTICLE_PARSER}}}}<p>OS-level virtualization incurs smaller start-up and run-time overhead than
HAL-based virtualization and thus forms an important building block for
developing fault-tolerant and intrusion-tolerant applications. A complete
implementation of OS-level virtualization on the Windows platform requires
virtualization of Windows services, such as system services like the Remote
Procedure Call Server Service (RPCSS), because they are essentially extensions
of the kernel. As Windows system services work very differently from their
counterparts on UNIX-style OS, i.e., daemons, and many of their implementation
details are proprietary, virtualizing Windows system services turned out to be
the most challenging technical barrier for OS-level virtualization for the
Windows platform. In this paper, we describe a general technique to virtualize
Windows services, and demonstrate its effectiveness by applying it to
successfully virtualize a set of important Windows system services and ordinary
services on different versions of Windows OS, including RPCSS, DcomLaunch, IIS
service group, Tlntsvr, MySQL, Apache2.2, CiSvc, ImapiService, etc.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper presents a remarkably simple, yet powerful, algorithm for robust
Principal Component Analysis (PCA). In the proposed approach, an outlier is set
apart from an inlier by comparing their coherence with the rest of the data
points. As inliers lie on a low dimensional subspace, they are likely to have
strong mutual coherence provided there are enough inliers. By contrast,
outliers do not typically admit low dimensional structures, wherefore an
outlier is unlikely to bear strong resemblance with a large number of data
points. The mutual coherences are computed by forming the Gram matrix of
normalized data points. Subsequently, the subspace is recovered from the span
of a small subset of the data points that exhibit strong coherence with the
rest of the data. As coherence pursuit only involves one simple matrix
multiplication, it is significantly faster than the state of-the-art robust PCA
algorithms. We provide a mathematical analysis of the proposed algorithm under
a random model for the distribution of the inliers and outliers. It is shown
that the proposed method can recover the correct subspace even if the data is
predominantly outliers. To the best of our knowledge, this is the first
provable robust PCA algorithm that is simultaneously non-iterative, can
tolerate a large number of outliers and is robust to linearly dependent
outliers.
</p>
{{{{ARTICLE_PARSER}}}}<p>Bibliometric indicators such as journal impact factors, h-indices, and total
citation counts are algorithmic artifacts that can be used in research
evaluation and management. These artifacts have no meaning by themselves, but
receive their meaning from attributions in institutional practices. We
distinguish four main stakeholders in these practices: (1) producers of
bibliometric data and indicators; (2) bibliometricians who develop and test
indicators; (3) research managers who apply the indicators; and (4) the
scientists being evaluated with potentially competing career interests. These
different positions may lead to different and sometimes conflicting
perspectives on the meaning and value of the indicators. The indicators can
thus be considered as boundary objects which are socially constructed in
translations among these perspectives. This paper proposes an analytical
clarification by listing an informed set of (sometimes unsolved) problems in
bibliometrics which can also shed light on the tension between simple but
invalid indicators that are widely used (e.g., the h-index) and more
sophisticated indicators that are not used or cannot be used in evaluation
practices because they are not transparent for users, cannot be calculated, or
are difficult to interpret.
</p>
{{{{ARTICLE_PARSER}}}}<p>Localization in a global map is critical to success in many autonomous robot
missions. This is particularly challenging for multi-robot operations in
unknown and adverse environments. Here, we are concerned with providing a small
unmanned ground vehicle (UGV) the ability to localize itself within a 2.5D
aerial map generated from imagery captured by a low-flying unmanned aerial
vehicle (UAV). We consider the scenario where GPS is unavailable and
appearance-based scene changes may have occurred between the UAV's flight and
the start of the UGV's mission. We present a GPS-free solution to this
localization problem that is robust to appearance shifts by exploiting
high-level, semantic representations of image and depth data. Using data
gathered at an urban test site, we empirically demonstrate that our technique
yields results within five meters of a GPS-based approach.
</p>
{{{{ARTICLE_PARSER}}}}<p>Despite the breakthroughs in accuracy and speed of single image
super-resolution using faster and deeper convolutional neural networks, one
central problem remains largely unsolved: how do we recover the finer texture
details when we super-resolve at large upscaling factors? During image
downsampling information is lost, making super-resolution a highly ill-posed
inverse problem with a large set of possible solutions. The behavior of
optimization-based super-resolution methods is therefore principally driven by
the choice of objective function. Recent work has largely focussed on
minimizing the mean squared reconstruction error (MSE). The resulting estimates
have high peak signal-to-noise-ratio (PSNR), but they are often overly
smoothed, lack high-frequency detail, making them perceptually unsatisfying. In
this paper, we present super-resolution generative adversarial network (SRGAN).
To our knowledge, it is the first framework capable of recovering
photo-realistic natural images from 4 times downsampling. To achieve this, we
propose a perceptual loss function which consists of an adversarial loss and a
content loss. The adversarial loss pushes our solution to the natural image
manifold using a discriminator network that is trained to differentiate between
the super-resolved images and original photo-realistic images. In addition, we
use a content loss function motivated by perceptual similarity instead of
similarity in pixel space. Trained on 350K images using the perceptual loss
function, our deep residual network was able to recover photo-realistic
textures from heavily downsampled images on public benchmarks.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper we study a particular aspect of the urban community policing:
routine patrol route planning. We seek routes that guarantee visibility, as
this has a sizable impact on the community perceived safety, allowing quick
emergency responses and providing surveillance of selected sites (e.g.,
hospitals, schools). The planning is restricted to the availability of vehicles
and strives to achieve balanced routes. We study an adaptation of the model for
the multi-vehicle covering tour problem, in which a set of locations must be
visited, whereas another subset must be close enough to the planned routes. It
constitutes an NP-complete integer programming problem. Suboptimal solutions
are obtained with several heuristics, some adapted from the literature and
others developed by us. We solve some adapted instances from TSPLIB and an
instance with real data, the former being compared with results from
literature, and latter being compared with empirical data.
</p>
{{{{ARTICLE_PARSER}}}}<p>To successfully complete a complex project, be it a construction of an
airport or of a backbone IT system, agents (companies or individuals) must form
a team having required competences and resources. A team can be formed either
by the project issuer based on individual agents' offers (centralized
formation); or by the agents themselves (decentralized formation) bidding for a
project as a consortium---in that case many feasible teams compete for the
contract. We investigate rational strategies of the agents (what salary should
they ask? with whom should they team up?). We propose concepts to characterize
the stability of the winning teams and study their computational complexity.
</p>
{{{{ARTICLE_PARSER}}}}<p>Many machine learning algorithms are based on the assumption that training
examples are drawn identically and independently. However, this assumption does
not hold anymore when learning from a networked sample because two or more
training examples may share some common objects, and hence share the features
of these shared objects. We first show that the classic approach of ignoring
this problem potentially can have a disastrous effect on the accuracy of
statistics, and then consider alternatives. One of these is to only use
independent examples, discarding other information. However, this is clearly
suboptimal. We analyze sample error bounds in a networked setting, providing
both improved and new results. Next, we propose an efficient weighting method
which achieves a better sample error bound than those of previous methods. Our
approach is based on novel concentration inequalities for networked variables.
</p>
{{{{ARTICLE_PARSER}}}}<p>Different graph generalizations have been recently used in an ad-hoc manner
to represent multilayer networks, i.e. systems formed by distinct layers where
each layer can be seen as a network. Similar constructions have also been used
to represent time-varying networks. We introduce the concept of MultiAspect
Graph (MAG) as a graph generalization that we prove to be isomorphic to a
directed graph, and also capable of representing all previous generalizations.
In our proposal, the set of vertices, layers, time instants, or any other
independent features are considered as an aspect of the MAG. For instance, a
MAG is able to represent multilayer or time-varying networks, while both
concepts can also be combined to represent a multilayer time-varying network
and even other higher-order networks. Since the MAG structure admits an
arbitrary (finite) number of aspects, it hence introduces a powerful modelling
abstraction for networked complex systems. This paper formalizes the concept of
MAG and derives theoretical results useful in the analysis of complex networked
systems modelled using the proposed MAG abstraction. We also present an
overview of the MAG applicability.
</p>
{{{{ARTICLE_PARSER}}}}<p>We determine the exact strong converse exponent of classical-quantum channel
coding, for every rate above the Holevo capacity. Our form of the exponent is
an exact analogue of Arimoto's, given as a transform of the Renyi capacities
with parameters alpha&gt;1. It is important to note that, unlike in the classical
case, there are many inequivalent ways to define the Renyi divergence of
states, and hence the R\'enyi capacities of channels. Our exponent is in terms
of the Renyi capacities corresponding to a version of the Renyi divergences
that has been introduced recently in [M\\"uller-Lennert, Dupuis, Szehr, Fehr and
Tomamichel, J. Math. Phys. 54, 122203, (2013)], and [Wilde, Winter, Yang,
Commun. Math. Phys. 331, (2014)]. Our result adds to the growing body of
evidence that this new version is the natural definition for the purposes of
strong converse problems.
</p>
{{{{ARTICLE_PARSER}}}}<p>We describe a general method for turning quantum circuits into sparse quantum
subsystem codes. The idea is to turn each circuit element into a set of
low-weight gauge generators that enforce the input-output relations of that
circuit element. Using this prescription, we can map an arbitrary stabilizer
code into a new subsystem code with the same distance and number of encoded
qubits but where all the generators have constant weight, at the cost of adding
some ancilla qubits. With an additional overhead of ancilla qubits, the new
code can also be made spatially local.
</p>
<p>Applying our construction to certain concatenated stabilizer codes yields
families of subsystem codes with constant-weight generators and with minimum
distance $d = n^{1-\epsilon}$, where $\epsilon = O(1/\sqrt{\log n})$. For
spatially local codes in $D$ dimensions we nearly saturate a bound due to
Bravyi and Terhal and achieve $d = n^{1-\epsilon-1/D}$. Previously the best
code distance achievable with constant-weight generators in any dimension, due
to Freedman, Meyer and Luo, was $O(\sqrt{n\log n})$ for a stabilizer code.
</p>
{{{{ARTICLE_PARSER}}}}<p>To address the contextual bandit problem, we propose an online random forest
algorithm. The analysis of the proposed algorithm is based on the sample
complexity needed to find the optimal decision stump. Then, the decision stumps
are assembled in a random collection of decision trees, Bandit Forest. We show
that the proposed algorithm is optimal up to logarithmic factors. The
dependence of the sample complexity upon the number of contextual variables is
logarithmic. The computational cost of the proposed algorithm with respect to
the time horizon is linear. These analytical results allow the proposed
algorithm to be efficient in real applications, where the number of events to
process is huge, and where we expect that some contextual variables, chosen
from a large set, have potentially non- linear dependencies with the rewards.
In the experiments done to illustrate the theoretical analysis, Bandit Forest
obtain promising results in comparison with state-of-the-art algorithms.
</p>
{{{{ARTICLE_PARSER}}}}<p>What if a successful company starts to receive a torrent of low-valued (one
or two stars) recommendations in its mobile apps from multiple users within a
short (say one month) period of time? Is it legitimate evidence that the apps
have lost in quality, or an intentional plan (via lockstep behavior) to steal
market share through defamation? In the case of a systematic attack to one's
reputation, it might not be possible to manually discern between legitimate and
fraudulent interaction within the huge universe of possibilities of
user-product recommendation. Previous works have focused on this issue, but
none of them took into account the context, modeling, and scale that we
consider in this paper. Here, we propose the novel method Online-Recommendation
Fraud ExcLuder (ORFEL) to detect defamation and/or illegitimate promotion of
online products by using vertex-centric asynchronous parallel processing of
bipartite (users-products) graphs. With an innovative algorithm, our results
demonstrate both efficacy and efficiency -- over 95% of potential attacks were
detected, and ORFEL was at least two orders of magnitude faster than the
state-of-the-art. Over a novel methodology, our main contributions are: (1) a
new algorithmic solution; (2) one scalable approach; and (3) a novel context
and modeling of the problem, which now addresses both defamation and
illegitimate promotion. Our work deals with relevant issues of the Web 2.0,
potentially augmenting the credibility of online recommendation to prevent
losses to both customers and vendors.
</p>
{{{{ARTICLE_PARSER}}}}<p>Recent graph computation approaches have demonstrated that a single PC can
perform efficiently on billion-scale graphs. While these approaches achieve
scalability by optimizing I/O operations, they do not fully exploit the
capabilities of modern hard drives and processors. To overcome their
performance, in this work, we introduce the Bimodal Block Processing (BBP), an
innovation that is able to boost the graph computation by minimizing the I/O
cost even further. With this strategy, we achieved the following contributions:
(1) M-Flash, the fastest graph computation framework to date; (2) a flexible
and simple programming model to easily implement popular and essential graph
algorithms, including the first single-machine billion-scale eigensolver; and
(3) extensive experiments on real graphs with up to 6.6 billion edges,
demonstrating M-Flash's consistent and significant speedup.
</p>
{{{{ARTICLE_PARSER}}}}<p>Privacy concerns in outsourced cloud databases have become more and more
important recently and many efficient and scalable query processing methods
over encrypted data have been proposed. However, there is very limited work on
how to securely process top-k ranking queries over encrypted databases in the
cloud. In this paper, we focus exactly on this problem: secure and efficient
processing of top-k queries over outsourced databases. In particular, we
propose the first efficient and provable secure top-k query processing
construction that achieves adaptively CQA security. We develop an encrypted
data structure called EHL and describe several secure sub-protocols under our
security model to answer top-k queries. Furthermore, we optimize our query
algorithms for both space and time efficiency. Finally, in the experiments, we
empirically analyze our protocol using real world datasets and demonstrate that
our construction is efficient and practical.
</p>
{{{{ARTICLE_PARSER}}}}<p>Motivation: Prediction of phenotypes from high-dimensional data is a crucial
task in precision biology and medicine. Many technologies employ genomic
biomarkers to characterize phenotypes. However, such elements are not
sufficient to explain the underlying biology. To improve this, pathway analysis
techniques have been proposed. Nevertheless, such methods have shown lack of
accuracy in phenotypes classification. Results: Here we propose a novel
methodology called MITHrIL (Mirna enrIched paTHway Impact anaLysis) for the
analysis of signaling pathways, which has built on top of the work of Tarca et
al., 2009. MITHrIL extends pathways by adding missing regulatory elements, such
as microRNAs, and their interactions with genes. The method takes as input the
expression values of genes and/or microRNAs and returns a list of pathways
sorted according to their deregulation degree, together with the corresponding
statistical significance (p-values). Our analysis shows that MITHrIL
outperforms its competitors even in the worst case. In addition, our method is
able to correctly classify sets of tumor samples drawn from TCGA. Availability:
MITHrIL is freely available at the following URL:
<a href=\"http://alpha.dmi.unict.it/mithril/\">this http URL</a>
</p>
{{{{ARTICLE_PARSER}}}}<p>In regression settings where explanatory variables have very low correlations
and there are relatively few effects, each of large magnitude, we expect the
Lasso to find the important variables with few errors, if any. This paper shows
that in a regime of linear sparsity---meaning that the fraction of variables
with a non-vanishing effect tends to a constant, however small---this cannot
really be the case, even when the design variables are stochastically
independent. We demonstrate that true features and null features are always
interspersed on the Lasso path, and that this phenomenon occurs no matter how
strong the effect sizes are. We derive a sharp asymptotic trade-off between
false and true positive rates or, equivalently, between measures of type I and
type II errors along the Lasso path. This trade-off states that if we ever want
to achieve a type II error (false negative rate) under a critical value, then
anywhere on the Lasso path the type I error (false positive rate) will need to
exceed a given threshold so that we can never have both errors at a low level
at the same time. Our analysis uses tools from approximate message passing
(AMP) theory as well as novel elements to deal with a possibly adaptive
selection of the Lasso regularizing parameter.
</p>
{{{{ARTICLE_PARSER}}}}<p>In recent years, numerous effective multi-object tracking (MOT) methods are
developed because of the wide range of applications. Existing performance
evaluations of MOT methods usually separate the object tracking step from the
object detection step by using the same fixed object detection results for
comparisons. In this work, we perform a comprehensive quantitative study on the
effects of object detection accuracy to the overall MOT performance, using the
new large-scale University at Albany DETection and tRACking (UA-DETRAC)
benchmark dataset. The UA-DETRAC benchmark dataset consists of 100 challenging
video sequences captured from real-world traffic scenes (over 140,000 frames
with rich annotations, including occlusion, weather, vehicle category,
truncation, and vehicle bounding boxes) for object detection, object tracking
and MOT system. We evaluate complete MOT systems constructed from combinations
of state-of-the-art object detection and object tracking methods. Our analysis
shows the complex effects of object detection accuracy on MOT system
performance. Based on these observations, we propose new evaluation tools and
metrics for MOT systems that consider both object detection and object tracking
for comprehensive analysis.
</p>
{{{{ARTICLE_PARSER}}}}<p>We prove geometric Ramsey-type statements on collections of lines in 3-space.
These statements give guarantees on the size of a clique or an independent set
in (hyper)graphs induced by incidence relations between lines, points, and
reguli in 3-space. Among other things, we prove that: (1) The intersection
graph of n lines in R^3 has a clique or independent set of size Omega(n^{1/3}).
(2) Every set of n lines in R^3 has a subset of n^{1/2} lines that are all
stabbed by one line, or a subset of Omega((n/log n)^{1/5}) such that no
6-subset is stabbed by one line. (3) Every set of n lines in general position
in R^3 has a subset of Omega(n^{2/3}) lines that all lie on a regulus, or a
subset of Omega(n^{1/3}) lines such that no 4-subset is contained in a regulus.
The proofs of these statements all follow from geometric incidence bounds --
such as the Guth-Katz bound on point-line incidences in R^3 -- combined with
Tur\'an-type results on independent sets in sparse graphs and hypergraphs.
Although similar Ramsey-type statements can be proved using existing generic
algebraic frameworks, the lower bounds we get are much larger than what can be
obtained with these methods. The proofs directly yield polynomial-time
algorithms for finding subsets of the claimed size.
</p>
{{{{ARTICLE_PARSER}}}}<p>Stochastic gradient descent (SGD) still is the workhorse for many practical
problems. However, it converges slow, and can be difficult to tune. It is
possible to precondition SGD to accelerate its convergence remarkably. But many
attempts in this direction either aim at solving specialized problems, or
result in significantly more complicated methods than SGD. This paper proposes
a new method to estimate a preconditioner such that the amplitudes of
perturbations of preconditioned stochastic gradient match that of the
perturbations of parameters to be optimized in a way comparable to Newton
method for deterministic optimization. Unlike the preconditioners based on
secant equation fitting as done in deterministic quasi-Newton methods, which
assume positive definite Hessian and approximate its inverse, the new
preconditioner works equally well for both convex and non-convex optimizations
with exact or noisy gradients. When stochastic gradient is used, it can
naturally damp the gradient noise to stabilize SGD. Efficient preconditioner
estimation methods are developed, and with reasonable simplifications, they are
applicable to large scaled problems. Experimental results demonstrate that
equipped with the new preconditioner, without any tuning effort, preconditioned
SGD can efficiently solve many challenging problems like the training of a deep
neural network or a recurrent neural network requiring extremely long term
memories.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider a network-preserved model of power networks with proper algebraic
constraints resulting from constant power loads. Both for the linear and the
nonlinear differential algebraic model of the network, we derive explicit
reduced models which are fully expressed in terms of ordinary differential
equations. For deriving these reduced models, we introduce the \"projected
incidence\" matrix which yields a novel decomposition of the reduced Laplacian
matrix. With the help of this new matrix, we provide a complementary approach
to Kron reduction which is able to cope with constant power loads and nonlinear
power flow equations.
</p>
{{{{ARTICLE_PARSER}}}}<p>In a paper that initiated the modern study of the stochastic block model,
Decelle et al., backed by Mossel et al., made the following conjecture: Denote
by $k$ the number of balanced communities, $a/n$ the probability of connecting
inside communities and $b/n$ across, and set
$\mathrm{SNR}=(a-b)^2/(k(a+(k-1)b)$; for any $k \geq 2$, it is possible to
detect communities efficiently whenever $\mathrm{SNR}&gt;1$ (the KS threshold),
whereas for $k\geq 4$, it is possible to detect communities
information-theoretically for some $\mathrm{SNR}&lt;1$. Massouli\'e, Mossel et
al.\ and Bordenave et al.\ succeeded in proving that the KS threshold is
efficiently achievable for $k=2$, while Mossel et al.\ proved that it cannot be
crossed information-theoretically for $k=2$. The above conjecture remained open
for $k \geq 3$.
</p>
<p>This paper proves this conjecture, further extending the efficient detection
to non-symmetrical SBMs with a generalized notion of detection and KS
threshold. For the efficient part, a linearized acyclic belief propagation
(ABP) algorithm is developed and proved to detect communities for any $k$ down
to the KS threshold in time $O(n \log n)$. Achieving this requires showing
optimality of ABP in the presence of cycles, a challenge for message passing
algorithms. The paper further connects ABP to a power iteration method with a
nonbacktracking operator of generalized order, formalizing the interplay
between message passing and spectral methods. For the information-theoretic
(IT) part, a non-efficient algorithm sampling a typical clustering is shown to
break down the KS threshold at $k=4$. The emerging gap is shown to be large in
some cases; if $a=0$, the KS threshold reads $b \gtrsim k^2$ whereas the IT
bound reads $b \gtrsim k \ln(k)$, making the SBM a good study-case for
information-computation gaps.
</p>
{{{{ARTICLE_PARSER}}}}<p>A new submodule clustering method via sparse and low-rank representation for
multi-way data is proposed in this paper. Instead of reshaping multi-way data
into vectors, this method maintains their natural orders to preserve data
intrinsic structures, e.g., image data kept as matrices. To implement
clustering, the multi-way data, viewed as tensors, are represented by the
proposed tensor sparse and low-rank model to obtain its submodule
representation, called a free module, which is finally used for spectral
clustering. The proposed method extends the conventional subspace clustering
method based on sparse and low-rank representation to multi-way data submodule
clustering by combining t-product operator. The new method is tested on several
public datasets, including synthetical data, video sequences and toy images.
The experiments show that the new method outperforms the state-of-the-art
methods, such as Sparse Subspace Clustering (SSC), Low-Rank Representation
(LRR), Ordered Subspace Clustering (OSC), Robust Latent Low Rank Representation
(RobustLatLRR) and Sparse Submodule Clustering method (SSmC).
</p>
{{{{ARTICLE_PARSER}}}}<p>Using phonological speech vocoding, we propose a platform for exploring
relations between phonology and speech processing, and in broader terms, for
exploring relations between the abstract and physical structures of a speech
signal. Our goal is to make a step towards bridging phonology and speech
processing and to contribute to the program of Laboratory Phonology. We show
three application examples for laboratory phonology: compositional phonological
speech modelling, a comparison of phonological systems and an experimental
phonological parametric text-to-speech (TTS) system. The featural
representations of the following three phonological systems are considered in
this work: (i) Government Phonology (GP), (ii) the Sound Pattern of English
(SPE), and (iii) the extended SPE (eSPE). Comparing GP- and eSPE-based vocoded
speech, we conclude that the latter achieves slightly better results than the
former. However, GP - the most compact phonological speech representation -
performs comparably to the systems with a higher number of phonological
features. The parametric TTS based on phonological speech representation, and
trained from an unlabelled audiobook in an unsupervised manner, achieves
intelligibility of 85% of the state-of-the-art parametric speech synthesis. We
envision that the presented approach paves the way for researchers in both
fields to form meaningful hypotheses that are explicitly testable using the
concepts developed and exemplified in this paper. On the one hand, laboratory
phonologists might test the applied concepts of their theoretical models, and
on the other hand, the speech processing community may utilize the concepts
developed for the theoretical phonological models for improvements of the
current state-of-the-art applications.
</p>
{{{{ARTICLE_PARSER}}}}<p>Automatic speaker verification (ASV) technology is recently finding its way
to end-user applications for secure access to personal data, smart services or
physical facilities. Similar to other biometric technologies, speaker
verification is vulnerable to spoofing attacks where an attacker masquerades as
a particular target speaker via impersonation, replay, text-to-speech (TTS) or
voice conversion (VC) techniques to gain illegitimate access to the system. We
focus on TTS and VC that represent the most flexible, high-end spoofing
attacks. Most of the prior studies on synthesized or converted speech detection
report their findings using high-quality clean recordings. Meanwhile, the
performance of spoofing detectors in the presence of additive noise, an
important consideration in practical ASV implementations, remains largely
unknown. To this end, we analyze the suitability of state-of-the-art synthetic
speech detectors under additive noise with a special focus on front-end
features. Our comparison includes eight acoustic feature sets, five related to
spectral magnitude and three to spectral phase information. Our extensive
experiments on ASVSpoof 2015 corpus reveal several important findings. Firstly,
all the countermeasures break down even at relatively high signal-to-noise
ratios (SNRs) and fail to generalize to noisy conditions. Secondly, speech
enhancement is not found helpful. Thirdly, GMM back-end generally outperforms
the more involved i-vector back-end. Fourthly, concerning the compared
features, the Mel-frequency cepstral coefficients (MFCCs) and subband spectral
centroid magnitude coefficients (SCMCs) perform the best on average though the
winner method depends on SNR and noise type. Finally, a study with two score
fusion strategies shows that combining different feature based systems improves
recognition accuracy for known and unknown attacks in both clean and noisy
conditions.
</p>
{{{{ARTICLE_PARSER}}}}<p>High-order ARX models can be used to approximate a quite general class of
linear systems in a parametric model structure, and well-established methods
can then be used to retrieve the true plant and noise models from the ARX
polynomials. However, this commonly used approach is only valid when the plant
is stable or if the unstable poles are shared with the true noise model. In
this contribution, we generalize this approach to allow the unstable poles not
to be shared, by introducing modifications to correctly retrieve the noise
model and noise variance.
</p>
{{{{ARTICLE_PARSER}}}}<p>Heuristic algorithms are able to optimize objective functions efficiently
because they use intelligently the information about the objective functions.
Thus, information utilization is critical to the performance of heuristics.
However, the concept of information utilization has remained vague and abstract
because there is no reliable metric to reflect the extent to which the
information about the objective function is utilized by heuristic algorithms.
In this paper, the metric of information utilization ratio (IUR) is defined,
which is the ratio of the utilized information quantity over the acquired
information quantity in the search process. The IUR proves to be well-defined.
Several examples of typical heuristic algorithms are given to demonstrate the
procedure of calculating the IUR. Empirical evidences on the correlation
between the IUR and the performance of a heuristic are also provided. The IUR
can be an index of how finely an algorithm is designed and guide the invention
of new heuristics and the improvement of existing ones.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider the well-studied cake cutting problem in which the goal is to
find an envy-free allocation based on queries from $n$ agents. The problem has
received attention in computer science, mathematics, and economics. It has been
a major open problem whether there exists a discrete and bounded envy-free
protocol. We resolve the problem by proposing a discrete and bounded envy-free
protocol for any number of agents. The maximum number of queries required by
the protocol is $n^{n^{n^{n^{n^n}}}}$. We additionally show that even if we do
not run our protocol to completion, it can find in at most $n^{n+1}$ queries a
partial allocation of the cake that achieves proportionality (each agent gets
at least $1/n$ of the value of the whole cake) and envy-freeness. Finally we
show that an envy-free partial allocation can be computed in $n^{n+1}$ queries
such that each agent gets a connected piece that gives the agent at least
$1/(3n)$ of the value of the whole cake.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a new approach on low-rank matrix recovery (LRMR) based on
Gabidulin Codes. Since most applications of LRMR deal with matrices over
infinite fields, we use the recently introduced generalization of Gabidulin
codes to fields of characterstic zero. We show that LRMR can be reduced to
decoding of Gabidulin codes and discuss which field extensions can be used in
the code construction.
</p>
{{{{ARTICLE_PARSER}}}}<p>We prove that Alekhnovich's algorithm can be used for row reduction of skew
polynomial matrices. This yields an $O(\ell^3 n^{(\omega+1)/2} \log(n))$
decoding algorithm for $\ell$-Interleaved Gabidulin codes of length $n$, where
$\omega$ is the matrix multiplication exponent, improving in the exponent of
$n$ compared to previous results.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we present a new algorithm for semi-supervised representation
learning. In this algorithm, we first find a vector representation for the
labels of the data points based on their local positions in the space. Then, we
map the data to lower-dimensional space using a linear transformation such that
the dependency between the transformed data and the assigned labels is
maximized. In fact, we try to find a mapping that is as discriminative as
possible. The approach will use Hilber-Schmidt Independence Criterion (HSIC) as
the dependence measure. We also present a kernelized version of the algorithm,
which allows non-linear transformations and provides more flexibility in
finding the appropriate mapping. Use of unlabeled data for learning new
representation is not always beneficial and there is no algorithm that can
deterministically guarantee the improvement of the performance by exploiting
unlabeled data. Therefore, we also propose a bound on the performance of the
algorithm, which can be used to determine the effectiveness of using the
unlabeled data in the algorithm. We demonstrate the ability of the algorithm in
finding the transformation using both toy examples and real-world datasets.
</p>
{{{{ARTICLE_PARSER}}}}<p>We propose a new class of space-time block codes based on finite-field
rank-metric codes in combination with a rank-metric-preserving mapping to the
set of Eisenstein integers. It is shown that these codes achieve maximum
diversity order and improve upon certain existing constructions. Moreover, we
present a new decoding algorithm for these codes which utilizes the algebraic
structure of the underlying finite-field rank-metric codes and employs
lattice-reduction-aided equalization. This decoder does not achieve the same
performance as the classical maximum-likelihood decoding methods, but has
polynomial complexity in the matrix dimension, making it usable for large field
sizes and numbers of antennas.
</p>
{{{{ARTICLE_PARSER}}}}<p>Cartograms combine statistical and geographical information in thematic maps,
where areas of geographical regions (e.g., countries, states) are scaled in
proportion to some statistic (e.g., population, income). Cartograms make it
possible to gain insight into patterns and trends in the world around us and
have been very popular visualizations for geo-referenced data for over a
century. This work surveys cartogram research in visualization, cartography and
geometry, covering a broad spectrum of different cartogram types: from the
traditional rectangular and table cartograms, to Dorling and diffusion
cartograms. A particular focus is the study of the major cartogram dimensions:
statistical accuracy, geographical accuracy, and topological accuracy. We
review the history of cartograms, describe the algorithms for generating them,
and consider task taxonomies. We also review quantitative and qualitative
evaluations, and we use these to arrive at design guidelines and research
challenges.
</p>
{{{{ARTICLE_PARSER}}}}<p>The \"r-trie\", a new kind of self-balancing ternary search trie, is presented.
It generalizes the simple balancing strategy used in Aragon and Seidel's
randomized binary search trees (better known as \"treaps\"). This means that an
r-trie is shaped like a ternary search trie built by inserting the strings in
random order. As a result, the time cost of searching, inserting, or deleting a
string of length k in an r-trie for n strings is in O(k + log n) with high
probability.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we propose a new framework to remove parts of the systematic
errors affecting popular restoration algorithms, with a special focus for image
processing tasks. Generalizing ideas that emerged for $\ell_1$ regularization,
we develop an approach re-fitting the results of standard methods towards the
input data. Total variation regularizations and non-local means are special
cases of interest. We identify important covariant information that should be
preserved by the re-fitting method, and emphasize the importance of preserving
the Jacobian (w.r.t. the observed signal) of the original estimator. Then, we
provide an approach that has a \"twicing\" flavor and allows re-fitting the
restored signal by adding back a local affine transformation of the residual
term. We illustrate the benefits of our method on numerical simulations for
image restoration tasks.
</p>
{{{{ARTICLE_PARSER}}}}<p>In cognitive radio networks, the channel gain between primary transceivers is
crucial for a cognitive transmitter (CT) to control the transmit power and
achieve the coexistence. To obtain the channel gain, a backhaul between the
primary system and the CT is needed. However, the backhaul is usually
unavailable in practice. To deal with this issue, two passive estimators are
proposed for the CT to obtain the channel gain between primary transceivers in
this paper. In particular, by sensing primary signals, a maximum likelihood
(ML) estimator is developed for the CT to obtain the channel gain between
primary transceivers. After demonstrating the high computational complexity of
the ML estimator, a median based (MB) estimator with proved low complexity is
proposed. Furthermore, the estimation accuracy of the MB estimation is
theoretically characterized. By comparing the ML estimator and the MB estimator
from the aspect of computational complexity as well as the estimation accuracy,
both advantages and disadvantages of two estimators are analyzed. Numerical
results show that the estimation errors of the ML estimator and the MB
estimator can be as small as $0.6$ dB and $0.7$ dB, respectively. Besides, the
ML estimator outperforms the MB estimator in terms of the estimation error if
the signal to noise ratio (SNR) of the sensed primary signals at the CT is no
smaller than $4$ dB. Otherwise, the MB estimator is superior to the ML
estimator from the aspect of both computational complexity and estimation
accuracy.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper proposes an alternating back-propagation algorithm for learning
the generator network model. The model is a non-linear generalization of factor
analysis. In this model, the mapping from the latent factors to the observed
vector is parametrized by a convolutional neural network. The alternating
back-propagation algorithm iterates between the following two steps: (1)
Inferential back-propagation, which infers the latent factors by Langevin
dynamics or gradient descent. (2) Learning back-propagation, which updates the
parameters given the inferred latent factors by gradient descent. The gradient
computations in both steps are powered by back-propagation, and they share most
of their code in common. We show that the alternating back-propagation
algorithm can learn realistic generator models of natural images, video
sequences, and sounds. Moreover, it can also be used to learn from incomplete
or indirect training data.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study the problem of planning Pareto-optimal journeys in public transit
networks. Most existing algorithms and speed-up techniques work by computing
subjourneys to intermediary stops until the destination is reached. In
contrast, the trip-based model focuses on trips and transfers between them,
constructing journeys as a sequence of trips. In this paper, we develop a
speed-up technique for this model inspired by principles behind existing
state-of-the-art speed-up techniques, Transfer Pattern and Hub Labelling. The
resulting algorithm allows us to compute Pareto-optimal (with respect to
arrival time and number of transfers) 24-hour profiles on very large real-world
networks in less than half a millisecond. Compared to the current state of the
art for bicriteria queries on public transit networks, this is up to two orders
of magnitude faster, while increasing preprocessing overhead by at most one
order of magnitude.
</p>
{{{{ARTICLE_PARSER}}}}<p>The Weighted Tree Augmentation Problem (WTAP) is a fundamental well-studied
problem in the field of network design. Given an undirected tree $G=(V,E)$, an
additional set of edges $L \subseteq V\times V$ disjoint from $E$ called
\textit{links}, and a cost vector $c\in \mathbb{R}_{\geq 0}^L$, WTAP asks to
find a minimum-cost set $F\subseteq L$ with the property that $(V,E\cup F)$ is
$2$-edge connected. The special case where $c_\ell = 1$ for all $\ell\in L$ is
called the Tree Augmentation Problem (TAP). Both problems are known to be
NP-hard.
</p>
<p>For the class of bounded cost vectors, we present a first improved
approximation algorithm for WTAP since more than three decades. Concretely, for
any $M\in \mathbb{Z}_{\geq 1}$ and $\epsilon &gt; 0,$ we present an LP based
$(\delta+\epsilon)$-approximation for WTAP restricted to cost vectors $c$ in
$[1,M]^L$ for $\delta \approx 1.96417$. For the special case of TAP we improve
this factor to $\frac{5}{3}+\epsilon$.
</p>
<p>Our results rely on a new LP, that significantly differs from existing LPs
achieving improved bounds for TAP. We round a fractional solution in two
phases. The first phase uses the fractional solution to decompose the tree and
its fractional solution into so-called $\beta$-simple pairs losing only an
$\epsilon$-factor in the objective function. We then show how to use the
additional constraints in our LP combined with the $\beta$-simple structure to
round a fractional solution in each part of the decomposition.
</p>
{{{{ARTICLE_PARSER}}}}<p>First responders are increasingly using social media to identify and reduce
crime for well-being and safety of the society. Images shared on social media
hurting religious, political, communal and other sentiments of people, often
instigate violence and create law &amp; order situations in society. This results
in the need for first responders to inspect the spread of such images and users
propagating them on social media. In this paper, we present a comparison
between different hand-crafted features and a Convolutional Neural Network
(CNN) model to retrieve similar images, which outperforms state-of-art
hand-crafted features. We propose an Open-Source-Intelligent (OSINT) real-time
image search system, robust to retrieve modified images that allows first
responders to analyze the current spread of images, sentiments floating and
details of users propagating such content. The system also aids officials to
save time of manually analyzing the content by reducing the search space on an
average by 67%.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we give an algorithm that, given an undirected graph $G$ of
$m$ edges and an integer $k$, computes a graph $G'$ and an integer $k'$ in
$O(k^4 m)$ time such that (1) the size of the graph $G'$ is $O(k^2)$, (2)
$k'\leq k$, and (3) $G$ has a feedback vertex set of size at most $k$ if and
only if $G'$ has a feedback vertex set of size at most $k'$. This is the first
linear-time polynomial-size kernel for Feedback Vertex Set. The size of our
kernel is $2k^2+k$ vertices and $4k^2$ edges, which is smaller than the
previous best of $4k^2$ vertices and $8k^2$ edges. Thus, we improve the size
and the running time simultaneously. We note that under the assumption of
$\mathrm{NP}\not\subseteq\mathrm{coNP}/\mathrm{poly}$, Feedback Vertex Set does
not admit an $O(k^{2-\epsilon})$-size kernel for any $\epsilon&gt;0$.
</p>
<p>Our kernel exploits $k$-submodular relaxation, which is a recently developed
technique for obtaining efficient FPT algorithms for various problems. The
$k$-submodular relaxation of Feedback Vertex Set can be seen as a half-integral
variant of $A$-path packing, and to obtain the linear-time complexity, we give
an efficient augmenting-path algorithm for this problem. We believe that this
combinatorial algorithm is of independent interest.
</p>
<p>A solver based on the proposed kernel and the efficient augmenting-path
algorithm has been submitted to the 1st parameterized algorithms and
computational experiments challenge and won the first place.
</p>
{{{{ARTICLE_PARSER}}}}<p>The concept of an \"approximation algorithm\" is usually only applied to
optimization problems since in optimization problems the performance of the
algorithm on any given input is a continuous parameter. We introduce a new
concept of approximation applicable to decision problems and functions,
inspired by Bayesian probability. From the perspective of a Bayesian reasoner
with limited computational resources, the answer to a problem that cannot be
solved exactly is uncertain and therefore should be described by a random
variable. It thus should make sense to talk about the expected value of this
random variable, an idea we formalize in the language of average-case
complexity theory by introducing the concept of optimal polynomial-time
estimator. We show that optimal polynomial-time estimators exhibit many
parallels with \"classical\" probability theory, prove some existence theorems
and completeness results.
</p>
{{{{ARTICLE_PARSER}}}}<p>Convolutional Neural Networks (CNNs) are extensively used in image and video
recognition, natural language processing and other machine learning
applications. The success of CNNs in these areas corresponds with a significant
increase in the number of parameters and computation costs. Recent approaches
towards reducing these overheads involve pruning and compressing the weights of
various layers without hurting the overall CNN performance. However, using
model compression to generate sparse CNNs mostly reduces parameters from the
fully connected layers and may not significantly reduce the final computation
costs. In this paper, we present a compression technique for CNNs, where we
prune the filters from CNNs that are identified as having a small effect on the
output accuracy. By removing whole planes in the network, together with their
connecting convolution kernels, the computational costs are reduced
significantly. In contrast to other techniques proposed for pruning networks,
this approach does not result in sparse connectivity patterns. Hence, our
techniques do not need the support of sparse convolution libraries and can work
with the most efficient BLAS operations for matrix multiplications. In our
results, we show that even simple filter pruning techniques can reduce
inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% while
regaining close to the original accuracy by retraining the networks.
</p>
{{{{ARTICLE_PARSER}}}}<p>We provide global convergence guarantees for the expectation-maximization
(EM) algorithm applied to mixtures of two Gaussians with known covariance
matrices. We show that EM converges geometrically to the correct mean vectors,
and provide simple, closed-form expressions for the convergence rate. As a
simple illustration, we show that in one dimension ten steps of the EM
algorithm initialized at $+ \infty$ result in less than 1% error estimation of
the means.
</p>
{{{{ARTICLE_PARSER}}}}<p>Protein contact prediction from sequence is an important problem. Recently
exciting progress has been made, but the predicted contacts for proteins
without many sequence homologs is still of low quality and not extremely useful
for de novo structure prediction. This paper presents a new deep learning
method that predicts contacts by integrating both evolutionary coupling (EC)
and sequence conservation information through an ultra-deep neural network
formed by two deep residual neural networks. This deep neural network allows us
to model very complex relationship between sequence and contact map as well as
long-range interdependency between contacts. Our method greatly outperforms
existing contact prediction methods and leads to much more accurate
contact-assisted protein folding. Tested on three datasets of 579 proteins, the
average top L long-range prediction accuracy obtained our method, the
representative EC method CCMpred and the CASP11 winner MetaPSICOV is 0.47, 0.21
and 0.30, respectively; the average top L/10 long-range accuracy of our method,
CCMpred and MetaPSICOV is 0.77, 0.47 and 0.59, respectively. Ab initio folding
using our predicted contacts as restraints can yield correct folds (i.e.,
TMscore&gt;0.6) for 203 test proteins, while that using MetaPSICOV- and
CCMpred-predicted contacts can do so for only 79 and 62 proteins, respectively.
Further, our contact-assisted models have much better quality than
template-based models. Using our predicted contacts as restraints, we can (ab
initio) fold 208 of the 398 membrane proteins with TMscore&gt;0.5. By contrast,
when the training proteins of our method are used as templates, homology
modeling can only do so for 10 of them. One interesting finding is that even if
we do not train our prediction models with any membrane proteins, our method
still works well on membrane protein prediction.
</p>
{{{{ARTICLE_PARSER}}}}<p>Providing opinions through labeling of images, tweets, etc. have drawn
immense interest in crowdsourcing markets. This invokes a major challenge of
aggregating multiple opinions received from different crowd workers for
deriving the final judgment. Generally, opinion aggregation models deal with
independent opinions, which are given unanimously and are not visible to all.
However, in many real-life cases, it is required to make the opinions public as
soon as they are received. This makes the opinions dependent and might
incorporate some bias. In this paper, we address a novel problem, hereafter
denoted as dependent judgment analysis, and discuss the requirements for
developing an appropriate model to deal with this problem. The challenge
remains to be improving the consensus by revealing true opinions.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper we study the routing and rebalancing problem for a fleet of
autonomous vehicles providing on-demand transportation within a congested urban
road network (that is, a road network where traffic speed depends on vehicle
density). We show that the congestion-free routing and rebalancing problem is
NP-hard and provide a randomized algorithm which finds a low-congestion
solution to the routing and rebalancing problem that approximately minimizes
the number of vehicles on the road in polynomial time. We provide theoretical
bounds on the probability of violating the congestion constraints; we also
characterize the expected number of vehicles required by the solution with a
commonly-used empirical congestion model and provide a bound on the
approximation factor of the algorithm. Numerical experiments on a realistic
road network with real-world customer demands show that our algorithm
introduces very small amounts of congestion. The performance of our algorithm
in terms of travel times and required number of vehicles is very close to (and
sometimes better than) the optimal congestion-free solution.
</p>
{{{{ARTICLE_PARSER}}}}<p>The virtuality continuum describes the degrees of positive virtuality under
the umbrella term mixed reality. Besides adding virtual information within a
mixed environment, diminished reality aims at reducing real world information.
Mann defined the term mediated reality (MR), which also considered diminished
reality, but without the possibility to describe different degrees of fusion
between a mixed and a diminished reality. That is why this work defines the new
term blending entropy that captures the relations between a mixed and a
diminished reality. The blending entropy is based on the information density of
the mediated reality and the actual area the user has to comprehend, which is
named perceptual frustum. We describe the blending entropy's twodimensional
dependencies and detail important points in the blending entropy's space.
</p>
{{{{ARTICLE_PARSER}}}}<p>The revelation principle is a fundamental theorem in many economics fields
such as game theory, mechanism design and auction theory etc. In this paper, I
construct an example to show that a social choice function which can be
implemented in Bayesian Nash equilibrium is not truthfully implementable. The
key point is that agents pay cost in the indirect mechanism, but pay nothing in
the direct mechanism. As a result, the revelation principle may not hold when
agent's cost cannot be neglected in the indirect mechanism.
</p>
{{{{ARTICLE_PARSER}}}}<p>Recently, the normalized subband adaptive filter (NSAF) algorithm has
attracted much attention for handling the colored input signals. Based on the
first-order Markov model of the optimal tap-weight vector, this paper provides
a convergence analysis of the standard NSAF. Following the analysis, both the
step size and the regularization parameter in the NSAF are jointly optimized in
such a way that minimizes the mean square deviation. The resulting
joint-optimization step size and regularization parameter (JOSR-NSAF) algorithm
achieves a good tradeoff between fast convergence rate and low steady-state
error. Simulation results in the context of acoustic echo cancellation
demonstrate good features of the proposed algorithm.
</p>
{{{{ARTICLE_PARSER}}}}<p>The graph removal lemma is an important structural result in graph theory. It
has many different variants and is closely related to property testing and
other areas. Our main aim is to develop removal lemmas of the same spirit for
two dimensional matrices. These are statements of the following type: fix a
finite family $F$ of matrices over some alphabet $\Gamma$. Suppose that for an
$n \times n$ matrix $M$ over $\Gamma$, for any couple of integers $s,t &gt; 0$ at
most $o(n^{s+t})$ of the $s \times t$ submatrices of $M$ are equal to matrices
from $F$. Then one can modify no more than $o(n^2)$ entries in $M$ to make it
$F$-free (that is, after the modification no submatrix of $M$ is equal to a
matrix from $F$).
</p>
<p>As a representative example, one of our main results is the following: fix an
$s \times t$ binary matrix $A$. For any $\epsilon &gt; 0$ there exists $\delta &gt;
0$ such that for any $n \times n$ binary matrix $M$ that contains at most
$\delta n^{s+t}$ copies of $A$, there exists a set of $\epsilon n^2$ entries of
$M$ that intersects every $A$-copy in $M$. Moreover, $\delta^{-1}$ is
polynomial in $\epsilon^{-1}$.
</p>
<p>The major difficulty is that the rows and the columns of a matrix are
ordered. These are the first removal lemma type results for two dimensional
graph-like objects with a predetermined order. Our results have direct
consequences in matrix property testing: they imply that for several types of
families $F$ and choices of the alphabet $\Gamma$, one can determine with good
probability whether a given matrix $M$ is $F$-free or $\epsilon$-far from
$F$-freeness (i.e., one needs to change at least an $\epsilon$-fraction of its
entries to make it $F$-free) by sampling only a constant number of entries in
$M$.
</p>
<p>In particular, we generalize an efficient induced removal lemma for bipartite
graphs of Alon, Fischer and Newman, making progress towards settling one of
their open problems.
</p>
{{{{ARTICLE_PARSER}}}}<p>Convolutional Neural Networks (CNNs) are extremely efficient, since they
exploit the inherent translation-invariance of natural images. However,
translation is just one of a myriad of useful spatial transformations. Can the
same efficiency be attained when considering other spatial invariances? Such
generalized convolutions have been considered in the past, but at a high
computational cost. We present a construction that is simple and exact, yet has
the same computational complexity that standard convolutions enjoy. It consists
of a constant image warp followed by a simple convolution, which are standard
blocks in deep learning toolboxes. With a carefully crafted warp, the resulting
architecture can be made invariant to one of a wide range of spatial
transformations. We show encouraging results in realistic scenarios, including
the estimation of vehicle poses in the Google Earth dataset (rotation and
scale), and face poses in Annotated Facial Landmarks in the Wild (3D rotations
under perspective).
</p>
{{{{ARTICLE_PARSER}}}}</div>
<div style="visibility:hidden; display: none;" id="links">http://arxiv.org/pdf/1609.04398{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04399{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04400{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04401{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04402{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04403{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04405{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04407{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04409{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04410{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04411{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04412{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04413{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04416{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04423{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04425{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04434{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04444{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04451{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04452{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04457{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04462{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04465{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04470{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04474{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04485{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04509{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04520{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04527{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04576{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04580{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04583{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04592{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04599{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04612{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04634{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04635{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04660{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04671{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04674{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04677{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04684{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04686{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04688{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04701{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04716{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04724{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04726{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04728{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04739{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04742{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04743{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04751{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04755{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04756{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04770{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04773{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04786{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04788{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04798{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04804{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04805{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04808{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1208.3537{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1510.01692{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1512.00853{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1512.08639{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1512.08791{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1602.08121{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1603.02606{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.01719{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.05289{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.06093{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.08898{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.04310{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.05930{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.08296{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.01964{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.02520{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.07133{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.08869{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.00508{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.00737{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.00906{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.01925{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.02498{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.02604{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04389{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04395{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.00391{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04008{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04459{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04503{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04570{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04759{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04769{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.02171{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.06645{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04456{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04459{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04463{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04537{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04570{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04575{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04651{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04759{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04803{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1512.07520{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.05291{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.02171{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04460{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04498{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04516{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04546{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04559{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04594{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04668{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04754{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04799{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1409.3562{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1502.02952{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1508.04784{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1511.09418{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.01710{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.01696{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03120{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03220{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04408{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04421{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04427{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04460{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04485{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04492{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04505{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04524{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04562{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04576{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04625{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04645{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04662{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04663{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04673{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04679{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04689{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04695{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04696{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04703{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04720{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04754{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04762{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04763{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04796{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04800{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1311.7470{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1409.3562{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1411.3334{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1507.05051{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1512.05034{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1603.03761{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.00850{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.07165{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.07237{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.07367{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.08105{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.02751{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.07539{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.05392{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.08394{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.03122{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.04016{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.05641{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.08393{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.01295{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.01696{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.02225{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04049{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04417{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04436{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04439{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04449{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04453{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04468{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04471{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04493{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04495{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04499{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04504{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04508{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04512{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04514{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04521{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04535{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04536{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04541{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04547{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04549{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04552{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04553{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04554{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04556{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04557{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04566{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04567{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04569{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04585{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04586{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04593{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04600{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04602{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04603{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04604{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04606{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04608{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04618{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04621{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04623{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04628{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04629{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04648{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04652{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04653{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04656{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04657{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04659{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04661{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04664{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04666{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04667{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04683{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04693{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04705{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04718{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04722{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04723{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04727{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04730{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04735{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04741{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04745{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04747{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04757{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04767{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04779{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04781{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04782{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04785{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04789{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04793{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04794{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04802{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1309.5502{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1402.2970{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1405.2600{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1408.0943{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1409.3562{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1411.3334{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1504.06952{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1505.06747{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1506.01406{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1510.05175{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1510.08237{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1511.01957{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1511.04136{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1512.03236{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1512.04202{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1512.08250{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1512.09080{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1601.00149{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1601.05991{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1603.03947{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1603.04184{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.01643{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.03655{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.04397{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.05899{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.03072{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.05716{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.08485{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.04042{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.05158{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.06921{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.08571{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.01299{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.03791{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.00905{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.01463{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.04112{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.08710{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.00368{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.00680{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.01408{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.02546{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03695{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03713{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04108{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04235{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.04382{{{{ARTICLE_PARSER}}}}</div>

</body>
</html>
