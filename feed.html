<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Atmospherny feed</title>
<style type="text/css">
@font-face {
    font-family: 'PT Serif';
    src: url(PT_Serif-Web-Regular.ttf);
    font-style: normal;
}

body {
	margin: 0;
}

#container {
	width: 100%;
	max-width: 860px;
	color: #333;
	margin: auto;
	background-color: #eee;
	border-style: solid;
	border-width: 2px;
	border-color: #009;
	padding: 1em;
}

button {
	width: 100%;
	margin: 0;
	background-color: #fff;
	border-style: dashed;
	border-color: #333;
	border-width: 1px;
	font-size: 48pt;
	padding: 0.5em;
	color: #222;
	font-family: Monospace;
	outline: none;
	-webkit-transition: background-color 2s;
	-moz-transition: background-color 2s;
	-o-transition: background-color 2s;
	transition: background-color 2s;
}

button:hover {
	background-color: #aaa;
	-webkit-transition: background-color 0.2s;
	-moz-transition: background-color 0.2s;
	-o-transition: background-color 0.2s;
	transition: background-color 0.2s;
}

table {
	width: 100%;
}

#current {
	font-size: 32px;
	font-family: 'PT Serif';
	padding: 0.3em;
}

#current * {
	font-size: 32px;	
}

summary {
	background-color: #ddd;
	border-style: dotted;
	border-width: 1px;
	padding: 0.3em;
	outline: none;
	cursor: pointer;
}

#progress {
	text-align: center;
}

#weekday {
	text-align: center;
}

a {
	color: #333;
}

a:visited {
	color: #888;
}

</style>

<script type="text/javascript">
var currentIndex = 0;
var myChoice = new Object();

var days = ['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday'];

function o(s) {
	return document.getElementById(s);
}

function next() {
	currentIndex++;
	if (currentIndex == o("titles").innerHTML.split("{{{{ARTICLE_PARSER}}}}").length) currentIndex=0;
	update();
	updateIndex();
}

function prev() {
	currentIndex--;
	if (currentIndex < 0) currentIndex = o("titles").innerHTML.split("{{{{ARTICLE_PARSER}}}}").length - 1;
	update();
	updateIndex();
}

function choose() {
	myChoice["a"+currentIndex] = "tr";
	update()
}

function clubs() {
	myChoice["a"+currentIndex] = "bl";
	update()
}

function spades() {
	myChoice["a"+currentIndex] = "br";
	update()	
}

function unchoose() {
	myChoice["a"+currentIndex] = "tl";
	update()
}

function ifchosen() {
	switch(myChoice["a"+currentIndex]) {
		case "tr":
			o("current-title").innerHTML = "&hearts; " + o("current-title").innerHTML
			break;
		case "br":
			o("current-title").innerHTML = "&spades; " + o("current-title").innerHTML
			break;
		case "bl":
			o("current-title").innerHTML = "&clubs; " + o("current-title").innerHTML
			break;
		default:
			o("current-title").innerHTML = "&diams; " + o("current-title").innerHTML		
	}
}

function update() {
	o("current-title").innerHTML = o("titles").innerHTML.split("{{{{ARTICLE_PARSER}}}}")[currentIndex];
	o("current-abstract").innerHTML = o("abstracts").innerHTML.split("{{{{ARTICLE_PARSER}}}}")[currentIndex] + "<br>" + "<a href='" + o("links").innerHTML.split("{{{{ARTICLE_PARSER}}}}")[currentIndex] + "' target='_blank'>download pdf</a>";
	ifchosen();
	updateStorage();
	o("progress").innerHTML = "#" + (currentIndex + 1);
}

function updateStorage() {
	localStorage["myChoice"] = JSON.stringify(myChoice)
}

function updateIndex() {
	localStorage["currentIndex"] = currentIndex;
}

function updateChoice() {
	try {
		myChoice = JSON.parse(localStorage["myChoice"]);
		currentIndex = parseInt(localStorage["currentIndex"]);
		if (typeof(currentIndex)!= "number") {
			localStorage["currentIndex"] = 0;
			currentIndex = 0;
		}
	} catch(e) {
		localStorage["currentIndex"] = 0;
		localStorage["myChoice"] = JSON.stringify("{}");
		currentIndex = 0;
		myChoice = {};
	} 
}

function listA() {
	buf = ""
	for(var i in myChoice) {
		if ((myChoice[i] == "tr") || (myChoice[i] == "bl")) buf += "#" + (parseInt(i.split("a")[1]) + 1) + " "
	}
	window.prompt("Share your choice:", buf);
}

function listB() {
	buf = ""
	for(var i in myChoice) {
		if ((myChoice[i] == "tr") || (myChoice[i] == "br")) buf += "#" + (parseInt(i.split("a")[1]) + 1) + " "
	}
	window.prompt("Share your choice:", buf);
}

function setWeekday() {
	var d = new Date();
	var n = d.getUTCHours();
	if (o("weekday").innerHTML != days[now.getDay()]) {
		if((days[now.getDay()] != "Saturday") && (days[now.getDay()] != "Sunday")) {
			if(n>3) {
				localStorage["myChoice"] = JSON.stringify({});
				localStorage["currentIndex"] = 0;
			}
		}
	}
}

</script>
</head>
<body onload="updateChoice();update()">

<div id="container">
<div id="current">

<div id="weekday">Wednesday</div>
<div id="progress"></div>

<p id="current-title">

</p>

<details>
<summary>abstract & pdf</summary>
<p id="current-abstract">

</p>
</details>

</div>
<table>
<tr>
<td><button onclick="prev()">
<
</button></td><td><button onclick="next()">
>
</button></td>
</tr>
<tr>
<td><button onclick="unchoose()">
&diams;
</button></td><td><button onclick="choose()">
&hearts;
</button></td>
</tr>
<tr>
<td><button onclick="clubs()">
&clubs;
</button></td><td><button onclick="spades()">
&spades;
</button></td>
</tr>

<tr><td><button onclick="listA()">{&clubs;,&hearts;}</button></td><td><button onclick="listB()">{&spades;,&hearts;}</button></td></tr>

</table>

</div>

<div id="viewer">

</div>

<div style="visibility:hidden; display: none;" id="titles">Turbulence and Vorticity in Galaxy Clusters Generated by Structure Formation.{{{{ARTICLE_PARSER}}}}Spatially resolved variations of the IMF mass normalisation in early-type galaxies as probed by molecular gas kinematics.{{{{ARTICLE_PARSER}}}}Observational Signatures of High-Redshift Quasars and Local Relics of Black Hole Seeds.{{{{ARTICLE_PARSER}}}}On the Gravitational Wave Background from Black Hole Binaries after the First LIGO Detections.{{{{ARTICLE_PARSER}}}}Probing WHIM around Galaxy Clusters with Fast Radio Bursts and the Sunyaev-Zel'dovich effect.{{{{ARTICLE_PARSER}}}}The High Cadence Transient Survey (HiTS) - I. Survey design and supernova shock breakout constraints.{{{{ARTICLE_PARSER}}}}Photo-$z$ with CuBAN$z$: An improved photometric redshift estimator using Clustering aided Back Propagation Neural network.{{{{ARTICLE_PARSER}}}}Partially Acoustic Dark Matter, Interacting Dark Radiation, and Large Scale Structure.{{{{ARTICLE_PARSER}}}}Corrections to $n_s$ and $n_t$ from high scale physics.{{{{ARTICLE_PARSER}}}}The growth of typical star-forming galaxies and their super massive black holes across cosmic time since z~2.{{{{ARTICLE_PARSER}}}}The evolution of star formation histories of quiescent galaxies.{{{{ARTICLE_PARSER}}}}The Milky Way's halo in 6D: Gaia's Radial Velocity Spectrometer performance.{{{{ARTICLE_PARSER}}}}Performance study of Lagrangian methods: reconstruction of large scale peculiar velocities and baryonic acoustic oscillations.{{{{ARTICLE_PARSER}}}}Unbiased pseudo-Cl power spectrum estimation with mode projection.{{{{ARTICLE_PARSER}}}}The Way To a Double Degenerate: $\sim15-20$ per cent of $1M_{\odot} \le M \le 8M_{\odot}$ Stars have a $M>1M_{\odot}$ Companion.{{{{ARTICLE_PARSER}}}}Uniform Contribution of Supernova Explosions to the Chemical Enrichment of Abell 3112 out to R200.{{{{ARTICLE_PARSER}}}}UV Absorption Line Ratios in Circumgalactic Medium at Low Redshift in Realistic Cosmological Hydrodynamic Simulations.{{{{ARTICLE_PARSER}}}}Long-term, Multiwavelength Light Curves of Ultra-cool Dwarfs: I. An Interplay of Starspots & Clouds Likely Drive the Variability of the L3.5 dwarf 2MASS 0036+18.{{{{ARTICLE_PARSER}}}}Long-term, Multiwavelength Light Curves of Ultra-Cool Dwarfs: II. The evolving Light Curves of the T2.5 SIMP 0136 & the Uncorrelated Light Curves of the M9 TVLM 513.{{{{ARTICLE_PARSER}}}}Discovery and Precise Characterization by the MEarth Project of LP 661-13, an Eclipsing Binary Consisting of Two Fully Convective Low-mass Stars.{{{{ARTICLE_PARSER}}}}Hidden Sector Hydrogen as Dark Matter: Small-scale Structure Formation Predictions and the Importance of Hyperfine Interactions.{{{{ARTICLE_PARSER}}}}The SAMI galaxy survey: Galaxy Interactions and Kinematic Anomalies in Abell 119.{{{{ARTICLE_PARSER}}}}Relativistic axions from collapsing Bose stars.{{{{ARTICLE_PARSER}}}}Equations for spinning test particles in equatorial orbits when they are orbiting in a weak rotating field.{{{{ARTICLE_PARSER}}}}Progenitors of type Ia supernovae.{{{{ARTICLE_PARSER}}}}Supermassive Black Hole Binary Environments: Effects on the Scaling Laws and Time to Detection for the Stochastic Background.{{{{ARTICLE_PARSER}}}}Radial-Velocity Fitting Challenge. II. First results of the analysis of the data set.{{{{ARTICLE_PARSER}}}}Measurement of Redshift Space Power Spectrum for BOSS galaxies and the Growth Rate at redshift 0.57.{{{{ARTICLE_PARSER}}}}Propagation of Acoustic Waves in Troposphere and Stratosphere.{{{{ARTICLE_PARSER}}}}Evolution of the Magnetic Field Distribution of Active Regions.{{{{ARTICLE_PARSER}}}}Cosmic backreaction and Gauss's law.{{{{ARTICLE_PARSER}}}}Interaction between the Supernova Remnant HB 3 and the Nearby Star-Forming Region W3.{{{{ARTICLE_PARSER}}}}On the Evolution of High-Redshift Active Galactic Nuclei.{{{{ARTICLE_PARSER}}}}LIC and LID considerations in the design and implementation of the MEMS laser pointing mechanism for the EUSO UV laser altimeter.{{{{ARTICLE_PARSER}}}}HI in Group Interactions: HCG 44.{{{{ARTICLE_PARSER}}}}The Filter Wheel and Filters development for the X-IFU instrument on-board Athena.{{{{ARTICLE_PARSER}}}}Finding faint HI structure in and around galaxies: scraping the barrel.{{{{ARTICLE_PARSER}}}}The molecular gas mass of M33.{{{{ARTICLE_PARSER}}}}Searching for the Time Variation in Supernova Remnant RX J1713.7-3946.{{{{ARTICLE_PARSER}}}}Convection in axially symmetric accretion discs with microscopic transport coefficients.{{{{ARTICLE_PARSER}}}}Forecasting performance of CMB experiments in the presence of complex foreground contaminations.{{{{ARTICLE_PARSER}}}}On the distance to the North Polar Spur and the local CO-H2 factor.{{{{ARTICLE_PARSER}}}}Photospheric response to EB-like event.{{{{ARTICLE_PARSER}}}}RAVE stars in K2 - I. Improving RAVE red giants spectroscopy using asteroseismology from K2 Campaign 1.{{{{ARTICLE_PARSER}}}}A new galactic chemical evolution model with dust: results for dwarf irregular galaxies and DLA systems.{{{{ARTICLE_PARSER}}}}A study of the circum-galactic medium at z ~ 0.6 using DLA-galaxies.{{{{ARTICLE_PARSER}}}}Recent changes in a flickering variability of the black hole X-ray transient V616 Mon = A0620-00.{{{{ARTICLE_PARSER}}}}High-precision limit on variation in the fine-structure constant from a single quasar absorption system.{{{{ARTICLE_PARSER}}}}Microscopic Calculations of Vortex-Nucleus Interaction in the Neutron Star Crust.{{{{ARTICLE_PARSER}}}}Modelling high-resolution spatially-resolved Supernova Remnant spectra with the Sardinia Radio Telescope.{{{{ARTICLE_PARSER}}}}Observations of Supernova Remnants with the Sardinia Radio Telescope.{{{{ARTICLE_PARSER}}}}Powers and Magnetization of Blazar Jets.{{{{ARTICLE_PARSER}}}}HI Observations of Galaxies in the Southern Filament of the Virgo Cluster with the SKA Pathfinder KAT-7 and the WSRT.{{{{ARTICLE_PARSER}}}}Probing Acceleration and Turbulence at Relativistic Shocks in Blazar Jets.{{{{ARTICLE_PARSER}}}}Matter of Life & Death: The impact of environmental conditions on the origins of stars and supermassive black holes.{{{{ARTICLE_PARSER}}}}A photo-evaporative gap in the closest planet forming disc.{{{{ARTICLE_PARSER}}}}Transits Probabilities Around Hypervelocity and Runaway Stars.{{{{ARTICLE_PARSER}}}}Potassium detection in the clear atmosphere of a hot-Jupiter: WASP-17b transmission spectroscopy.{{{{ARTICLE_PARSER}}}}A generalized bayesian inference method for constraining the interiors of super Earths and sub-Neptunes.{{{{ARTICLE_PARSER}}}}Bayesian analysis of interiors of HD 219134b, Kepler-10b, Kepler-93b, CoRoT-7b, 55 Cnc e, and HD 97658b using stellar abundance proxies.{{{{ARTICLE_PARSER}}}}Diffusion in plasma: the Hall effect, compositional waves, and chemical spots.{{{{ARTICLE_PARSER}}}}Mapping the Similarities of Spectra: Global and Locally-biased Approaches to SDSS Galaxy Data.{{{{ARTICLE_PARSER}}}}Multi-Wavelength Polarimetry and Spectral Study of M87 Jet During 2002-2008.{{{{ARTICLE_PARSER}}}}Gravitational Waves as a New Probe of Bose-Einstein Condensate Dark Matter.{{{{ARTICLE_PARSER}}}}Spectroscopic comparison between ultraluminous X-ray sources and magnetar bursts.{{{{ARTICLE_PARSER}}}}Some observational tests of a minimal galaxy formation model.{{{{ARTICLE_PARSER}}}}High-Fidelity VLA Imaging of the Radio Structure of 3C273.{{{{ARTICLE_PARSER}}}}Probability of coincidental similarity among the orbits of small bodies - I. Pairing.{{{{ARTICLE_PARSER}}}}Large-scale magnetic field in the accretion discs of young stars: the influence of magnetic diffusion, buoyancy and Hall effect.{{{{ARTICLE_PARSER}}}}Do dark matter halos explain lensing peaks?.{{{{ARTICLE_PARSER}}}}Quantum-gravity-induced dual lensing and IceCube neutrinos.{{{{ARTICLE_PARSER}}}}Calibrating Cluster Number Counts with CMB lensing.{{{{ARTICLE_PARSER}}}}Large Number, Dark Matter, Dark Energy, and the Superstructures in the Universe (with Extension).{{{{ARTICLE_PARSER}}}}IVOA Recommendation: VOTable Format Definition Version 1.3.{{{{ARTICLE_PARSER}}}}Dark influences III. Structural characterization of minor mergers of dwarf galaxies with dark satellites.{{{{ARTICLE_PARSER}}}}Time series analysis of long-term photometry of BM Canum Venaticorum.{{{{ARTICLE_PARSER}}}}On neutron stars in f(R) theories: small radii, large masses and large energy emitted in a merger.{{{{ARTICLE_PARSER}}}}Transformations between Jordan and Einstein frames: Bounces, antigravity, and crossing singularities.{{{{ARTICLE_PARSER}}}}Enhanced tidal stripping of satellites in the galactic halo from dark matter self-interactions.{{{{ARTICLE_PARSER}}}}Cosmological Constraints on Higgs-Dilaton Inflation.{{{{ARTICLE_PARSER}}}}Primordial features and Planck polarization.{{{{ARTICLE_PARSER}}}}Discovery of transient infrared emission from dust heated by stellar tidal disruption flares.{{{{ARTICLE_PARSER}}}}Soft Gamma Rays from Heavy WIMPs.{{{{ARTICLE_PARSER}}}}Gravitational waves within the magnetar model of superluminous supernovae and gamma-ray bursts.{{{{ARTICLE_PARSER}}}}NoSOCS in SDSS. V. Red Disc and Blue Bulge Galaxies Across Different Environments.{{{{ARTICLE_PARSER}}}}Qatar Exoplanet Survey : Qatar-3b, Qatar-4b and Qatar-5b.{{{{ARTICLE_PARSER}}}}Hydrogen Epoch of Reionization Array (HERA).{{{{ARTICLE_PARSER}}}}On the Formation and Chemical Composition of Super Earths.{{{{ARTICLE_PARSER}}}}A luminous hot accretion flow in the low-luminosity active galactic nucleus NGC 7213.{{{{ARTICLE_PARSER}}}}Relating information entropy and mass variance to measure bias and non-Gaussianity.{{{{ARTICLE_PARSER}}}}Formation and evolution of blue compact dwarfs: The origin of their steep rotation curves.{{{{ARTICLE_PARSER}}}}Disky elliptical galaxies and the allegedly over-massive black hole in the compact massive `ES' galaxy NGC 1271.{{{{ARTICLE_PARSER}}}}Chandra and Swift X-ray Observations of the X-ray Pulsar SMC X-2 During the Outburst of 2015.{{{{ARTICLE_PARSER}}}}Metals in the z~3 intergalactic medium: results from an ultra-high signal-to-noise ratio UVES quasar spectrum.{{{{ARTICLE_PARSER}}}}AKARI near-infrared spectroscopy of the extended green object G318.05+0.09: Detection of CO fundamental ro-vibrational emission.{{{{ARTICLE_PARSER}}}}Asteroseismic properties of solar-type stars observed with the NASA K2 mission: results from Campaigns 1-3 and prospects for future observations.{{{{ARTICLE_PARSER}}}}Double Compton and Cyclo-Synchrotron in Super-Eddington Disks, Magnetized Coronae, and Jets.{{{{ARTICLE_PARSER}}}}Detecting fast radio bursts at decametric wavelengths.{{{{ARTICLE_PARSER}}}}An Ordered Magnetic Field in the Protoplanetary Disk of AB Aur Revealed by Mid-Infrared Polarimetry.{{{{ARTICLE_PARSER}}}}High proper motion objects towards the inner Milky Way: characterisation of newly identified nearby stars from the VISTA Variables in the Via Lactea Survey.{{{{ARTICLE_PARSER}}}}Predicted Information Content of Gaia-Hipparcos.{{{{ARTICLE_PARSER}}}}The hydrodynamic stability of gaseous cosmic filaments.{{{{ARTICLE_PARSER}}}}The panchromatic view of the Magellanic Clouds from Classical Cepheids. I. Distance, Reddening and Geometry of the Large Magellanic Cloud disk.{{{{ARTICLE_PARSER}}}}Arrival Times of Gravitational Radiation Peaks for Binary Inspiral.{{{{ARTICLE_PARSER}}}}Black Hole Ringing, Quasinormal Modes, and Light Rings.{{{{ARTICLE_PARSER}}}}Breaking the sound barrier in AdS/CFT.{{{{ARTICLE_PARSER}}}}Vector boson and charmonia measurements in $p$+Pb collisions with ATLAS.{{{{ARTICLE_PARSER}}}}Search for the decay of nature's rarest isotope $^{\rm {180m}}$Ta.{{{{ARTICLE_PARSER}}}}Commissioning of the COBRA demonstrator and investigation of surface events as its main background.{{{{ARTICLE_PARSER}}}}Exclusive photoproduction of a $\gamma\,\rho$ pair with a large invariant mass.{{{{ARTICLE_PARSER}}}}New results related to QGP-like effects in small systems with ALICE.{{{{ARTICLE_PARSER}}}}Di-Jet Imbalance Measurements at $\sqrt{s_{NN}} = 200$ GeV at STAR.{{{{ARTICLE_PARSER}}}}Extraction of $t$-slopes from experimental $\gamma p\rightarrow K^+\Lambda$ and $\gamma p\rightarrow K^+\Sigma_0$ cross section data.{{{{ARTICLE_PARSER}}}}A high-resolution CMOS imaging detector for the search of neutrinoless double beta decay in $^{82}$Se.{{{{ARTICLE_PARSER}}}}Measurement of electrons from beauty-hadron decays in p-Pb collisions at $\mathbf{\sqrt{s_{\rm NN}}=5.02}$ TeV and Pb-Pb collisions at $\mathbf{\sqrt{s_{\rm NN}}=2.76}$ TeV.{{{{ARTICLE_PARSER}}}}The 5 MeV bump - a nuclear whodunit mystery.{{{{ARTICLE_PARSER}}}}A Database for Storing the Results of Material Radiopurity Measurements.{{{{ARTICLE_PARSER}}}}Dispersive treatment of $K_S\to\gamma\gamma$ and $K_S\to\gamma\ell^+\ell^-$.{{{{ARTICLE_PARSER}}}}Collisions of Small Nuclei in the Thermal Model.{{{{ARTICLE_PARSER}}}}Hidden Local Symmetry and Beyond.{{{{ARTICLE_PARSER}}}}Phonon-particle coupling effects in single-particle energies of semi-magic nuclei.{{{{ARTICLE_PARSER}}}}Light-Front spin-dependent Spectral Function and Nucleon Momentum Distributions for a Three-Body System.{{{{ARTICLE_PARSER}}}}Exclusive photoproduction of a $\gamma\,\rho$ pair with a large invariant mass.{{{{ARTICLE_PARSER}}}}Microscopic Calculations of Vortex-Nucleus Interaction in the Neutron Star Crust.{{{{ARTICLE_PARSER}}}}$\phi$ meson self-energy in nuclear matter from $\phi N$ resonant interactions.{{{{ARTICLE_PARSER}}}}The 5 MeV bump - a nuclear whodunit mystery.{{{{ARTICLE_PARSER}}}}Energy loss as the origin of an universal scaling law of the elliptic flow.{{{{ARTICLE_PARSER}}}}Van der Waals interactions in hadron resonance gas: From nuclear matter to lattice QCD.{{{{ARTICLE_PARSER}}}}Octet Baryon Magnetic Moments from Lattice QCD: Approaching Experiment from the Three-Flavor Symmetric Point.{{{{ARTICLE_PARSER}}}}On neutron stars in f(R) theories: small radii, large masses and large energy emitted in a merger.{{{{ARTICLE_PARSER}}}}Breaking the sound barrier in AdS/CFT.{{{{ARTICLE_PARSER}}}}Droplet phase in a nonlocal isoperimetric problem under confinement.{{{{ARTICLE_PARSER}}}}$SU(1,1)$ solution for the Dunkl-Coulomb problem in two dimensions and its coherent states.{{{{ARTICLE_PARSER}}}}A vanishing theorem for co-Higgs bundles on the moduli space of bundles.{{{{ARTICLE_PARSER}}}}Linear Stability of Hyperbolic Moment Models for Boltzmann Equation.{{{{ARTICLE_PARSER}}}}Basis Tensor Gauge Theory.{{{{ARTICLE_PARSER}}}}Statistical analysis of low rank tomography with compressive random measurements.{{{{ARTICLE_PARSER}}}}Intertwinings for general $\beta$ Laguerre and Jacobi processes.{{{{ARTICLE_PARSER}}}}A Tannakian approach to dimensional reduction of principal bundles.{{{{ARTICLE_PARSER}}}}Quantization of compact Riemannian symmetric spaces.{{{{ARTICLE_PARSER}}}}One-Dimensional Super Calabi-Yau Manifolds and their Mirrors.{{{{ARTICLE_PARSER}}}}Rigidity of the Laughlin liquid.{{{{ARTICLE_PARSER}}}}Modified Scattering and Beating Effect for Coupled Schr\\"odinger Systems on Product Spaces with Small Initial Data.{{{{ARTICLE_PARSER}}}}Equidistribution of jellium energy for Coulomb and Riesz Interactions.{{{{ARTICLE_PARSER}}}}2D 2nd order Laplace superintegrable systems, Heun equations, QES and Bocher contractions.{{{{ARTICLE_PARSER}}}}Inapproximability of the Partition Function for the Antiferromagnetic Ising and Hard-Core Models.{{{{ARTICLE_PARSER}}}}Polyakov relation for the sphere and higher genus surfaces.{{{{ARTICLE_PARSER}}}}Sum of exit times in a series of two metastable states.{{{{ARTICLE_PARSER}}}}Superstring theories as low-energy limit of supergroup gauge theories.{{{{ARTICLE_PARSER}}}}Operator systems and convex sets with many normal cones.{{{{ARTICLE_PARSER}}}}Mixed symmetry Wilson-loop interactions in the worldline formalism.{{{{ARTICLE_PARSER}}}}The energy of the alphabet model.{{{{ARTICLE_PARSER}}}}Real bundle gerbes, orientifolds and twisted KR-homology.{{{{ARTICLE_PARSER}}}}Classical Holographic Codes.{{{{ARTICLE_PARSER}}}}Geometric Phase of a Spin-1/2 Particle Coupled to a Quantum Vector Operator.{{{{ARTICLE_PARSER}}}}Dual Matter-Wave Inertial Sensors in Weightlessness.{{{{ARTICLE_PARSER}}}}Fixed-Point Adiabatic Quantum Search.{{{{ARTICLE_PARSER}}}}Extreme Quantum Advantage when Simulating Strongly Coupled Classical Systems.{{{{ARTICLE_PARSER}}}}Subsystems and time in quantum mechanics.{{{{ARTICLE_PARSER}}}}Parallel self-testing of (tilted) EPR pairs via copies of (tilted) CHSH.{{{{ARTICLE_PARSER}}}}Efficiency bounds on thermoelectric transport in magnetic fields: The role of inelastic processes.{{{{ARTICLE_PARSER}}}}Shortcuts to adiabaticity in the double well.{{{{ARTICLE_PARSER}}}}Non-adiabatic quantum phase transition in a trapped spinor condensate.{{{{ARTICLE_PARSER}}}}Methods for Finding Analytic Solutions for Time Dependent Two-Level Quantum Systems and Its Generalizations.{{{{ARTICLE_PARSER}}}}From the discrete Weyl -- Wigner formalism for symmetric ordering to a number -- phase Wigner function.{{{{ARTICLE_PARSER}}}}Statistical analysis of low rank tomography with compressive random measurements.{{{{ARTICLE_PARSER}}}}Concrete Security Against Adversaries with Quantum Superposition Access to Encryption and Decryption Oracles.{{{{ARTICLE_PARSER}}}}Exponential Enhancement of the Efficiency of Quantum Annealing by Non-Stochastic Hamiltonians.{{{{ARTICLE_PARSER}}}}Dynamic Stark effect, light emission, and entanglement generation in a laser-driven quantum optical system.{{{{ARTICLE_PARSER}}}}Three-players conflicting interest games and nonlocality.{{{{ARTICLE_PARSER}}}}Local correlations in the attractive 1D Bose gas: from Bethe ansatz to the Gross-Pitaevskii equation.{{{{ARTICLE_PARSER}}}}Spin-0 to Spin-1/2 Deterministic Dynamics: From Relativistic Quantum Potential to Quantum Stress Tensor.{{{{ARTICLE_PARSER}}}}Quantum Pseudo-Telepathy in Spin Systems: Magic Square Game Under Magnetic Fields and Dzyaloshinskii-Moriya Interaction.{{{{ARTICLE_PARSER}}}}Optimal Ultra-wide Spatial-Spectral Windows for Hyperentangled Two-photon Emission.{{{{ARTICLE_PARSER}}}}Atoms and Molecules in Cavities: From Weak to Strong Coupling in QED Chemistry.{{{{ARTICLE_PARSER}}}}Entanglement verification with detection-efficiency mismatch.{{{{ARTICLE_PARSER}}}}Selective Purcell enhancement of two closely linked zero-phonon transitions of a silicon carbide color center.{{{{ARTICLE_PARSER}}}}Liquid quantum droplets of ultracold magnetic atoms.{{{{ARTICLE_PARSER}}}}Demonstration of the Jaynes-Cummings ladder with Rydberg-dressed atoms.{{{{ARTICLE_PARSER}}}}Fundamental limitation on quantum broadcast networks.{{{{ARTICLE_PARSER}}}}An Anisotropic Landau-Lifschitz-Gilbert model of dissipation in qubits.{{{{ARTICLE_PARSER}}}}Causal and causally separable processes.{{{{ARTICLE_PARSER}}}}A one parameter fit for glassy dynamics as a quantum corollary of the liquid to solid transition.{{{{ARTICLE_PARSER}}}}Optical trapping of nanoparticles by full solid-angle focusing.{{{{ARTICLE_PARSER}}}}Sequential feedback scheme outperforms the parallel scheme for Hamiltonian parameter estimation.{{{{ARTICLE_PARSER}}}}Entanglement Detection with Fewer Measurements based on the Geometric Criterion.{{{{ARTICLE_PARSER}}}}An optical nanofiber-based interface for single molecules.{{{{ARTICLE_PARSER}}}}Non-convexity of private capacity and classical environment-assisted capacity of a quantum channel.{{{{ARTICLE_PARSER}}}}Single-photon absorber based on strongly interacting Rydberg atoms.{{{{ARTICLE_PARSER}}}}Relaxation time for monitoring the quantumness of an intense cavity field.{{{{ARTICLE_PARSER}}}}Asymptotic Entropy Bounds.{{{{ARTICLE_PARSER}}}}Mixed symmetry Wilson-loop interactions in the worldline formalism.{{{{ARTICLE_PARSER}}}}Quenching a Quantum Critical State by the Order Parameter: Dynamical Quantum Phase Transitions and Quantum Speed Limits.{{{{ARTICLE_PARSER}}}}Symmetry Enrichment in Three-Dimensional Topological Phases.{{{{ARTICLE_PARSER}}}}How many bits specify a quantum state?.{{{{ARTICLE_PARSER}}}}Survey and Taxonomy of Self-Aware and Self-Adaptive Autoscaling Systems in the Cloud.{{{{ARTICLE_PARSER}}}}Noisy Beam Alignment Techniques for Reciprocal MIMO Channels.{{{{ARTICLE_PARSER}}}}Detecting Text in Natural Image with Connectionist Text Proposal Network.{{{{ARTICLE_PARSER}}}}Recommendations for Intelligent Code Reorganization.{{{{ARTICLE_PARSER}}}}Integrated, reliable and cloud-based personal health record: A scoping review.{{{{ARTICLE_PARSER}}}}Reliable Attribute-Based Object Recognition Using High Predictive Value Classifiers.{{{{ARTICLE_PARSER}}}}Co-active Learning to Adapt Humanoid Movement for Manipulation.{{{{ARTICLE_PARSER}}}}Wireless Security with Beamforming Technique.{{{{ARTICLE_PARSER}}}}Joint Extraction of Events and Entities within a Document Context.{{{{ARTICLE_PARSER}}}}Compiling Process Networks to Interaction Nets.{{{{ARTICLE_PARSER}}}}In-place Graph Rewriting with Interaction Nets.{{{{ARTICLE_PARSER}}}}Kruskal's Tree Theorem for Acyclic Term Graphs.{{{{ARTICLE_PARSER}}}}Reasoning about Graph Programs.{{{{ARTICLE_PARSER}}}}Token-passing Optimal Reduction with Embedded Read-back.{{{{ARTICLE_PARSER}}}}Efficient Completion of Weighted Automata.{{{{ARTICLE_PARSER}}}}Introducing Distributed Dynamic Data-intensive (D3) Science: Understanding Applications and Infrastructure.{{{{ARTICLE_PARSER}}}}Proceedings of the Seventh International Symposium on Games, Automata, Logics and Formal Verification.{{{{ARTICLE_PARSER}}}}Extreme Quantum Advantage when Simulating Strongly Coupled Classical Systems.{{{{ARTICLE_PARSER}}}}Towards Energy-Efficient Communication Management in the Distributed Control of Networked Cyber-Physical Systems.{{{{ARTICLE_PARSER}}}}DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images.{{{{ARTICLE_PARSER}}}}An Experimental Study of LSTM Encoder-Decoder Model for Text Simplification.{{{{ARTICLE_PARSER}}}}Pursuit on a Graph under Partial Information from Sensors.{{{{ARTICLE_PARSER}}}}A Greedy Algorithm to Cluster Specialists.{{{{ARTICLE_PARSER}}}}Longest Common Subsequence in at Least $k$ Length Order-isomorphic Substrings.{{{{ARTICLE_PARSER}}}}Recurrent Coevolutionary Feature Embedding Processes for Recommendation.{{{{ARTICLE_PARSER}}}}Unsupervised Monocular Depth Estimation with Left-Right Consistency.{{{{ARTICLE_PARSER}}}}Making Neural Networks Robust to Label Noise: a Loss Correction Approach.{{{{ARTICLE_PARSER}}}}Construction of Gray maps for groups of order 16.{{{{ARTICLE_PARSER}}}}Blending Entropy: A Term for Adressing Information Density in Mediated Reality.{{{{ARTICLE_PARSER}}}}Power Allocation and Effective Capacity of AF Successive Relays.{{{{ARTICLE_PARSER}}}}Social Learning over Weakly-Connected Graphs.{{{{ARTICLE_PARSER}}}}A Bayesian implementable social choice function may not be truthfully implementable.{{{{ARTICLE_PARSER}}}}Dynamic Proximity-aware Resource Allocation in Vehicle-to-Vehicle (V2V) Communications.{{{{ARTICLE_PARSER}}}}Stability of intersections of graphs in the plane and the van Kampen obstruction.{{{{ARTICLE_PARSER}}}}Large-scale multiscale particle models in inhomogeneous domains: modelling and implementation.{{{{ARTICLE_PARSER}}}}Hacking of the AES with Boolean Functions.{{{{ARTICLE_PARSER}}}}Small Extended Formulation for Knapsack Cover Inequalities from Monotone Circuits.{{{{ARTICLE_PARSER}}}}nsroot: Minimalist Process Isolation Tool Implemented With Linux Namespaces.{{{{ARTICLE_PARSER}}}}Correlations of consumption patterns in social-economic networks.{{{{ARTICLE_PARSER}}}}3D Simulation for Robot Arm Control with Deep Q-Learning.{{{{ARTICLE_PARSER}}}}Estimating the Distance Between Macro Base Station and Users in Heterogeneous Networks.{{{{ARTICLE_PARSER}}}}Graph Aggregation.{{{{ARTICLE_PARSER}}}}Some Open Problems related to Creative Telescoping.{{{{ARTICLE_PARSER}}}}Analysis of Kelner and Levin graph sparsification algorithm for a streaming setting.{{{{ARTICLE_PARSER}}}}Learning conditional independence structure for high-dimensional uncorrelated vector processes.{{{{ARTICLE_PARSER}}}}Lie-X: Depth Image Based Articulated Object Pose Estimation, Tracking, and Action Recognition on Lie Groups.{{{{ARTICLE_PARSER}}}}Character-Level Language Modeling with Hierarchical Recurrent Neural Networks.{{{{ARTICLE_PARSER}}}}Concrete Security Against Adversaries with Quantum Superposition Access to Encryption and Decryption Oracles.{{{{ARTICLE_PARSER}}}}A Proximal Gradient Algorithm for Decentralized Composite Optimization over Directed Networks.{{{{ARTICLE_PARSER}}}}Towards Deep Compositional Networks.{{{{ARTICLE_PARSER}}}}Quantitative identification of technological discontinuities using simulation modeling.{{{{ARTICLE_PARSER}}}}A Unified Gender-Aware Age Estimation.{{{{ARTICLE_PARSER}}}}Robust Resource Allocation for MIMO Wireless Powered Communication Networks Based on a Non-linear EH Model.{{{{ARTICLE_PARSER}}}}Did the Train Reach its Destination: The Complexity of Finding a Witness.{{{{ARTICLE_PARSER}}}}Instrumenting an SMT Solver to Solve Hybrid Network Reachability Problems.{{{{ARTICLE_PARSER}}}}Probabilistic Saliency Estimation.{{{{ARTICLE_PARSER}}}}Image Decomposition Using a Robust Regression Approach.{{{{ARTICLE_PARSER}}}}C-ITS Deployment in Europe - Current Status and Outlook.{{{{ARTICLE_PARSER}}}}VIPLFaceNet: An Open Source Deep Face Recognition SDK.{{{{ARTICLE_PARSER}}}}Scalable Algorithms for Generating and Analyzing Structural Brain Networks with a Varying Number of Nodes.{{{{ARTICLE_PARSER}}}}Crafting a multi-task CNN for viewpoint estimation.{{{{ARTICLE_PARSER}}}}Design of a Ternary Edge-Triggered D Flip-Flap-Flop for Multiple-Valued Sequential Logic.{{{{ARTICLE_PARSER}}}}Information Theoretic Structure Learning with Confidence.{{{{ARTICLE_PARSER}}}}Updated Core Libraries of the ALPS Project.{{{{ARTICLE_PARSER}}}}Mapping the Similarities of Spectra: Global and Locally-biased Approaches to SDSS Galaxy Data.{{{{ARTICLE_PARSER}}}}Envy-Free Cake-Cutting in Two Dimensions.{{{{ARTICLE_PARSER}}}}A Holistic Approach for Predicting Links in Coevolving Multilayer Networks.{{{{ARTICLE_PARSER}}}}Associating Grasping with Convolutional Neural Network Features.{{{{ARTICLE_PARSER}}}}Method to Assess the Temporal Persistence of Potential Biometric Features: Application to Oculomotor, and Gait-Related Databases.{{{{ARTICLE_PARSER}}}}Noisy Inductive Matrix Completion Under Sparse Factor Models.{{{{ARTICLE_PARSER}}}}Self-Sustaining Iterated Learning.{{{{ARTICLE_PARSER}}}}Feynman Machine: The Universal Dynamical Systems Computer.{{{{ARTICLE_PARSER}}}}Multimodal Attention for Neural Machine Translation.{{{{ARTICLE_PARSER}}}}The CUDA LATCH Binary Descriptor: Because Sometimes Faster Means Better.{{{{ARTICLE_PARSER}}}}A Generic Bet-and-run Strategy for Speeding Up Traveling Salesperson and Minimum Vertex Cover.{{{{ARTICLE_PARSER}}}}SEAL's operating manual: a Spatially-bounded Economic Agent-based Lab.{{{{ARTICLE_PARSER}}}}Inapproximability of the Partition Function for the Antiferromagnetic Ising and Hard-Core Models.{{{{ARTICLE_PARSER}}}}Dictionary learning based image enhancement for rarity detection.{{{{ARTICLE_PARSER}}}}Improvements and Generalizations of Stochastic Knapsack and Multi-Armed Bandit Approximation Algorithms: Full Version.{{{{ARTICLE_PARSER}}}}Optimal learning with Bernstein Online Aggregation.{{{{ARTICLE_PARSER}}}}On Lipschitz Continuity and Smoothness of Loss Functions in Learning to Rank.{{{{ARTICLE_PARSER}}}}Inapproximability of Nash Equilibrium.{{{{ARTICLE_PARSER}}}}Low-Rank Approximation and Completion of Positive Tensors.{{{{ARTICLE_PARSER}}}}Signal Processing on Graphs: Causal Modeling of Unstructured Data.{{{{ARTICLE_PARSER}}}}Statistical Modeling and Estimation of Censored Pathloss Data.{{{{ARTICLE_PARSER}}}}$n$-permutability and linear Datalog implies symmetric Datalog.{{{{ARTICLE_PARSER}}}}On repeated zero-sum games with incomplete information and asymptotically bounded values.{{{{ARTICLE_PARSER}}}}Optimal Staged Self-Assembly of General Shapes.{{{{ARTICLE_PARSER}}}}Build your own clarithmetic I: Setup and completeness.{{{{ARTICLE_PARSER}}}}ExtraPush for Convex Smooth Decentralized Optimization over Directed Networks.{{{{ARTICLE_PARSER}}}}Seeding K-Means using Method of Moments.{{{{ARTICLE_PARSER}}}}On bounding the difference between the maximum degree and the chromatic number by a constant.{{{{ARTICLE_PARSER}}}}Reaping the Benefits of Bundling under High Production Costs.{{{{ARTICLE_PARSER}}}}Deep Relative Attributes.{{{{ARTICLE_PARSER}}}}A Submodule Clustering Method for Multi-way Data by Sparse and Low-Rank Representation.{{{{ARTICLE_PARSER}}}}Nested Mini-Batch K-Means.{{{{ARTICLE_PARSER}}}}Extension complexity of polytopes with few vertices or facets.{{{{ARTICLE_PARSER}}}}Training with Exploration Improves a Greedy Stack-LSTM Parser.{{{{ARTICLE_PARSER}}}}Challenges in Bayesian Adaptive Data Analysis.{{{{ARTICLE_PARSER}}}}Construction of de Bruijn Sequences from Product of Two Irreducible Polynomials.{{{{ARTICLE_PARSER}}}}Non-convexity of private capacity and classical environment-assisted capacity of a quantum channel.{{{{ARTICLE_PARSER}}}}View Synthesis by Appearance Flow.{{{{ARTICLE_PARSER}}}}A Multi-perspective Analysis of Carrier-Grade NAT Deployment.{{{{ARTICLE_PARSER}}}}Boda-RTC: Productive Generation of Portable, Efficient Code for Convolutional Neural Networks on Mobile Computing Platforms.{{{{ARTICLE_PARSER}}}}A Tale of Two Bases: Local-Nonlocal Regularization on Image Patches with Convolution Framelets.{{{{ARTICLE_PARSER}}}}Image Caption Generation with Text-Conditional Semantic Attention.{{{{ARTICLE_PARSER}}}}Compressing and Indexing Stock Market Data.{{{{ARTICLE_PARSER}}}}Neural Autoregressive Collaborative Filtering for Implicit Feedback.{{{{ARTICLE_PARSER}}}}Spatio-Temporal Network Dynamics Framework for Energy-Efficient Ultra-Dense Cellular Networks.{{{{ARTICLE_PARSER}}}}Optimal Energy Allocation For Delay-Constrained Traffic Over Fading Multiple Access Channels.{{{{ARTICLE_PARSER}}}}Learning to Sketch Human Facial Portraits using Personal Styles by Case-Based Reasoning.{{{{ARTICLE_PARSER}}}}How Much Do Downlink Pilots Improve Cell-Free Massive MIMO?.{{{{ARTICLE_PARSER}}}}Reconstruction Algorithms for Sums of Affine Powers.{{{{ARTICLE_PARSER}}}}An Evolutionary Algorithm to Learn SPARQL Queries for Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia.{{{{ARTICLE_PARSER}}}}Diagnostic Prediction Using Discomfort Drawings with IBTM.{{{{ARTICLE_PARSER}}}}Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation Algorithm.{{{{ARTICLE_PARSER}}}}Holistic Small Cell Traffic Balancing across Licensed and Unlicensed Bands.{{{{ARTICLE_PARSER}}}}On the Performance of Cell-Free Massive MIMO with Short-Term Power Constraints.{{{{ARTICLE_PARSER}}}}A Non-convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing.{{{{ARTICLE_PARSER}}}}Communication complexity of approximate Nash equilibria.{{{{ARTICLE_PARSER}}}}The Incentive Ratio in Exchange Economies.{{{{ARTICLE_PARSER}}}}A Flexible Recommendation System for Cable TV.{{{{ARTICLE_PARSER}}}}A Large-Scale Characterization of User Behaviour in Cable TV.{{{{ARTICLE_PARSER}}}}Why is Differential Evolution Better than Grid Search for Tuning Defect Predictors?.{{{{ARTICLE_PARSER}}}}Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks.{{{{ARTICLE_PARSER}}}}Error-tolerant Exemplar Queries on RDF Graphs.{{{{ARTICLE_PARSER}}}}When categorization-based stranger avoidance explains the uncanny valley: A comment on MacDorman & Chattopadhyay (2016).{{{{ARTICLE_PARSER}}}}Wav2Letter: an End-to-End ConvNet-based Speech Recognition System.{{{{ARTICLE_PARSER}}}}SecComp: Towards Practically Defending Against Component Hijacking in Android Applications.{{{{ARTICLE_PARSER}}}}Finite-sample and asymptotic analysis of generalization ability with an application to penalized regression.{{{{ARTICLE_PARSER}}}}Acoustic intensity, energy-density and diffuseness estimation in a directionally-constrained region.{{{{ARTICLE_PARSER}}}}ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data.{{{{ARTICLE_PARSER}}}}The Founded Semantics and Constraint Semantics of Logic Rules.{{{{ARTICLE_PARSER}}}}Scientific Computing Using Consumer Video-Gaming Hardware Devices.{{{{ARTICLE_PARSER}}}}Deep Learning a Grasp Function for Grasping under Gripper Pose Uncertainty.{{{{ARTICLE_PARSER}}}}</div>
<div style="visibility:hidden; display: none;" id="abstracts"><p>Turbulence is a key ingredient for the evolution of the intracluster medium,
whose properties can be predicted with high resolution numerical simulations.
We present initial results on the generation of solenoidal and compressive
turbulence in the intracluster medium during the formation of a small-size
cluster using highly resolved, non-radiative cosmological simulations, with a
refined monitoring in time. In this first of a series of papers, we closely
look at one simulated cluster whose formation was distinguished by a merger
around $z \sim 0.3$. We separate laminar gas motions, turbulence and shocks
with dedicated filtering strategies and distinguish the solenoidal and
compressive components of the gas flows using Hodge-Helmholtz decomposition.
Solenoidal turbulence dominates the dissipation of turbulent motions ($\sim
95\%$) in the central cluster volume at all epochs. The dissipation via
compressive modes is found to be more important ($\sim 30 \%$ of the total)
only at large radii ($\geq 0.5 ~r_{\rm vir}$) and close to merger events. We
show that enstrophy (vorticity squared) is good proxy of solenoidal turbulence.
All terms ruling the evolution of enstrophy (i.e. baroclinic, compressive,
stretching and advective terms) are found to be significant, but in amounts
that vary with time and location. Two important trends for the growth of
enstrophy in our simulation are identified: first, enstrophy is continuously
accreted into the cluster from the outside, and most of that accreted enstrophy
is generated near the outer accretion shocks by baroclinic and compressive
processes. Second, in the cluster interior vortex stretching is dominant,
although the other terms also contribute substantially.
</p>
{{{{ARTICLE_PARSER}}}}<p>We here present the first spatially-resolved study of the IMF in external
galaxies derived using a dynamical tracer of the mass-to-light ratio. We use
the kinematics of relaxed molecular gas discs in seven early-type galaxies
(ETGs) selected from the ATLAS3D survey to dynamically determine mass-to-light
ratio (M/L) gradients. These M/L gradients are not very strong in the inner
parts of these objects, and galaxies that do show variations are those with the
highest specific star formation rates. Stellar population parameters derived
from star formation histories are then used in order to estimate the stellar
initial mass function function (IMF) mismatch parameter, and shed light on its
variation within ETGs. Some of our target objects require a light IMF,
otherwise their stellar population masses would be greater than their dynamical
masses. In contrast, other systems seem to require heavier IMFs to explain
their gas kinematics. Our analysis again confirms that IMF variation seems to
be occurring within massive ETGs. We find good agreement between our IMF
normalisations derived using molecular gas kinematics and those derived using
other techniques. Despite this, we do not see find any correlation between the
IMF normalisation and galaxy dynamical properties or stellar population
parameters, either locally or globally. In the future larger studies which use
molecules as tracers of galaxy dynamics can be used to help us disentangle the
root cause of IMF variation.
</p>
{{{{ARTICLE_PARSER}}}}<p>Observational constraints on the birth and early evolution of massive black
holes (BHs) come from two extreme regimes. At high redshift, quasars signal the
rapid growth of billion-solar-mass BHs and indicate that these objects began
remarkably heavy and/or accreted mass at rates above the Eddington limit. At
low redshift, the smallest nuclear BHs known are found in dwarf galaxies and
provide the most concrete limits on the mass of BH seeds. Here we review
current observational work in these fields that together are critical for our
understanding of the origin of massive BHs in the Universe.
</p>
{{{{ARTICLE_PARSER}}}}<p>The detection of gravitational waves from the merger of binary black holes by
the LIGO Collaboration has opened a new window to astrophysics. With the
sensitivities of ground based detectors in the coming years we can only detect
the local black hole binary mergers. The integrated merger rate can instead be
probed by the gravitational-wave background, the incoherent superposition of
the released energy in gravitational waves during binary-black-hole
coalescence. Through that, the properties of the binary black holes can be
studied. In this work we show that by measuring the energy density
$\Omega_{GW}$ (in units of the cosmic critical density) of the
gravitational-wave background, we can search for the rare $\sim 100 M_{\odot}$
massive black holes formed in the Universe. In addition, we can answer how
often the least massive BHs of mass $&gt; 3 M_{\odot}$ form. Finally, if there are
multiple channels for the formation of binary black holes and if any of them
predicts a narrow mass range for the black holes, then the
gravitational-wave-background spectrum may have features that with the future
Einstein Telescope can be detected.
</p>
{{{{ARTICLE_PARSER}}}}<p>We propose a new method to probe the Warm Hot Intergalactic Medium (WHIM)
beyond the virial radius (R_200) of a cluster of galaxies, where X-ray
observations are not easily achievable. In this method, we use dispersion
measures (DMs) of Fast Radio Bursts (FRBs) that appear behind the cluster and
the Sunyaev-Zel'dovich (SZ) effect towards the cluster. The DMs reflect the
density of the intracluster medium (ICM) including the WHIM. If we observe a
sufficient number of FRBs in the direction of the cluster, we can derive the
density profile from the DMs. Similarly, we can derive the pressure profile
from the SZ effect. By combining the density and the pressure profiles, the
temperature profile can be obtained. Based on mock observations of nearby
clusters, we find that the density of the WHIM can be determined even at &gt; 2
R_200} from the cluster center when FRB observations with the Square Kilometre
Array (SKA) become available. The temperature can be derived out to r~ 1.5
R_200, and the radius is limited by the current sensitivity of SZ observations.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present the first results of the High cadence Transient Survey (HiTS), a
survey whose objective is to detect and follow up optical transients with
characteristic timescales from hours to days, especially the earliest hours of
supernova (SN) explosions. HiTS uses the Dark Energy Camera (DECam) and a
custom made pipeline for image subtraction, candidate filtering and candidate
visualization, which runs in real-time to be able to react rapidly to the new
transients. We discuss the survey design, the technical challenges associated
with the real-time analysis of these large volumes of data and our first
results. In our 2013, 2014 and 2015 campaigns we have detected more than 120
young SN candidates, but we did not find a clear signature from the short-lived
SN shock breakouts (SBOs) originating after the core collapse of red supergiant
stars, which was the initial science aim of this survey. Using the empirical
distribution of limiting-magnitudes from our observational campaigns we
measured the expected recovery fraction of randomly injected SN light curves
which included SBO optical peaks produced with models from Tominaga et al.
(2011) and Nakar &amp; Sari (2010). From this analysis we cannot rule out the
models from Tominaga et al. (2011) under any reasonable distributions of
progenitor masses, but we can marginally rule out the brighter and longer-lived
SBO models from Nakar &amp; Sari (2010) under our best-guess distribution of
progenitor masses. Finally, we highlight the implications of this work for
future massive datasets produced by astronomical observatories such as LSST.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present an improved photometric redshift estimator code, CuBAN$z$, that is
publicly available at https://goo.gl/fpk90V}{https://goo.gl/fpk90V. It uses the
back propagation neural network along with clustering of the training set,
which makes it more efficient than existing neural network codes. In CuBAN$z$,
the training set is divided into several self learning clusters with galaxies
having similar photometric properties and spectroscopic redshifts within a
given span. The clustering algorithm uses the color information (i.e. $u-g$,
$g-r$ etc.) rather than the apparent magnitudes at various photometric bands as
the photometric redshift is more sensitive to the flux differences between
different bands rather than the actual values. Separate neural networks are
trained for each cluster using all possible colors, magnitudes and
uncertainties in the measurements. For a galaxy with unknown redshift, we
identify the closest possible clusters having similar photometric properties
and use those clusters to get the photometric redshifts using the particular
networks that were trained using those cluster members. For galaxies that do
not match with any training cluster, the photometric redshifts are obtained
from a separate network that uses entire training set. This clustering method
enables us to determine the redshifts more accurately. SDSS Stripe 82 catalog
has been used here for the demonstration of the code. For the clustered sources
with redshift range $z_{\rm spec}&lt;0.7$, the residual error ($\langle (z_{{\rm
spec}}-z_{{\rm phot}})^2\rangle^{1/2} $) in the training/testing phase is as
low as 0.03 compared to the existing ANNz code that provides residual error on
the same test data set of 0.05. Further, we provide a much better estimate of
the uncertainty of the derived photometric redshift.
</p>
{{{{ARTICLE_PARSER}}}}<p>The standard paradigm of collisionless cold dark matter is in tension with
measurements on large scales. In particular, the best fit values of the Hubble
rate $H_0$ and the matter density perturbation $\sigma_8$ inferred from the
cosmic microwave background seem inconsistent with the results from direct
measurements. We show that both problems can be solved in a framework in which
dark matter consists of two distinct components, a dominant component and a
subdominant component. The primary component is cold and collisionless. The
secondary component is also cold, but interacts strongly with dark radiation,
which itself forms a tightly coupled fluid. The growth of density perturbations
in the subdominant component is inhibited by dark acoustic oscillations due to
its coupling to the dark radiation, solving the $\sigma_8$ problem, while the
presence of tightly coupled dark radiation ameliorates the $H_0$ problem. The
subdominant component of dark matter and dark radiation continue to remain in
thermal equilibrium until late times, inhibiting the formation of a dark disk.
We present an example of a simple model that naturally realizes this scenario
in which both constituents of dark matter are thermal WIMPs. Our scenario can
be tested by future stage-IV experiments designed to probe the CMB and large
scale structure.
</p>
{{{{ARTICLE_PARSER}}}}<p>When modelling inflaton fluctuations as a free quantum scalar field, the
initial vacuum is conventionally imposed at the infinite past. This is called
the Bunch-Davies (BD) vacuum. If however an asymptotically Minkowskian past
does not exist, this becomes inconsistent. We derive corrections to the scalar
spectral index $n_s$ and the tensor tilt $n_t$ descending from arbitrary mixed
states or from explicit non-BD initial conditions. The former may stem from
some pre-inflationary background and can redshift away whereas the latter are
induced by a timelike hypersurface parametrising a physical cut-off. In both
cases, we find that corrections scale in parts or fully as $\mathcal
O(\epsilon)$ where $\epsilon$ is the first slow-roll parameter. The precise
observational footprint is hence dependent on the model driving inflation.
Further, we show how the inflationary consistency relation is altered. We thus
provide an analytic handle on possible high scale or pre-inflationary physics.
</p>
{{{{ARTICLE_PARSER}}}}<p>Understanding galaxy formation and evolution requires studying the interplay
between the growth of galaxies and the growth of their black holes across
cosmic time. Here we explore a sample of Ha-selected star-forming galaxies from
the HiZELS survey and use the wealth of multi-wavelength data in the COSMOS
field (X-rays, far-infrared and radio) to study the relative growth rates
between typical galaxies and their central supermassive black holes, from
z=2.23 to z=0. Typical star-forming galaxies at z~1-2 have black hole accretion
rates (BHARs) of 0.001-0.01 Msun/yr and star formation rates (SFRs) of ~10-40
Msun/yr, and thus grow their stellar mass much quicker than their black hole
mass (~3.3 orders of magnitude faster). However, ~3% of the sample (the sources
detected directly in the X-rays) show a significantly quicker growth of the
black hole mass (up to 1.5 orders of magnitude quicker growth than the typical
sources). BHARs fall from z=2.23 to z=0, with the decline resembling that of
star formation rate density or the typical SFR. We find that the average black
hole to galaxy growth (BHAR/SFR) is approximately constant for star-forming
galaxies in the last 11 Gyrs. The relatively constant BHAR/SFR suggests that
these two quantities evolve equivalently through cosmic time and with
practically no delay between the two.
</p>
{{{{ARTICLE_PARSER}}}}<p>Although there has been much progress in understanding how galaxies evolve,
we still do not understand how and when they stop forming stars and become
quiescent. We address this by applying our galaxy spectral energy distribution
models, which incorporate physically motivated star formation histories (SFHs)
from cosmological simulations, to a sample of quiescent galaxies at
$0.2&lt;z&lt;2.1$. A total of 845 quiescent galaxies with multi-band photometry
spanning rest-frame ultraviolet through near-infrared wavelengths are selected
from the CANDELS dataset. We compute median SFHs of these galaxies in bins of
stellar mass and redshift. At all redshifts and stellar masses, the median SFHs
rise, reach a peak, and then decline to reach quiescence. At high redshift, we
find that the rise and decline are fast, as expected because the Universe is
young. At low redshift, the duration of these phases depends strongly on
stellar mass. Low-mass galaxies ($\log(M_{\ast}/M_{\odot})\sim9.5$) grow on
average slowly, take a long time to reach their peak of star formation
($\gtrsim 4$ Gyr), and the declining phase is fast ($\lesssim 2$ Gyr).
Conversely, high-mass galaxies ($\log(M_{\ast}/M_{\odot})\sim11$) grow on
average fast ($\lesssim 2$ Gyr), and, after reaching their peak, decrease the
star formation slowly ($\gtrsim 3$ Gyr). These findings are consistent with
galaxy stellar mass being a driving factor in determining how evolved galaxies
are, with high-mass galaxies being the most evolved at any time (i.e.,
downsizing). The different durations we observe in the declining phases also
suggest that low- and high-mass galaxies experience different quenching
mechanisms that operate on different timescales.
</p>
{{{{ARTICLE_PARSER}}}}<p>Gaia's Radial Velocity Spectrometer (RVS) has been operating in routine phase
for over one year since initial commissioning. RVS continues to work well but
the higher than expected levels of straylight reduce the limiting magnitude.
The end-of-mission radial-velocity (RV) performance requirement for G2V stars
was 15 km/s at V = 16.5 mag. Instead, 15 km/s precision is achieved at 15 &lt; V &lt;
16 mag, consistent with simulations that predict a loss of 1.4 mag. Simulations
also suggest that changes to Gaia's onboard software could recover ~0.14 mag of
this loss. Consequently Gaia's onboard software was upgraded in April 2015. The
status of this new commissioning period is presented, as well as the latest
scientific performance of the on-ground processing of RVS spectra. We
illustrate the implications of the RVS limiting magnitude on Gaia's view of the
Milky Way's halo in 6D using the Gaia Universe Model Snapshot (GUMS).
</p>
{{{{ARTICLE_PARSER}}}}<p>NoAM for \"No Action Method\" is a framework for reconstructing the past orbits
of observed tracers of the large scale mass density field. It seeks exact
solutions of the equations of motion (EoM), satisfying initial homogeneity and
the final observed particle (tracer) positions. The solutions are found
iteratively reaching a specified tolerance defined as the RMS of the distance
between reconstructed and observed positions. Starting from a guess for the
initial conditions, NoAM advances particles using standard N-body techniques
for solving the EoM. Alternatively, the EoM can be replaced by any
approximation such as Zel'dovich and second order perturbation theory (2LPT).
NoAM is suitable for billions of particles and can easily handle non-regular
volumes, redshift space, and other constraints. We implement NoAM to
systematically compare Zel'dovich, 2LPT, and N-body dynamics over diverse
configurations ranging from idealized high-res periodic simulation box to
realistic galaxy mocks. Our findings are (i) Non-linear reconstructions with
Zel'dovich, 2LPT, and full dynamics perform better than linear theory only for
idealized catalogs in real space. For realistic catalogs, linear theory is the
optimal choice for reconstructing velocity fields smoothed on scales &gt; 5 Mpc/h.
(ii) all non-linear back-in-time reconstructions tested here, produce
comparable enhancement of the baryonic oscillation signal in the correlation
function.
</p>
{{{{ARTICLE_PARSER}}}}<p>With the steadily improving sensitivity afforded by current and future galaxy
surveys, a robust extraction of two-point correlation function measurements may
become increasingly hampered by the presence of astrophysical foregrounds or
observational systematics. The concept of mode projection has been introduced
as a means to remove contaminants for which it is possible to construct a
spatial map reflecting the expected signal contribution. Owing to its
computational efficiency compared to minimum-variance methods, the sub-optimal
pseudo-Cl (PCL) power spectrum estimator is a popular tool for the analysis of
high-resolution data sets. Here, we integrate mode projection into the
framework of PCL power spectrum estimation. In contrast to results obtained
with optimal estimators, we show that the uncorrected projection of template
maps leads to biased power spectra. Based on analytical calculations, we find
exact closed-form expressions for the expectation value of the bias and
demonstrate that they can be recast in a form that allows a numerically
efficient evaluation, preserving the favorable O(l_max^3) time complexity of
PCL estimator algorithms. Using simulated data sets, we assess the scaling of
the bias with various analysis parameters and demonstrate that it can be
reliably removed. We conclude that in combination with mode projection, PCL
estimators allow for a fast and robust computation of power spectra in the
presence of systematic effects -- properties in high demand for the analysis of
ongoing and future large scale structure surveys.
</p>
{{{{ARTICLE_PARSER}}}}<p>We find that $\sim 15-20$ per cent of A-type stars or red giants are bound
with a massive companion ($M_{\rm secondary} &gt; 1M_{\odot}$) in an intermediate
wide orbit ($0.5&lt;P&lt;5000\mbox{ yr}$). These massive binaries are expected to
form wide-orbit, double-degenerate systems (WODDs) within $\lesssim10\mbox{
Gyr}$ implying that $\sim10$ per cent of white dwarfs (WDs) are expected to be
part of a WODD with a lighter WD companion. These findings are based on an
analysis of previous adaptive optics observations of A-type stars and radial
velocity measurements of red giants and shed light on the claimed discrepancy
between the seemingly high multiplicity function of stars and the rather low
number of detected double degenerates. We expect that GAIA will find $\sim 10$
new WODDs within $20\mbox{ pc}$ from the sun. These results put a stringent
constraint on the collision model of type Ia supernovae in which triple stellar
systems that include a WODD as the inner binary are required to be abundant.
</p>
{{{{ARTICLE_PARSER}}}}<p>The spatial distribution of the metals residing in the intra-cluster medium
(ICM) of galaxy clusters records all the information on a cluster's
nucleosynthesis and chemical enrichment history. We present measurements from
deep Suzaku and Chandra observations of the cool-core galaxy cluster Abell 3112
out its virial radius (~1470 kpc). We find that the ratio of the observed
supernova type Ia explosions to the total supernova explosions have a uniform
distribution at a level of 12-16% out to the cluster's virial radius. The
non-varying supernova enrichment suggests that the ICM was enriched by metals
at an early stage before the cluster itself was formed. We also find that the
2D delayed detonations models CDDT produce significantly worse fits to the
X-ray spectra compared to simple 1D W7 models. This may indicate that CDDT
explosions are not a dominant process of enriching the ICM.
</p>
{{{{ARTICLE_PARSER}}}}<p>Utilizing high-resolution cosmological hydrodynamic simulations we
investigate various ultra-violet absorption lines in the circumgalactic medium
of star forming galaxies at low redshift, in hopes of checking and alleviating
the claimed observational conundrum of the ratio of NV to OVI absorbers, among
others. We find a satisfactory agreement between simulations and extant
observational data with respect to the ratios of the following four line pairs
examined, NV/OVI, SiIV/OVI, NIII/OVI and NII/OVI. For the pairs involving
nitrogen lines, we examine two cases of nitrogen abundance, one with constant
N/O ratio and the other with varying N/O ratio, with the latter motivated by
theoretical considerations of two different synthetic sources of nitrogen that
is empirically verified independently. Along a separate vector, for all line
pairs, we examine two cases of radiation field, one with the Haardt-Madau
background radiation field and the other with an additional local radiation
field sourced by hot gas in the host galaxy. In all cases, two-sample
Kolmogorov-Smirnov tests indicate excellent agreements. We find that the
apparent agreements between simulations and observations will be strongly
tested, if the bulk of current upper limits of various line ratios are turned
into actual detections. We show that an increase in observational sensitivity
by 0.2 dex will already start to significantly constrain the models.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present multi-telescope, ground-based, multiwavelength optical and
near-infrared photometry of the variable L3.5 ultra-cool dwarf 2MASSW
J0036159+182110. We present 22 nights of photometry of 2MASSW J0036159+182110,
including 7 nights of simultaneous, multiwavelength photometry, spread over
~120 days allowing us to determine the rotation period of this ultra-cool dwarf
to be 3.080 +/- 0.001 hr. Our many nights of multiwavelength photometry allow
us to observe the evolution, or more specifically the lack thereof, of the
light curve over a great many rotation periods. The lack of discernible phase
shifts in our multiwavelength photometry, and that the amplitude of variability
generally decreases as one moves to longer wavelengths for 2MASSW
J0036159+182110, is generally consistent with starspots driving the variability
on this ultra-cool dwarf, with starspots that are ~100 degrees K hotter or
cooler than the ~1700 K photosphere. Also, reasonably thick clouds are required
to fit the spectra of 2MASSW J0036159+182110, suggesting there likely exists
some complex interplay between the starspots driving the variability of this
ultra-cool dwarf and the clouds that appear to envelope this ultra-cool dwarf.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present 17 nights of ground-based, near-infrared photometry of the
variable L/T transition brown dwarf SIMP J013656.5+093347 and an additional 3
nights of ground-based photometry of the radio-active late M-dwarf TVLM
513-46546. Our TVLM 513-46546 photometry includes 2 nights of simultaneous,
multiwavelength, ground-based photometry, in which we detect obvious J-band
variability, but do not detect I-band variability of similar amplitude,
confirming that the variability of TVLM 513-46546 most likely arises from
clouds or aurorae, rather than starspots. Our photometry of SIMP
J013656.5+093347 includes 15 nights of J-band photometry that allow us to
observe how the variable light curve of this L/T transition brown dwarf evolves
from rotation period to rotation period, night-to-night and week-to-week. We
estimate the rotation period of SIMP J013656.5+093347 as 2.406 +/- 0.008 hours,
and do not find evidence for obvious differential rotation. The peak-to-peak
amplitude displayed by SIMP J013656.5+093347 in our light curves evolves from
greater than 6% to less than 1% in a matter of days, and the typical timescale
for significant evolution of the SIMP J013656.5+093347 light curve appears to
be approximately &lt;1 to 10 rotation periods. This suggests that those performing
spectrophotometric observations of brown dwarfs should be cautious in their
interpretations comparing the spectra between a variable brown dwarf's maximum
flux and minimum flux from observations lasting only approximately a rotation
period, as these comparisons may depict the spectral characteristics of a
single, ephemeral snapshot, rather than the full range of characteristics.
</p>
{{{{ARTICLE_PARSER}}}}<p>We report the detection of stellar eclipses in the LP 661-13 system. We
present the discovery and characterization of this system, including high
resolution spectroscopic radial velocities and a photometric solution spanning
two observing seasons. LP 661-13 is a low mass binary system with an orbital
period of $4.7043512^{+0.0000013}_{-0.0000010}$ days at a distance of $24.9 \pm
1.3$ parsecs. LP 661-13A is a $0.30795 \pm 0.00084$ $M_\odot$ star while LP
661-13B is a $0.19400 \pm 0.00034$ $M_\odot$ star. The radius of each component
is $0.3226 \pm 0.0033$ $R_\odot$ and $0.2174 \pm 0.0023$ $R_\odot$,
respectively. We detect out of eclipse modulations at a period slightly shorter
than the orbital period, implying that at least one of the components is not
rotating synchronously. We find that each component is slightly inflated
compared to stellar models, and that this cannot be reconciled through age or
metallicity effects. As a nearby eclipsing binary system where both components
are near or below the full-convection limit, LP 661-13 will be a valuable test
of models for the structure of cool dwarf stars.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study the atomic physics and the astrophysical implications of a model in
which the dark matter is the analog of hydrogen in a secluded sector. The self
interactions between dark matter particles include both elastic scatterings as
well as inelastic processes due to a hyperfine transition. The self-interaction
cross sections are computed by numerically solving the coupled Schr\\"{o}dinger
equations for this system. We show that these self interactions exhibit the
right velocity dependence to explain the low dark matter density cores seen in
small galaxies while being consistent with all constraints from observations of
clusters of galaxies. For a viable solution, the dark hydrogen mass has to be
in 10--100 GeV range and the dark fine-structure constant has to be larger than
0.02. Precisely for this range of parameters, we show that significant cooling
losses may occur due to inelastic excitations to the hyperfine state and
subsequent decays, with implications for the evolution of low-mass halos and
the early growth of supermassive black holes. Cooling from excitations to
higher $n$ levels of dark hydrogen and subsequent decays is possible at the
cluster scale, with a strong dependence on halo mass. Finally, we show that the
minimum halo mass is in the range of $10^{3.5}$ to $10^7 M_\odot$ for the
viable regions of parameter space, significantly larger than the typical
predictions for weakly-interacting dark matter models. This pattern of
observables in cosmological structure formation is unique to this model, making
it possible to rule in or rule out hidden sector hydrogen as a viable dark
matter model.
</p>
{{{{ARTICLE_PARSER}}}}<p>Galaxy mergers are important events that can determine the fate of a galaxy
by changing its morphology, star-formation activity and mass growth. Merger
systems have commonly been identified from their disturbed morphologies, and we
now can employ Integral Field Spectroscopy to detect and analyze the impact of
mergers on stellar kinematics as well. We visually classified galaxy morphology
using deep images ($\mu_{\rm r} = 28\,\rm mag\,\, arcsec^{-2}$) taken by the
Blanco 4-m telescope at the Cerro Tololo Inter-American Observatory. In this
paper we investigate 63 bright ($M_{\rm r}&lt;-19.3$) spectroscopically-selected
galaxies in Abell 119; of which 53 are early type and 20 galaxies show a
disturbed morphology by visual inspection. A misalignment between the major
axes in the photometric image and the kinematic map is conspicuous in
morphologically-disturbed galaxies. Our sample is dominated by early-type
galaxies, yet it shows a surprisingly tight Tully-Fisher relation except for
the morphologically-disturbed galaxies which show large deviations. Three out
of the eight slow rotators in our sample are morphology disturbed. The
visually-selected morphologically-disturbed galaxies are generally more
asymmetric, visually as well as kinematically. Our findings suggest that galaxy
interactions, including mergers and perhaps fly-bys, play an important role in
determining the orientation and magnitude of galaxy's angular momentum.
</p>
{{{{ARTICLE_PARSER}}}}<p>The substructures of light bosonic (axion-like) dark matter may condense into
compact Bose stars. We study collapses of the critical-mass stars caused by
attractive self-interaction of the axion-like particles and find that these
processes proceed in an unexpected universal way. First, nonlinear self-similar
evolution (similar to \"wave collapse\" in plasma physics) forces the particles
to fall into the star center. Second, collisions in the dense center create an
outgoing stream of mildly relativistic particles which carries away an
essential part of the star mass. The collapse stops when the star remnant is no
longer able to support the self-similar infall feeding the collisions. We
shortly discuss possible astrophysical and cosmological implications of these
phenomena.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper formulates, via the Mathisson - Papapetrou - Dixon equations, the
system of equations for a test particle with spin when it is orbiting a weak
Kerr metric. We shall restrict ourselves to the case of circular orbits with
the purpose of comparing our results with the results of the literature. In
particular, we solve the set of equations of motion for the case of circular
trajectories both spinless and spinning test particles around rotating bodies
in equatorial plane. The results obtained are an important guideline for the
study of the effects of the particles with spin in rotating gravitational
fields such as Gravitomagnetics Effects or gravitational waves.
</p>
{{{{ARTICLE_PARSER}}}}<p>Natures of progenitors of type Ia Supernovae (SNe Ia) have not yet been
clarified. There has been long and intensive discussion on whether the
so-called single degenerate (SD) scenario or the double degenerate (DD)
scenario, or anything else, could explain a major population of SNe Ia, but the
conclusion has not yet been reached. With rapidly increasing observational data
and new theoretical ideas, the field of studying the SN Ia progenitors has been
quickly developing, and various new insights have been obtained in recent
years. This article aims at providing a summary of the current situation
regarding the SN Ia progenitors, both in theory and observations. It seems
difficult to explain the emerging diversity seen in observations of SNe Ia by a
single population, and we emphasize that it is important to clarify links
between different progenitor scenarios and different sub-classes of SNe Ia.
</p>
{{{{ARTICLE_PARSER}}}}<p>One of the primary gravitational wave (GW) sources for pulsar timing arrays
(PTAs) is the stochastic background formed by supermassive black holes binaries
(SMBHBs). In this paper, we investigate how the environments of SMBHBs will
effect the sensitivity of PTAs by deriving scaling laws for the signal-to-noise
ratio (SNR) of the optimal cross-correlation statistic. The presence of gas and
stars around SMBHBs will accelerate the merger at large distances, depleting
the GW stochastic background at low frequencies. We show that environmental
interactions may delay detection by a few years or more, depending on the PTA
configuration and the frequency at which the dynamical evolution transitions
from being dominated by environmental effects to GW-dominated.
</p>
{{{{ARTICLE_PARSER}}}}<p>Radial-velocity (RV) signals induce RV variations an order of magnitude
larger than the signal created by the orbit of Earth-twins, thus preventing
their detection. The goal of this paper is to compare the efficiency of the
different methods used to deal with stellar signals to recover extremely
low-mass planets despite. However, because observed RV variations at the m/s
precision level or below is a combination of signals induced by unresolved
orbiting planets, by the star, and by the instrument, performing such a
comparison using real data is extremely challenging. To circumvent this
problem, we generated simulated RV measurements including realistic stellar and
planetary signals. Different teams analyzed blindly those simulated RV
measurements, using their own method to recover planetary signals despite
stellar RV signals. By comparing the results obtained by the different teams
with the planetary and stellar parameters used to generate the simulated RVs,
it is therefore possible to compare the efficiency of these different methods.
The most efficient methods to recover planetary signals {take into account the
different activity indicators,} use red-noise models to account for stellar RV
signals and a Bayesian framework to provide model comparison in a robust
statistical approach. Using the most efficient methodology, planets can be
found down to K/N= K_pl/RV_rms*sqrt{N_obs}=5 with a threshold of K/N=7.5 at the
level of 80-90% recovery rate found for a number of methods. These recovery
rates drop dramatically for K/N smaller than this threshold. In addition, for
the best teams, no false positives with K/N &gt; 7.5 were detected, while a
non-negligible fraction of them appear for smaller K/N. A limit of K/N = 7.5
seems therefore a safe threshold to attest the veracity of planetary signals
for RV measurements with similar properties to those of the different RV
fitting challenge systems.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a measurement of two-dimensional (2D) redshift-space power
spectrum for the Baryon Oscillation Spectroscopic Survey (BOSS) Data Release 11
CMASS galaxies in the North Galactic Cap (NGC) based on the method developed by
Jing &amp; Borner (2001). In this method, we first measure the 2D redshift-space
correlation function for the CMASS galaxies, and obtain the 2D power spectrum
based on Fourier Transform of the correlation function. The method is tested
with an N-body mock galaxy catalog, which demonstrates that the method can
yield an accurate and unbiased measurement of the redshift-space power spectrum
given the input 2D correlation function is correct. Compared with previous
measurements in literature that are usually based on direct Fourier Transform
in redshift space, our method has the advantages that the window function and
shot-noise are fully corrected. In fact, our 2D power spectrum, by its
construction, can accurately reproduce the 2D correlation function, and in the
meanwhile can reproduce, for example, the 2D power spectrum of Beutler et al.
(2014) accurately if ours is convolved with the window function they provided.
Thus, our measurement can facilitate a direct comparison with the theoretical
predictions. With this accurate measurement of the 2D power spectrum, we then
develop a method to measure the structure growth rate, by separating the
anisotropic redshift-space power spectrum from the isotropic real-space power
spectrum. We have also carefully corrected for the nonlinearities in the
mapping from real space to redshift space, according to the theoretical model
of Zhang et al. (2013). Finally, we obtain f(zeff)sigma_8(zeff)=0.438\pm0.037
at the effective redshift zeff=0.57, where f(zeff) is the linear growth rate at
redshift zeff. The result is useful for constraining cosmological parameters.
The measurements of 2D power spectrum will be released soon.
</p>
{{{{ARTICLE_PARSER}}}}<p>Acoustic waves are those waves which travel with the speed of sound through a
medium. H. Lamb has derived a cutoff frequency for stratified and isothermal
medium for the propagation of acoustic waves. In order to find the cutoff
frequency many methods were introduced after Lamb's work. In this paper, we
have chosen the method to determine cutoff frequencies for acoustic waves
propagating in non-isothermal media. This turning point frequency method can be
applied to various atmospheres like solar atmosphere, stellar atmosphere,
earth's atmosphere etc. Here, we have analytically derived the cutoff frequency
and have graphically analyzed and compared with the Lamb's cut-off frequencyfor
earth's troposphere, lower and upper stratosphere.
</p>
{{{{ARTICLE_PARSER}}}}<p>Aims. Although the temporal evolution of active regions (ARs) is relatively
well understood, the processes involved continue to be the subject of
investigation. We study how the magnetic field of a series of ARs evolves with
time to better characterise how ARs emerge and disperse. Methods. We examine
the temporal variation in the magnetic field distribution of 37 emerging ARs. A
kernel density estimation plot of the field distribution was created on a
log-log scale for each AR at each time step. We found that the central portion
of the distribution is typically linear and its slope was used to characterise
the evolution of the magnetic field. Results. The slopes were seen to evolve
with time, becoming less steep as the fragmented emerging flux coalesces. The
slopes reached a maximum value of ~ -1.5 just before the time of maximum flux
before becoming steeper during the decay phase towards the quiet Sun value of ~
-3. This behaviour differs significantly from a classical diffusion model,
which produces a slope of -1. These results suggest that simple classical
diffusion is not responsible for the observed changes in field distribution,
but that other processes play a significant role in flux dispersion.
Conclusions. We propose that the steep negative slope seen during the late
decay phase is due to magnetic flux reprocessing by (super)granular convective
cells.
</p>
{{{{ARTICLE_PARSER}}}}<p>Cosmic backreaction refers to the general question of whether a homogeneous
and isotropic cosmological model is able to predict the correct expansion
dynamics of our inhomogeneous Universe. One aspect of this issue concerns the
validity of the continuous approximation: does a system of point masses expand
the same way as a fluid does? This article shows that it is not exactly the
case in Newtonian gravity, although the associated corrections vanish in an
infinite Universe. It turns out that Gauss's law is a key ingredient for such
corrections to vanish. Backreaction therefore generically arises in alternative
theories of gravitation, which threatens the trustworthiness of their
cosmological tests. This phenomenon is illustrated with a toy-model of massive
gravity.
</p>
{{{{ARTICLE_PARSER}}}}<p>We performed millimeter observations in CO lines toward the supernova remnant
(SNR) HB 3. Substantial molecular gas around -45 km s^-1 is detected in the
conjunction region between the SNR HB 3 and the nearby W3 complex. This
molecular gas is distributed along the radio continuum shell of the remnant.
Furthermore, the shocked molecular gas indicated by line wing broadening
features is also distributed along the radio shell and inside it. By both
morphological correspondence and dynamical evidence, we confirm that the SNR HB
3 is interacting with the -45 km s^-1 molecular cloud (MC), in essence, with
the nearby H II region/MC complex W3. The red-shifted line wing broadening
features indicate that the remnant is located at the nearside of the MC. With
this association, we could place the remnant at the same distance as the W3/W4
complex, which is 1.95 +- 0.04 kpc. The spatial distribution of aggregated
young stellar object candidates (YSOc) shows a correlation to the shocked
molecular strip associated with the remnant. We also find a binary clump of CO
at (l = 132.94 deg, b = 1.12 deg) around -51.5 km s^-1 inside the projected
extent of the remnant, and it is associated with significant mid-infrared
(mid-IR) emission. The binary system also has a tail structure resembling the
tidal tails of interacting galaxies. According to the analysis of CO emission
lines, the larger clump in this binary system is about stable, and the smaller
clump is significantly disturbed.
</p>
{{{{ARTICLE_PARSER}}}}<p>We build a simple physical model to study the high-redshift active galactic
Nucleus (AGN) evolution within the co-evolution framework of central black
holes (BHs) and their host galaxies. The correlation between the circular
velocity of a dark halo $V_c$ and the velocity dispersion of a galaxy $\sigma$
is used to link the dark matter halo mass and BH mass. The dark matter halo
mass function is converted to the BH mass function for any given redshift. The
high-redshift optical AGN luminosity functions (LFs) are constructed. At $z\sim
4$, the flattening feature is not shown at the faint end of the optical AGN LF.
This is consistent with observational results. If the optical AGN LF at $z\sim
6$ can be reproduced in the case in which central BHs have the
Eddington-limited accretion, it is possible for the AGN lifetime to have a
small value of $2\times 10^5$ yrs. The X-ray AGN LFs and X-ray AGN number
counts are also calculated at $2.0&lt;z&lt;5.0$ and $z&gt;3$, respectively, using the
same parameters adopted in the calculation for the optical AGN LF at $z\sim 4$.
It is estimated that about 30 AGNs per $\rm{deg}^2$ at $z&gt;6$ can be detected
with a flux limit of $3\times 10^{-17}~\rm{erg~cm^{-2}~s^{-1}}$ in the $0.5-2$
keV band. Additionally, the cosmic reionization is also investigated. The
ultraviolet photons emitted from the high-redshift AGNs mainly contribute to
the cosmic reionization, and the central BHs of the high-redshift AGNs have a
mass range of $10^6-10^8M_\odot$. We also discuss some uncertainties in both
the AGN LFs and AGN number counts originating from the $M_{\rm{BH}}-\sigma$
relation, Eddington ratio, AGN lifetime, and X-ray attenuation in our model.
</p>
{{{{ARTICLE_PARSER}}}}<p>The EUSO (Extreme Universe Space Observatory) project is developing a new
mission concept for the scientific research of Ultra High Energy Cosmic Rays
(UHECRs) from space. The EUSO wide-field telescope will look down from space
onto the Earth night sky to detect UV photons emitted from air showers
generated by UHECRs in our atmosphere. In this article we concentrate on the
mitigation strategies agreed so far, and in particular on the implementation of
a careful early selection and testing of subsystem materials (including
optics), design and interfaces of the subsystem and an optimization of the
instrument operational concept.
</p>
{{{{ARTICLE_PARSER}}}}<p>Extending deep observations of the neutral atomic hydrogen (HI) to the
environment around galaxy groups can reveal a complex history of group
interactions which is invisible to studies that focus on the stellar component.
Hickson Compact Group 44 (HCG 44) is a nearby example and we have combined HI
data from the Karoo Array Telescope, Westerbork Synthesis Radio Telescope, and
Arecibo Legacy Fast ALFA survey, in order to achieve high column density
sensitivity (N_HI &lt; 2x10^18 cm^-2) to the neutral gas over a large
field-of-view beyond the compact group itself. We find the giant HI tail north
of HCG 44 contains 1.1x10^9 M_Sun of gas and extends 450 kpc from the compact
group: twice as much mass and 33% further than previously detected. However,
the additional gas is still unable to account for the known HI deficiency of
HCG 44. The tail likely formed through a strong tidal interaction and HI clouds
in the tail have survived for 1 Gyr or more after being stripped. This has
important implications for understanding the survival of neutral clouds in the
intragroup and circumgroup medium, and we discuss their survival in the context
of simulations of cold gas in hot halos. HCG 44 is one of a growing number of
galaxy groups found to have more extended HI in the intragroup and circumgroup
medium than previously measured. Our results provide constraints for
simulations on the properties of galaxy group halos, and reveal a glimpse of
what will be seen by future powerful HI telescopes and surveys.
</p>
{{{{ARTICLE_PARSER}}}}<p>Athena is the large mission selected by ESA in 2013 to investigate the
science theme \"Hot and Energetic Universe\" and presently scheduled for launch
in 2028. One of the two instruments located at the focus of the 12 m-long
Athena telescope is the X-ray Integral Field Unit (X-IFU). This is an array of
TES micro-calorimeters that will be operated at temperatures of 50 mK in order
to perform high resolution spectroscopy with an energy resolution down to 2.5
eV at energies &lt; 7 keV. In order to cope with the large dynamical range of
X-ray fluxes spanned by the celestial objects Athena will be observing, the
X-IFU will be equipped with a filter wheel. This will allow the user to fine
tune the instrument set-up based on the nature of the target, thus optimizing
the scientific outcomes of the observation. A few positions of the filter wheel
will also be used to host a calibration source and to allow the measurement of
the instrument intrinsic background.
</p>
{{{{ARTICLE_PARSER}}}}<p>Soon to be operational HI survey instruments such as APERTIF and ASKAP will
produce large datasets. These surveys will provide information about the HI in
and around hundreds of galaxies with a typical signal-to-noise ratio of $\sim$
10 in the inner regions and $\sim$ 1 in the outer regions. In addition, such
surveys will make it possible to probe faint HI structures, typically located
in the vicinity of galaxies, such as extra-planar-gas, tails and filaments.
These structures are crucial for understanding galaxy evolution, particularly
when they are studied in relation to the local environment. Our aim is to find
optimized kernels for the discovery of faint and morphologically complex HI
structures. Therefore, using HI data from a variety of galaxies, we explore
state-of-the-art filtering algorithms. We show that the intensity-driven
gradient filter, due to its adaptive characteristics, is the optimal choice. In
fact, this filter requires only minimal tuning of the input parameters to
enhance the signal-to-noise ratio of faint components. In addition, it does not
degrade the resolution of the high signal-to-noise component of a source. The
filtering process must be fast and be embedded in an interactive visualization
tool in order to support fast inspection of a large number of sources. To
achieve such interactive exploration, we implemented a multi-core CPU (OpenMP)
and a GPU (OpenGL) version of this filter in a 3D visualization environment
($\tt{SlicerAstro}$).
</p>
{{{{ARTICLE_PARSER}}}}<p>[Abridged] Do some environments favor efficient conversion of molecular gas
into stars? To answer this, we need to be able to estimate the H2 mass.
Traditionally, this is done using CO and a few assumptions but the Herschel
observations in the FIR make it possible to estimate the molecular gas mass
independently of CO. Previous attempts to derive gas masses from dust emission
suffered from biases. Generally, dust surface densities, HI column densities,
and CO intensities are used to derive a gas-to-dust ratio (GDR) and the local
CO intensity to H2 column density ratio (XCO), sometimes allowing for an
additional CO-dark gas component (Kdark). We tested earlier methods, revealing
degeneracies among the parameters, and then used a Bayesian formalism to derive
the most likely values for each of the parameters mentioned above as a function
of position in the nearby low metallicity spiral galaxy M33. The data are from
the IRAM 30m CO(2-1) line, high-resolution HI and Herschel dust continuum
observations. Solving for GDR, XCO, and Kdark in macro pixels 500 pc in size,
we find that (i) allowing for CO-dark gas significantly improves fits; (ii)
Kdark decreases with galactocentric distance; (iii) GDR is slightly higher than
initially expected and increases with galactocentric distance; (iv) the total
amount of dark gas closely follows the radially decreasing CO emission, as
might be expected if the dark gas is H2 where CO is photodissociated. The total
amount of H2, including dark gas, yields an average XCO of twice the galactic
value of 2e20 cm^-2/(K km/s), 55% of this traced directly through CO. The
rather constant fraction of dark gas suggests that there is no large population
of diffuse H2 clouds (unrelated to GMCs) without CO emission. Unlike in large
spirals, we detect no systematic radial trend in XCO, possibly linked to the
absence of a radial decrease in CO line ratios.
</p>
{{{{ARTICLE_PARSER}}}}<p>Supernova Remnant RX J1713.7-3946 emits synchrotron X-rays and very high
energy $\gamma$-rays. Recently, thermal X-ray line emission is detected from
ejecta plasma. CO and HI observations indicate that a highly inhomogeneous
medium surrounding the SNR. It is interacting with dense molecular clouds in
the northwest and the southwest of the remnant. The origin of the $\gamma$-ray
emission from RX J1713.7-3946 is still uncertain. Detection of rapid
variability in X-ray emission from RX J1713.7-3946 indicates the magnetic field
$B$ $\sim$ mG. In this work, we investigate the time variation in X-ray flux,
luminosity and photon index of RX J1713.7-3946. For this investigation, we
study the northwest part of the remnant using Suzaku data in 2006 and 2010. We
present preliminary results based on our analysis and interpretations about
these X-ray time variability.
</p>
{{{{ARTICLE_PARSER}}}}<p>The vertical structure of stationary thin accretion discs is calculated from
the energy balance equation with heat generation due to microscopic ion
viscosity {\eta} and electron heat conductivity {\kappa}, both depending on
temperature. In the optically thin discs it is found that for the heat
conductivity increasing with temperature, the vertical temperature gradient
exceeds the adiabatic value at some height, suggesting convective instability
in the upper disc layer. There is a critical Prandtl number, Pr = 4/9, above
which a Keplerian disc become fully convective. The vertical density
distribution of optically thin laminar accretion discs as found from the
hydrostatic equilibrium equation cannot be generally described by a polytrope
but in the case of constant viscosity and heat conductivity. In the optically
thick discs with radiation heat transfer, the vertical disc structure is found
to be convectively stable for both absorption dominated and scattering
dominated opacities, unless a very steep dependence of the viscosity
coefficient on temperature is assumed. A polytropic-like structure in this case
is found for Thomson scattering dominated opacity.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a new, semi-analytic framework for estimating the level of
residuals present in CMB maps derived from multi-frequency Cosmic Microwave
Background (CMB) data and forecasting their impact on cosmological parameters.
The data are assumed to contain non-negligible signals of astrophysical and/or
Galactic origin, which we clean using parametric component separation
technique. We account for discrepancies between the foreground model assumed
during the separation procedure and the true one, allowing for differences in
scaling laws and/or their spatial variations. Our estimates and their
uncertainties include both systematic and statistical effects and are averaged
over the instrumental noise and CMB signal realizations. The framework can be
further extended to account self-consistently for existing uncertainties in the
foreground models. We demonstrate and validate the framework on simple study
cases which aim at estimating the tensor-to-scalar ratio, r. The proposed
approach is computationally efficient permitting an investigation of hundreds
of set-ups and foreground models on a single CPU.
</p>
{{{{ARTICLE_PARSER}}}}<p>Most models identify the X-ray bright North Polar Spur (NPS) with a hot
interstellar (IS) bubble in the Sco-Cen star-forming region at $\simeq$130 pc.
An opposite view considers the NPS as a distant structure associated with
Galactic nuclear outflows. Constraints on the NPS distance can be obtained by
comparing the foreground IS gas column inferred from X-ray absorption to the
distribution of gas and dust along the line of sight. Absorbing columns towards
shadowing molecular clouds simultaneously constrain the CO-H$_{2}$ conversion
factor. We derived the columns of X-ray absorbing matter NH(abs) from spectral
fitting of dedicated XMM-Newton observations towards the NPS southern terminus
(l=29{\deg}, b=+5 to +11{\deg}). The IS matter distribution was obtained from
absorption lines in stellar spectra, 3D dust maps and emission data, including
high spatial resolution CO measurements recorded for this purpose. NH(abs)
varies from $\simeq$ 4.3 to $\simeq$ 1.3 x 10$^{21}$ cm$^{-2}$ along the 19
fields. Relationships between X-ray brightness, absorbing column and hardness
ratio demonstrate a brightness decrease with latitude governed by increasing
absorption. The comparison with absorption data, local and large-scale dust
maps rules out a NPS near side closer than 300 pc. The correlation between
NH(abs) and the reddening increases with the sightline length from 300 pc to 4
kpc and is the tightest with Planck $\tau_{353}$-based reddening, suggesting a
much larger distance. N(H)/E(B-V) $\simeq$ 4.1 x 10$^{21}$ cm$^{-2}$
mag$^{-1}$. NH(abs) absolute values are compatible with HI-CO clouds at -5
$\leq$ V(LSR) $\leq$ +25 to +45 km s$^{-1}$ and a NPS potentially far beyond
the Local Arm. A molecular cloud shadow at b=+9deg constrains X$_{CO}$ to
$\leq$ 1.0 x 10$^{20}$ cm$^{-2}$ K$^{-1}$ km$^{-1}$ s. The average X$_{CO}$ is
$\leq$ 0.75 x 10$^{20}$ cm$^{-2}$ K$^{-1}$ km$^{-1}$ s.
</p>
{{{{ARTICLE_PARSER}}}}<p>Ellerman Bombs are signatures of magnetic reconnection, which is an important
physical process in the solar atmosphere. How and where they occur is a subject
of debate. In this paper we analyse Sunrise/IMaX data together with 3D MHD
simulations that aim to reproduce the exact scenario proposed for the formation
of these features. Although the observed event seems to be more dynamic and
violent than the simulated one, simulations clearly confirm the basic scenario
for the production of EBs. The simulations also reveal the full complexity of
the underlying process. The simulated observations show that the Fe I 525.02 nm
line gives no information on the height where reconnection takes place. It can
only give clues about the heating in the aftermath of the reconnection. The
information on the magnetic field vector and velocity at this spatial
resolution is, however, extremely valuable because it shows what numerical
models miss and how they can be improved.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a set of 87 RAVE stars with detected solar like oscillations,
observed during Campaign 1 of the K2 mission (RAVE K2-C1 sample). This dataset
provides a useful benchmark for testing the gravities provided in RAVE Data
Release 4 (DR4), and is key for the calibration of the RAVE Data Release 5
(DR5). In the present work, we use two different pipelines, GAUFRE (Valentini
et al. 2013) and Sp_Ace (Boeche et al. 2015), to determine atmospheric
parameters and abundances by fixing log(g) to the seismic one. Our strategy
ensures highly consistent values among all stellar parameters, leading to more
accurate chemical abundances. A comparison of the chemical abundances obtained
here with and without the use of seismic log(g) information has shown that an
underestimated (overestimated) gravity leads to an underestimated
(overestimated) elemental abundance (e.g. [Mg/H] is underestimated by ~0.25 dex
when the gravity is underestimated by 0.5 dex). We then perform a comparison
between the seismic gravities and the spectroscopic gravities presented in the
RAVE DR4 catalogue, extracting a calibration for log(g) of RAVE giants in the
colour interval 0.50&lt;(J - Ks)&lt;0.85. Finally, we show a comparison of the
distances, temperatures, extinctions (and ages) derived here for our RAVE K2-C1
sample with those derived in RAVE DR4 and DR5.DR5 performs better than DR4
thanks to the seismic calibration, although discrepancies can still be
important for objects for which the difference between DR4/DR5 and seismic
gravities differ by more than ~0.5 dex. The method illustrated in this work
will be used for analysing RAVE targets present in the other K2 campaigns, in
the framework of Galactic Archaeology investigations.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a galactic chemical evolution model which adopts updated
prescriptions for all the main processes governing the dust cycle. We follow in
detail the evolution of the abundances of several chemical species (C, O, S,
Si, Fe and Zn) in the gas and dust of a typical dwarf irregular galaxy. The
dwarf irregular galaxy is assumed to evolve with a low but continuous level of
star formation and experience galactic winds triggered by supernova explosions.
We predict the evolution of the gas to dust ratio in such a galaxy and discuss
critically the main processes involving dust, such as dust production by AGB
stars and Type II SNe, destruction and accretion (gas condensation in clouds).
We then apply our model to Damped Lyman-Alpha systems which are believed to be
dwarf irregulars, as witnessed by their abundance patterns. Our main
conclusions are: i) we can reproduce the observed gas to dust ratio in dwarf
galaxies. ii) We find that the process of dust accretion plays a fundamental
role in the evolution of dust and in certain cases it becomes the dominant
process in the dust cycle. On the other hand, dust destruction seems to be a
negligible process in irregulars. iii) Concerning Damped Lyman-Alpha systems,
we show that the observed gas-phase abundances of silicon, normalized to
volatile elements (zinc and sulfur), are in agreement with our model. iv) The
abundances of iron and silicon in DLA systems suggest that the two elements
undergo a different history of dust formation and evolution. Our work casts
light on the nature of iron-rich dust: the observed depletion pattern of iron
is well reproduced only when an additional source of iron dust is considered.
Here we explore the possibility of a contribution from Type Ia SNe as well as
an efficient accretion of iron nano-particles.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present the study of a sample of nine QSO fields, with damped-Ly-alpha
(DLA) or sub-DLA systems at z~0.6, observed with the X-Shooter spectrograph at
the Very Large Telescope. By suitably positioning the X-Shooter slit based on
high spatial resolution images of HST/ACS we are able to detect absorbing
galaxies in 7 out of 9 fields (~ 78\% success rate) at impact parameters from
10 to 30 kpc. In 5 out of 7 fields the absorbing galaxies are confirmed via
detection of multiple emission lines at the redshift of DLAs where only 1 out
of 5 also emits a faint continuum. In 2 out of these 5 fields we detect a
second galaxy at the DLA redshift. Extinction corrected star formation rates
(SFR) of these DLA-galaxies, estimated using their H-alpha fluxes, are in the
range 0.3-6.7 M_sun yr^-1. The emission metallicities of these five
DLA-galaxies are estimated to be from 0.2 to 0.9 Z_sun. Based on the Voigt
profile fits to absorption lines we find the metallicity of the absorbing
neutral gas to be in a range of 0.05--0.6 Z_sun. The two remaining DLA-galaxies
are quiescent galaxies with SFR &lt; 0.4 M_sun yr^-1 (3-sigma) presenting
continuum emission but weak or no emission lines. Using X-Shooter spectrum we
estimate i-band absolute magnitude of -19.5+/-0.2 for both these DLA-galaxies
that indicates they are sub-L* galaxies. Comparing our results with that of
other surveys in the literature we find a possible redshift evolution of the
SFR of DLA-galaxies.
</p>
{{{{ARTICLE_PARSER}}}}<p>V616 Mon = A0620-00 is a prototype of black hole transient X-ray binaries.
Our 2003-16 optical photometry of the object during X-ray quiescence, obtained
by 50-250 cm telescopes in Crimea, Caucasus Mountains and Slovakia, consists of
~ 7660 CCD frames in Johnson-Cousins $V,R,R_C,I$ bands and the integral light.
During 2003, 2008-9 and 2015-16 passive states, the phase light curve of the
binary exhibited mainly variations caused by an ellipsoidal shape of the red
dwarf component. During 2004-6 and 2009-14 active states a significant
aperiodic broad-band variability (flickering) was present, arising in a black
hole accretion disk and a bright spot, where the mass transfer stream hits the
outer edge of the disk. Long term photometry of our minima times, together with
available positions of superior conjunctions of the red dwarf found from
spectroscopy, allowed us to refine the orbital period of V616 Mon to
0.32301407(5) days.
</p>
{{{{ARTICLE_PARSER}}}}<p>The brightest southern quasar above redshift $z=1$, HE 0515$-$4414, with its
strong intervening metal absorption-line system at $z_{abs}=1.1508$, provides a
unique opportunity to precisely measure or limit relative variations in the
fine-structure constant ($\Delta\alpha/\alpha$). A variation of just $\sim$3
parts per million (ppm) would produce detectable velocity shifts between its
many strong metal transitions. Using new and archival observations from the
Ultraviolet and Visual Echelle Spectrograph (UVES) we obtain an extremely high
signal-to-noise ratio spectrum (peaking at S/N $\approx250$ pix$^{-1}$). This
provides the most precise measurement of $\Delta\alpha/\alpha$ from a single
absorption system to date, $\Delta\alpha/\alpha=-1.42\pm0.55_{\rm
stat}\pm0.65_{\rm sys}$ ppm, comparable with the precision from previous, large
samples of $\sim$150 absorbers. The largest systematic error in all (but one)
previous similar measurements, including the large samples, was long-range
distortions in the wavelength calibration. These would add a $\sim$2 ppm
systematic error to our measurement and up to $\sim$10 ppm to other
measurements using Mg and Fe transitions. However, we corrected the UVES
spectra using well-calibrated spectra of the same quasar from the High Accuracy
Radial velocity Planet Searcher (HARPS), leaving a residual 0.59 ppm systematic
uncertainty, the largest contribution to our total systematic error. A similar
approach, using short observations on future, well-calibrated spectrographs to
correct existing, high S/N spectra, would efficiently enable a large sample of
reliable $\Delta\alpha/\alpha$ measurements. The high S/N UVES spectrum also
provides insights into analysis difficulties, detector artifacts and systematic
errors likely to arise from 25-40-m telescopes.
</p>
{{{{ARTICLE_PARSER}}}}<p>We investigate the dynamics of a quantized vortex and a nuclear impurity
immersed in a neutron superfluid within a fully microscopic time-dependent
three-dimensional approach. The magnitude and even the sign of the force
between the quantized vortex and the nuclear impurity have been a matter of
debate for over four decades. We determine that the vortex and the impurity
repel at neutron densities, 0.014 fm$^{-3}$ and 0.031 fm$^{-3}$, which are
relevant to the neutron star crust and the origin of glitches, while previous
calculations have concluded that the force changes its sign between these two
densities and predicted contradictory signs. The magnitude of the force
increases with the density of neutron superfluid, while the magnitude of the
pairing gap decreases in this density range.
</p>
{{{{ARTICLE_PARSER}}}}<p>Supernova Remnants (SNRs) exhibit spectra featured by synchrotron radio
emission arising from the relativistic electrons, and high-energy emission from
both leptonic (Bremsstrahlung and Inverse Compton) and hadronic processes
(${\pi}^0$ mesons decay) which are a direct signature of cosmic rays
acceleration. Thanks to radio single-dish imaging observations obtained in
three frequency bands (1.6, 7, 22 GHz) with the Sardinia Radio Telescope
(www.srt.inaf.it), we can model different SNR regions separately. Indeed, in
order to disentangle interesting and peculiar hadron contributions in the
high-energy spectra (gamma-ray band) and better constrain SNRs as cosmic rays
emitters, it is crucial to fully constrain lepton contributions first through
radio-observed parameters. In particular, the Bremsstrahlung and Inverse
Compton bumps observed in gamma-rays are bounded to synchrotron spectral slope
and cut-off in the radio domain. Since these parameters vary for different SNR
regions and electron populations, spatially-resolved radio spectra are then
required for accurate multi-wavelength modelling.
</p>
{{{{ARTICLE_PARSER}}}}<p>In the frame of the Astronomical Validation activities for the 64m Sardinia
Radio Telescope, we performed 5-22 GHz imaging observations of the
complex-morphology supernova remnants (SNRs) W44 and IC443. We adopted
innovative observing and mapping techniques providing unprecedented accuracy
for single-dish imaging of SNRs at these frequencies, revealing morphological
details typically available only at lower frequencies through interferometry
observations. High-frequency studies of SNRs in the radio range are useful to
better characterize the spatially-resolved spectra and the physical parameters
of different regions of the SNRs interacting with the ISM. Furthermore,
synchrotron-emitting electrons in the high-frequency radio band are also
responsible for the observed high-energy phenomenology as -e.g.- Inverse
Compton and bremsstrahlung emission components observed in gamma-rays, to be
disentangled from hadron emission contribution (providing constraints on the
origin of cosmic rays).
</p>
{{{{ARTICLE_PARSER}}}}<p>In this work I review the observational constraints imposed on the energetics
and magnetisation of quasar jets, in the context of theoretical expectations.
The discussion is focused on issues regarding the jet production efficiency,
matter content, and particle acceleration. I show that if the ratio of
electron-positron-pairs to protons is of order $15$, as is required to achieve
agreement between jet powers computed using blazar spectral fits and those
computed using radio-lobe calorimetry, the magnetization of blazar jets in
flat-spectrum-radio-quasars (FSRQ) must be significant. This result favors the
reconnection mechanism for particle acceleration and explains the large
Compton-dominance of blazar spectra that is often observed, without the need to
postulate very low jet magnetization.
</p>
{{{{ARTICLE_PARSER}}}}<p>We map the Hi distribution of galaxies in a $\sim 1.5^\circ \times 2.5^\circ$
region located at the virial radius south of the Virgo cluster using the
KAT$-$7 and the WSRT interferometers. Because of the different beam sizes of
the two telescopes, a similar column density sensitivity of $\rm N_{Hi} \sim
10^{18}\,cm^{-2}$ was reached with the two observations over 16.5 km/s. We
pioneer a new approach to combine the observations and take advantage of their
sensitivity to both the large and small scale structures. Out to an
unprecedented extent, we detect an Hi tail of $\sim 60$ kpc being stripped off
NGC 4424, a peculiar spiral galaxy. The properties of the galaxy, together with
the shape of the tail, suggest that NGC 4424 is a post-merger galaxy undergoing
a ram pressure stripping as it falls towards the centre of the Virgo Cluster.
We detect a total of 14 galaxies and 3 Hi clouds lacking optical counterparts.
One of the clouds is a new detection with an Hi mass of $\rm 7\times10^7\,
M_\odot$ and a strong Hi profile with $W_{50} = 73$ km/s. We find that 10 out
of the 14 galaxies present Hi deficiencies not higher than those of the
cluster's late spirals, suggesting that the environmental effects are not more
pronounced in the region than elsewhere in the cluster.
</p>
{{{{ARTICLE_PARSER}}}}<p>Diffusive shock acceleration (DSA) at relativistic shocks is widely thought
to be an important acceleration mechanism in various astrophysical jet sources,
including radio-loud active galactic nuclei such as blazars. Such acceleration
can produce the non-thermal particles that emit the broadband continuum
radiation that is detected from extragalactic jets. An important recent
development for blazar science is the ability of Fermi-LAT spectroscopy to pin
down the shape of the distribution of the underlying non-thermal particle
population. This paper highlights how multi-wavelength spectra spanning optical
to X-ray to gamma-ray bands can be used to probe diffusive acceleration in
relativistic, oblique, magnetohydrodynamic (MHD) shocks in blazar jets.
Diagnostics on the MHD turbulence near such shocks are obtained using thermal
and non-thermal particle distributions resulting from detailed Monte Carlo
simulations of DSA. These probes are afforded by the characteristic property
that the synchrotron $\nu F_{\nu}$ peak energy does not appear in the gamma-ray
band above 100 MeV. We investigate self-consistently the radiative synchrotron
and inverse Compton signatures of the simulated particle distributions.
Important constraints on the diffusive mean free paths of electrons, and the
level of electromagnetic field turbulence are identified for three different
case study blazars, Mrk 501, BL Lacertae and AO 0235+164. The X-ray excess of
AO 0235+164 in a flare state can be modelled as the signature of bulk Compton
scattering of external radiation fields, thereby tightly constraining the
energy-dependence of the diffusion coefficient for electrons. The concomitant
interpretations that turbulence levels decline with remoteness from jet shocks,
and the probable significant role for non-gyroresonant diffusion, are posited.
</p>
{{{{ARTICLE_PARSER}}}}<p>Observational evidence suggests that some very large supermassive black holes
(SMBHs) already existed less than 1 Gyr after the Big Bang. Explaining the
formation and growth of the 'seeds' of these SMBHs is quite challenging. We
explore the formation of such seeds in the direct collapse scenario. Using 3D
hydrodynamical simulations, we investigate the impact of turbulence and
rotation on the fragmentation behavior of collapsing primordial gas in the
presence of a strong UV radiation background, which keeps the gas hot.
Additionally, we explore different ways in which the collapsing gas may be able
to stay hot, and thus limit fragmentation. Using a one-zone model, we examine
the interplay between magnetic fields, turbulence, and a UV radiation
background.
</p>
<p>Feedback processes from stars and black holes shape the interstellar medium
(ISM) out of which new generations of luminous objects form. To understand the
properties of these objects, e.g. the stellar initial mass function, it is
vital to have knowledge of the chemical and thermodynamical properties of the
feedback-regulated ISM. To better understand the chemo-thermal state and
fragmentation behavior of gas in high-redshift galaxies, we updated, improved,
and extended a photodissociation region code. Our computational code, PDR-Zz,
is described in detail. Using this code, a grid of models is run, covering a
sizable range in physical properties. This allows us to systematically explore
the overall impact of various feedback effects, both radiative and chemical, on
the chemical and thermal balance of the gas in different physical regimes.
</p>
{{{{ARTICLE_PARSER}}}}<p>The dispersal of the circumstellar discs of dust and gas surrounding young
low- mass stars has important implications for the formation of planetary
systems. Photo- evaporation from energetic radiation from the central object is
thought to drive the dispersal in the majority of discs, by creating a gap
which disconnects the outer from the inner regions of the disc and then
disperses the outer disc from the inside-out, while the inner disc keeps
draining viscously onto the star. In this Letter we show that the disc around
TW Hya, the closest protoplanetary disc to Earth, may be the first object where
a photoevaporative gap has been imaged around the time at which it is being
created. Indeed the detected gap in the ALMA images is consistent with the
expectations of X-ray photoevaporation models, thus not requiring the presence
of a planet. The photoevaporation model is also consistent with a broad range
of properties of the TW Hya system, e.g. accretion rate and the location of the
gap at the onset of dispersal. We show that the central, unresolved 870 {\mu}m
continuum source might be produced by free free emission from the gas and/or
residual dust inside the gap.
</p>
{{{{ARTICLE_PARSER}}}}<p>In the blooming field of exoplanetary science, NASA's Kepler Space Telescope
has revolutionized our understanding of exoplanets. Kepler's very precise and
long-duration photometry is ideal for detecting planetary transits around
Sun-like stars. The forthcoming Transiting Exoplanet Survey Satellite (TESS) is
expected to continue Kepler's legacy. In this paper, we explore the possibility
of detecting planetary transits around hypervelocity and runaway stars, which
should host a very compact system as consequence of their turbulent origin. We
find that the probability of a multi-planetary transit is $10^{-3}\lesssim
P\lesssim 10^{-1}$. We therefore need to observe $\sim 10-1000$ high-velocity
stars to spot a transit. We predict that the European Gaia satellite, along
with TESS, could spot such transits.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present FORS2 (attached to ESO's Very Large Telescope) observations of the
exoplanet WASP-17b during its primary transit, for the purpose of differential
spectrophotometry analysis. We use the instrument in its Mask eXchange Unit
(MXU) mode to simultaneously obtain low resolution spectra of the planet
hosting star, as well as several reference stars in the field of view. The
integration of these spectra within broadband and smaller 100\AA~bins provides
us with 'white' and spectrophotometric light curves, from 5700 to 8000\AA.
Through modelling the white light curve, we obtain refined bulk and transit
parameters of the planet, as well as wavelength-dependent variations of the
planetary radius from smaller spectral bins through which the transmission
spectrum is obtained. The inference of transit parameters, as well as the noise
statistics, is performed using a Gaussian Process model. We achieve a typical
precision in the transit depth of a few hundred parts per million from various
transit light curves. From the transmission spectra we rule out a flat spectrum
at &gt;3$\sigma$ and detect marginal presence of the pressure-broadened sodium
wings. Furthermore, we detect the wing of the potassium absorption line in the
upper atmosphere of the planet with 3$\sigma$ confidence, both facts pointing
to a relatively shallow temperature gradient of the atmosphere. These
conclusions are mostly consistent with previous studies of this exo-atmosphere,
although previous potassium measurements have been inconclusive.
</p>
{{{{ARTICLE_PARSER}}}}<p>We aim to present a generalized Bayesian inference method for constraining
interiors of super Earths and sub-Neptunes. Our methodology succeeds in
quantifying the degeneracy and correlation of structural parameters for high
dimensional parameter spaces. Specifically, we identify what constraints can be
placed on composition and thickness of core, mantle, ice, ocean, and
atmospheric layers given observations of mass, radius, and bulk refractory
abundance constraints (Fe, Mg, Si) from observations of the host star's
photospheric composition. We employed a full probabilistic Bayesian inference
analysis that formally accounts for observational and model uncertainties.
Using a Markov chain Monte Carlo technique, we computed joint and marginal
posterior probability distributions for all structural parameters of interest.
We included state-of-the-art structural models based on self-consistent
thermodynamics of core, mantle, high-pressure ice, and liquid water.
Furthermore, we tested and compared two different atmospheric models that are
tailored for modeling thick and thin atmospheres, respectively. First, we
validate our method against Neptune. Second, we apply it to synthetic
exoplanets of fixed mass and determine the effect on interior structure and
composition when (1) radius, (2) atmospheric model, (3) data uncertainties, (4)
semi-major axes, (5) atmospheric composition (i.e., a priori assumption of
enriched envelopes versus pure H/He envelopes), and (6) prior distributions are
varied. Our main conclusions are: [...]
</p>
{{{{ARTICLE_PARSER}}}}<p>Using a generalized Bayesian inference method, we aim to explore the possible
interior structures of six selected exoplanets for which planetary mass and
radius measurements are available in addition to stellar host abundances:
HD~219134b, Kepler-10b, Kepler-93b, CoRoT-7b, 55~Cnc~e, and HD~97658b. We aim
to investigate the importance of stellar abundance proxies for the planetary
bulk composition (namely Fe/Si and Mg/Si) on prediction of planetary interiors.
We performed a full probabilistic Bayesian inference analysis to formally
account for observational and model uncertainties while obtaining confidence
regions of structural and compositional parameters of core, mantle, ice layer,
ocean, and atmosphere. We determined how sensitive our parameter predictions
depend on (1) different estimates of bulk abundance constraints and (2)
different correlations of bulk abundances between planet and host star. [...]
Although the possible ranges of interior structures are large, structural
parameters and their correlations are constrained by the sparse data. The
probability for the tested exoplanets to be Earth-like is generally very low.
Furthermore, we conclude that different estimates of planet bulk abundance
constraints mainly affect mantle composition and core size.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider diffusion caused by a combined influence of the electric current
and the Hall effect, and argue that such diffusion can form inhomogeneities of
the chemical composition in plasma. The considered mechanism can be responsible
for a formation of element spots in laboratory and astrophysical plasmas. This
current-driven diffusion can be accompanied by propagation of a particular type
waves in which the impurity number density oscillate alone. These compositional
waves exist if the magnetic pressure in plasma is much greater than the gas
pressure,
</p>
{{{{ARTICLE_PARSER}}}}<p>We apply a novel spectral graph technique, that of locally-biased
semi-supervised eigenvectors, to study the diversity of galaxies. This
technique permits us to characterize empirically the natural variations in
observed spectra data, and we illustrate how this approach can be used in an
exploratory manner to highlight both large-scale global as well as small-scale
local structure in Sloan Digital Sky Survey (SDSS) data. We use this method in
a way that simultaneously takes into account the measurements of spectral lines
as well as the continuum shape. Unlike Principal Component Analysis, this
method does not assume that the Euclidean distance between galaxy spectra is a
good global measure of similarity between all spectra, but instead it only
assumes that local difference information between similar spectra is reliable.
Moreover, unlike other nonlinear dimensionality methods, this method can be
used to characterize very finely both small-scale local as well as large-scale
global properties of realistic noisy data. The power of the method is
demonstrated on the SDSS Main Galaxy Sample by illustrating that the derived
embeddings of spectra carry an unprecedented amount of information. By using a
straightforward global or unsupervised variant, we observe that the main
features correlate strongly with star formation rate and that they clearly
separate active galactic nuclei. Computed parameters of the method can be used
to describe line strengths and their interdependencies. By using a
locally-biased or semi-supervised variant, we are able to focus on typical
variations around specific objects of astronomical interest. We present several
examples illustrating that this approach can enable new discoveries in the data
as well as a detailed understanding of very fine local structure that would
otherwise be overwhelmed by large-scale noise and global trends in the data.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a multi-wavelength polarimetric and spectral study of M87 jet
obtained at sub- arcsecond resolution between 2002 and 2008. The observations
include multi-band archival VLA polarimetry data sets along with the HST
imaging polarimetry. These observations have better angular resolution than
previous work by factors of 2-3 and in addition, allow us to explore the time
domain. These observations envelope the huge flare in HST-1 located at 0.\"86
from the nucleus (Cheung et al. 2007; Harris et al. 2009; Madrid 2009; Perlman
et al. 2011). The increased resolution enables us to view more structure in
each knot, showing several resolved sub-components. We also see apparent
helical structure in the polarization vectors in several knots, with
polarization vectors turning either clockwise or counterclockwise near the flux
maxima in various places as well as show filamentary undulations. Some of these
characteristics are correlated with flux and polarization maxima while others
are not. We also examine the total flux and fractional polarization and look
for changes in both radio and optical since the observations of Perlman et al.
(1999) and test them against various models based on shocks and instabilities
in the jet. Our results are broadly consistent with previous spine-sheath
models and recollimation shock models, however, they require additional
combinations of features to explain the observed complexity, e.g. shearing of
magnetic field lines near the jet surface and compression of the toroidal
component near shocks. In particular, in many regions we find apparently
helical features both in total flux and polarization. We discuss the physical
interpretation of these features.
</p>
{{{{ARTICLE_PARSER}}}}<p>There exists a class of ultralight Dark Matter (DM) models which could form a
Bose-Einstein condensate (BEC) in the early universe and behave as a single
coherent wave instead of individual particles in galaxies. We show that a
generic BEC DM halo intervening along the line of sight of a gravitational wave
(GW) signal could induce an observable change in the speed of GW, with the
effective refractive index depending only on the mass and self-interaction of
the constituent DM particles and the GW frequency. Hence, we propose to use the
deviation in the speed of GW as a new probe of the BEC DM parameter space. With
a multi-messenger approach to GW astronomy and/or with extended sensitivity to
lower GW frequencies, the entire BEC DM parameter space can be effectively
probed by our new method in the near future.
</p>
{{{{ARTICLE_PARSER}}}}<p>Nearby galaxies host ultra-luminous X-ray sources (ULXs), whose nature
remains largely unknown. Until the discovery of the first ULX pulsar, M82 X-2,
the mechanism powering the large luminosities of most ULXs was thought to be
super-Eddington accretion onto black holes. The ULX pulsar clearly indicates
that this hypothesis is not universal, and the question arises if other ULXs
are as well powered by accretion onto neutron stars. One possibility to have
highly super-Eddington luminosity is by reducing the opacity by strong magnetic
fields as in magnetars, as proposed for M82 X-2. To study the link between ULXs
and magnetar bursts/flares, we have performed a comparative spectral study
between these classes, which both emit at similar super-Eddington luminosities
at around $L \sim 10^{40}$ erg s$^{-1}$. We find that, when their spectra are
fitted with dual thermal models, the long term spectral variations of ULXs are
similar to short term spectral variability seen during magnetar flares. In both
classes of sources the black body temperatures and radii follow similar
$R_\textrm{bb} \propto T_\textrm{bb}^{-2}$ trends, the only difference being
that the temperatures in ULXs are an order of magnitude lower and thus they
have much larger characteristic emission areas. Furthermore, the ratio between
the luminosities of the two black body components shows the same one-to-one
dependence in ULXs and magnetars. We discuss the possible origins of these
similarities, and speculate that the reason for the lack of pulsations in
\"magnetar-ULXs\" could be that their photospheres are extended far beyond the
neutron star surface.
</p>
{{{{ARTICLE_PARSER}}}}<p>Dark matter simulations can serve as a basis for creating galaxy histories
via the galaxy-dark matter connection. Here, one such model by Becker (2015) is
implemented with several variations on three different dark matter simulations.
Stellar mass and star formation rates are assigned to all simulation subhalos
at all times, using subhalo mass gain to determine stellar mass gain. The
observational properties of the resulting galaxy distributions are compared to
each other and observations for a range of redshifts from 0-2. Although many of
the galaxy distributions seem reasonable, there are noticeable differences as
simulations, subhalo mass gain definitions, or subhalo mass definitions are
altered, suggesting that the model should change as these properties are
varied. Agreement with observations may improve by including redshift
dependence in the added-by-hand random contribution to star formation rate.
There appears to be an excess of faint quiescent galaxies as well (perhaps due
in part to differing definitions of quiescence). The ensemble of galaxy
formation histories for these models tend to have more scatter around their
average histories (for a fixed final stellar mass) than the two more predictive
and elaborate semi-analytic models of Guo et al (2013) and Henriques et al
(2015), and require more basis fluctuations (using PCA) to capture 90 percent
of the scatter around their average histories.
</p>
<p>The codes to plot model predictions (in some cases alongside observational
data) are publicly available to test other mock catalogues at
https://github.com/jdcphysics/validation/codes/vsuite . Information on how to
use these codes is in the appendix.
</p>
{{{{ARTICLE_PARSER}}}}<p>3C273, the nearest bright quasar, comprises a strong nuclear core and a
bright, one-sided jet extending ~ 23 arcseconds to the SW. The source has been
the subject of imaging campaigns in all wavebands. Extensive observations of
this source have been made with the Very Large Array and other telescopes as
part of a campaign to understand the jet emission mechanisms. Partial results
from the VLA radio campaign have been published, but to date, the complete set
of VLA imaging results has not been made available. We have utilized the VLA to
determine the radio structure of 3C273 in Stokes I, Q, and U, over the widest
possible frequency and resolution range. The VLA observed the source in all
four of its configurations, and with all eight of its frequency bands, spanning
73.8 MHz to 43 GHz. The data were taken in a pseudo-spectral line mode to
minimize the VLA's correlator errors, and were fully calibrated with subsequent
self-calibration techniques to maximise image fidelity. Images in Stokes
parameters I, Q, and U, spanning a resolution range from 6 arcseconds to 88
milliarcseconds are presented. Spectral index images, showing the evolution of
the jet component are shown. Polarimetry demonstrates the direction of the
magnetic fields responsible for the emission, and rotation measure maps show
the RM to be very small with no discernible trend along or across the jet. This
paper presents a small subset of these images to demonstrate the major
characteristics of the source emission. A library of all ~500 images has been
made available for open, free access by interested parties.
</p>
{{{{ARTICLE_PARSER}}}}<p>Probability of coincidental clustering among the orbits of comets, asteroids
and meteoroids depends on many factors like: the size of the orbital sample
searched for clusters or the size of the identified group, it is different for
groups of 2,3,4, ... members. Probability of coincidental clustering is
assessed by the numerical simulation, therefore, it depends also on the method
used for the synthetic orbits generation.
</p>
<p>We tested the impact of some of these factors. For a given size of the
orbital sample we have assessed probability of random pairing among several
orbital populations of different sizes. We have found how these probabilities
vary with the size of the orbital samples.
</p>
<p>Finally, keeping fixed size of the orbital sample we have shown that the
probability of random pairing can be significantly different for the orbital
samples obtained by different observation techniques. Also for the user
convenience we have obtained several formulae which, for given size of the
orbital sample, one can use to calculate the similarity threshold corresponding
to the small value of the probability of coincidental similarity among two
orbits.
</p>
{{{{ARTICLE_PARSER}}}}<p>We investigate the fossil magnetic field in the accretion and protoplanetary
discs using the Shakura and Sunyaev approach. The distinguishing feature of
this study is the accurate solution of the ionization balance equations and the
induction equation with Ohmic diffusion, magnetic ambipolar diffusion, buoyancy
and the Hall effect. We consider the ionization by cosmic rays, X-rays and
radionuclides, radiative recombinations, recombinations onto dust grains, and
also thermal ionization. The buoyancy appears as the additional mechanism of
magnetic flux escape in the steady-state solution of the induction equation.
Calculations show that Ohmic diffusion and magnetic ambipolar diffusion
constraint the generation of the magnetic field inside the `dead' zones. The
magnetic field in these regions is quasi-vertical. The buoyancy constraints the
toroidal magnetic field strength close to the disc inner edge. As a result, the
toroidal and vertical magnetic fields become comparable. The Hall effect is
important in the regions close to the borders of the `dead' zones because
electrons are magnetized there. The magnetic field in these regions is
quasi-radial. We calculate the magnetic field strength and geometry for the
discs with accretion rates $(10^{-8}-10^{-6})\,\rm{M}_{\odot}\,\rm{yr}^{-1}$.
The fossil magnetic field geometry does not change significantly during the
disc evolution while the accretion rate decreases. We construct the synthetic
maps of dust emission polarized due to the dust grain alignment by the magnetic
field. In the polarization maps, the `dead' zones appear as the regions with
the reduced values of polarization degree in comparison to those in the
adjacent regions.
</p>
{{{{ARTICLE_PARSER}}}}<p>We have investigated a recently proposed halo-based model, Camelus, for
predicting weak-lensing peak counts, and compared its results over a collection
of 162 cosmologies with those from N-body simulations. While counts from both
models agree for peaks with $\mathcal{S/N}&gt;1$ (where $\mathcal{S/N}$ is the
ratio of the peak height to the r.m.s. shape noise), we find $\approx 50\%$
fewer counts for peaks near $\mathcal{S/N}=0$ and significantly higher counts
in the negative $\mathcal{S/N}$ tail. Adding shape noise reduces the
differences to within $20\%$ for all cosmologies. We also found larger
covariances that are more sensitive to cosmological parameters. As a result,
credibility regions in the $\{\Omega_m, \sigma_8\}$ are $\approx 30\%$ larger.
Even though the credible contours are commensurate, each model draws its
predictive power from different types of peaks. Low peaks, especially those
with $2&lt;\mathcal{S/N}&lt;3$, convey important cosmological information in N-body
data, as shown in \cite{DietrichHartlap, Kratochvil2010}, but \textsc{Camelus}
constrains cosmology almost exclusively from high significance peaks
$(\mathcal{S/N}&gt;3)$. Our results confirm the importance of using a
cosmology-dependent covariance with at least a 14\% improvement in parameter
constraints. We identified the covariance estimation as the main driver behind
differences in inference, and suggest possible ways to make Camelus even more
useful as a highly accurate peak count emulator.
</p>
{{{{ARTICLE_PARSER}}}}<p>Momentum-space curvature, which is expected in some approaches to the
quantum-gravity problem, can produce dual redshift, a feature which introduces
energy dependence of the travel times of ultrarelativistic particles, and dual
lensing, a feature which mainly affects the direction of observation of
particles. In our recent <a href=\"/abs/1605.00496\">arXiv:1605.00496</a> we explored the possibility that dual
redshift might be relevant in the analysis of IceCube neutrinos, obtaining
results which are preliminarily encouraging. Here we explore the possibility
that also dual lensing might play a role in the analysis of IceCube neutrinos.
In doing so we also investigate issues which are of broader interest, such as
the possibility of estimating the contribution by background neutrinos and some
noteworthy differences between candidate \"early neutrinos\" and candidate \"late
neutrinos\".
</p>
{{{{ARTICLE_PARSER}}}}<p>CMB Stage-4 experiments will reduce the uncertainties on the gravitational
lensing potential by an order of magnitude compared to current measurements,
and will also produce a Sunyaev-Zel'dovich (SZ) cluster catalog containing
$\sim10^{5}$ objects, two orders of magnitudes higher than what is currently
available. In this paper we propose to combine these two observables and show
that it is possible to calibrate the masses of the full Stage-4 cluster catalog
internally owing to the high signal to noise measurement of the CMB lensing
convergence field. We find that a CMB Stage-4 experiment will constrain the
hydrostatic bias parameter to sub-percent accuracy. We also show constraints on
a non parametric $Y-M$ relationship which could be used to study its evolution
with mass and redshift. Finally we present a joint likelihood for thermal SZ
(tSZ) flux and mass measurements, and show that it could lead to a
$\sim5\sigma$ detection of the lower limit on the sum of the neutrino masses in
the normal hierarchy ($\sum m_{\nu}=60 \textrm{meV}$) once combined with
measurements of the primordial CMB and CMB lensing power spectra.
</p>
{{{{ARTICLE_PARSER}}}}<p>Since there are dark matter particles (neutrino) with mass about 10^(-1)eV in
the universe, the superstructures with a scale of 10^(19) solar mass [large
number A is about 10^(19)] appeared around the era of the hydrogen
recombination. The redshift z distributions of quasars support the existence of
superstructures. Since there are superstructures in the universe, it is not
necessary for the hypothesis of dark energy. While neutrino is related to
electro-weak field, the fourth stable elementary particles (delta particle)
with mass about 10^(0)eV to 10^(1)eV is related to gravitation-\"strong\" field,
which suggests p + anti(p)--&gt; n/anti(n) + anti(delta particle)/(delta particle)
and that some new meta-stable baryons appeared near the TeV region. Therefore,
a twofold standard model diagram is proposed, and related to many experiment
phenomena: The new meta-stable baryons' decays produce delta particles, which
are helpful to explain the Dijet asymmetry phenomena at LHC of CERN, the
different results for the Fermilab's data peak, etc; However, according to the
(B-L) invariance, the sterile \"neutrino\" from \"the event excess in MiniBooNe\"
can not be the fourth neutrino but rather the delta particle; We think that the
delta particles are related to the phenomenon about neutrinos FTL, and that
anti-neutrinos are faster than neutrinos. FTL is also related to the cosmic
inflation, singular point disappearance, and abnormal red shift of SN Ia. Some
experiments and observations are suggested. In the Extension section, we
clarify \"mass tree\", our finite universe, cosmic dual expansions, dual SM etc.
And the LHC can look for new particles with decay products graviton/delta
particle and new interaction indeed.
</p>
{{{{ARTICLE_PARSER}}}}<p>This document describes the structures making up the VOTable standard. The
main part of this document describes the adopted part of the VOTable standard;
it is followed by appendices presenting extensions which have been proposed
and/or discussed, but which are not part of the standard.
</p>
{{{{ARTICLE_PARSER}}}}<p>In the current concordance cosmology small halos are expected to be
completely dark and can significantly perturb low-mass galaxies during minor
merger interactions. These interactions may well contribute to the diversity of
the dwarf galaxy population. Dwarf galaxies in the field are often observed to
have peculiarities in their structure, morphology, and kinematics, as well as
strong bursts of star formation without apparent cause. We aim to characterize
the signatures of minor mergers of dwarf galaxies with dark satellites to aid
their observational identification. We explore and quantify a variety of
structural, morphological, and kinematic indicators of merging dwarf galaxies
and their remnants using a suite of hydrodynamical simulations. The most
sensitive indicators of mergers with dark satellites are large asymmetries in
the gaseous and stellar distributions, enhanced central surface brightness and
starbursts, and velocity offsets and misalignments between the cold gas and
stellar components. In general, merging systems span a wide range of values of
the most commonly used indicators, while isolated objects tend to have more
confined values. Interestingly, we find in our simulations that a significantly
off-centered burst of star formation can pinpoint the location of the dark
satellite. Observational systems with such characteristics are perhaps the most
promising for unveiling the presence of the hitherto, missing satellites.
</p>
{{{{ARTICLE_PARSER}}}}<p>Studying RS CVn binaries is challenging, because in addition to spot
activity, other effects such as mass transfer between the components and
gravitational distortion of their spherical forms may distort their light
curves. Such effects can, however, be removed from the data by subtracting a
mean light curve phased with the orbital period. We study a quarter of a
century of standard Johnson differential V photometry of the RS CVn binary BM
CVn. Our main aims are to determine the activity cycles, the rate of surface
differential rotation and the rotation period of the active longitudes of BM
CVn. The Continuous Period Search (CPS) is applied to the photometry. The
changes of the mean and amplitude of the light curves are used to search for
activity cycles. The rotation period changes give an estimate of the rate of
surface differential rotation. The Kuiper method is applied to the epochs of
the primary and secondary minima to search for active longitudes. The
photometry reveals the presence of a stable mean light curve (MLC) connected to
the orbital period P_orb = 20d.6252 of this binary. We remove this MLC from the
original V magnitudes which gives us the corrected V' magnitudes. These two
samples of V and V' data are analysed separately with CPS. The fraction of
unreliable CPS models decreases when the MLC is removed. The same significant
activity cycle of approximately 12.5 years is detected in both V and V'
samples. The estimate for the surface differential rotation coefficient, k &gt;=
0.10, is the same for both samples, but the number of unrealistic period
estimates decreases after removing the MLC. The same active longitude period of
P_al = 20d.511 +- 0d.005 is detected in the V and V' magnitudes. This long-term
regularity in the epochs of primary and secondary minima of the light curves is
not caused by the MLC. On the contrary, the MLC hampers the detection of active
longitudes.
</p>
{{{{ARTICLE_PARSER}}}}<p>In the context of f(R) gravity theories, we show that the apparent mass of a
neutron star as seen from an observer at infinity is numerically calculable but
requires careful matching, first at the star's edge, between interior and
exterior solutions, none of them being totally Schwarzschild-like but
presenting instead small oscillations of the curvature scalar R; and second at
large radii, where the Newtonian potential is used to identify the mass of the
neutron star. We find that for the same equation of state, this mass definition
is always larger than its general relativistic counterpart. We exemplify this
with quadratic $R^2$ and Hu-Sawicki-like modifications of the standard General
Relativity action. Therefore, the finding of two-solar mass neutron stars
basically imposes no constraint on stable f(R) theories. However, star radii
are in general smaller than in General Relativity, which can give an
observational handle on such classes of models at the astrophysical level. Both
larger masses and smaller matter radii are due to much of the apparent
effective energy residing in the outer metric for scalar-tensor theories.
Finally, because the f(R) neutron star masses can be much larger than General
Relativity counterparts, the total energy available for radiating gravitational
waves could be of order several solar masses, and thus a merger of these stars
constitutes an interesting wave source.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study the relation between the Jordan-Einstein frame transition and the
possible description of the crossing of singularities in flat Friedmann
universes, using the fact that the regular evolution in one frame can
correspond to crossing singularities in the other frame. We show that some
interesting effects arise in simple models such as one with a massless scalar
field or another wherein the potential is constant in the Einstein frame. The
dynamics in these models and in their conformally coupled counterparts are
described in detail, and a method for the continuation of such cosmological
evolutions beyond the singularity is developed. We compare our approach with
some other, recently developed, approaches to the problem of the crossing of
singularities.
</p>
{{{{ARTICLE_PARSER}}}}<p>We investigate the effects of self-interacting dark matter (SIDM) on the
tidal stripping and evaporation of satellite galaxies in a Milky Way-like host.
We use a suite of five zoom-in, dark-matter-only simulations, two with
velocity-independent SIDM cross sections, two with velocity-dependent SIDM
cross sections, and one cold dark matter simulation for comparison. After
carefully assigning stellar mass to satellites at infall, we find that stars
are stripped at a higher rate in SIDM than in CDM. In contrast, the total bound
dark matter mass loss rate is minimally affected, with subhalo evaporation
having negligible effects on satellites for viable SIDM models. Centrally
located stars in SIDM haloes disperse out to larger radii as cores grow.
Consequently, the half-light radius of satellites increases, stars become more
vulnerable to tidal stripping, and the stellar mass function is suppressed. We
find that the ratio of core radius to tidal radius accurately predicts the
relative strength of enhanced SIDM stellar stripping. Velocity-independent SIDM
models show a modest increase in the stellar stripping effect with satellite
mass, whereas velocity-dependent SIDM models show a large increase in this
effect towards lower masses, making observations of ultra-faint dwarfs prime
targets for distinguishing between and constraining SIDM models. Due to small
cores in the largest satellites of velocity-dependent SIDM, no identifiable
imprint is left on the all-sky properties of the stellar halo. While our
results focus on SIDM, the main physical mechanism of enhanced tidal stripping
of stars apply similarly to satellites with cores formed via other means.
</p>
{{{{ARTICLE_PARSER}}}}<p>We test the viability of the Higgs-dilaton model (HDM) compared to the
evolving dark energy ($w_0 w_a$CDM) model, in which the cosmological constant
model $\Lambda$CDM is also nested, by using the latest cosmological data that
includes the cosmic microwave background temperature, polarization and lensing
data from the \textit{Planck} satellite (2015 data release), the BICEP and Keck
Array experiments, the Type Ia supernovae from the JLA catalog, the baryon
acoustic oscillations from CMASS, LOWZ and 6dF, the weak lensing data from the
CFHTLenS survey and the matter power Spectrum measurements from the SDSS (data
release 7). We find that the values of all cosmological parameters allowed by
the Higgs-dilaton inflation model are well within the \textit{Planck} satellite
(2015 data release) constraints. In particular, we have that $w_0 =
-1.0001^{+0.0072}_{-0.0074}$, $w_a = 0.00^{+0.15}_{-0.16}$, $n_s =
0.9693^{+0.0083}_{-0.0082}$, $\alpha_s = -0.001^{+0.013}_{-0.014}$ and
$r_{0.05} = 0.0025^{+0.0017}_{-0.0016}$ (95.5\%C.L.). We also place new
stringent constraints on the couplings of the Higgs-dilaton model and we find
that $\xi_{\chi} &lt; 0.00328$ and $\xi_h / \sqrt{\lambda} =
59200^{+30000}_{-20000}$ (95.5\%C.L.). Furthermore, we report that the HDM is
at a slightly better footing than the $w_0 w_a$CDM model, as they both have
practically the same chi-square, i.e. $\Delta \chi^2 = \chi^2_{w_0
w_a\mathrm{CDM}}-\chi^2_{\mathrm{HDM}}=0.18$, with the HDM model having two
fewer parameters. Finally Bayesian evidence favors equally the two models, with
the HDM being preferred by the AIC and DIC information criteria.
</p>
{{{{ARTICLE_PARSER}}}}<p>With the Planck 2015 Cosmic Microwave Background (CMB) temperature and
polarization data, we search for possible features in the primordial power
spectrum (PPS). We revisit the Wiggly Whipped Inflation (WWI) framework and
demonstrate how generation of some particular primordial features can improve
the fit to Planck data. WWI potential allows the scalar field to transit from a
steeper potential to a nearly flat potential through a discontinuity either in
potential or in its derivatives. WWI offers the inflaton potential
parametrizations that generate a wide variety of features in the primordial
power spectra incorporating most of the localized and non-local inflationary
features that are obtained upon reconstruction from temperature and
polarization angular power spectrum. At the same time, in a single framework it
allows us to have a background parameter estimation with a nearly free-form
primordial spectrum. Using Planck 2015 data, we constrain the primordial
features in the context of Wiggly Whipped Inflation and present the features
that are supported both by temperature and polarization. WWI model provides
more than $13$ improvement in $\chi^2$ fit to the data with respect to the best
fit power law model considering combined temperature and polarization data from
Planck and B-mode polarization data from BICEP and Planck dust map. We use 2-4
extra parameters in the WWI model compared to the featureless strict slow roll
inflaton potential. We find that the differences between the temperature and
polarization data in constraining background cosmological parameters such as
baryon density, cold dark matter density are reduced to a good extent if we use
primordial power spectra from WWI. We also discuss the extent of bispectra
obtained from the best potentials in arbitrary triangular configurations using
the BI-spectra and Non-Gaussianity Operator (BINGO).
</p>
{{{{ARTICLE_PARSER}}}}<p>Stars that pass within the Roche radius of a supermassive black hole will be
tidally disrupted, yielding a sudden injection of gas close to the black hole
horizon which produces an electromagnetic flare. A few dozen of these flares
have been discovered in recent years, but current observations provide poor
constraints on the bolometric luminosity and total accreted mass of these
events. Using images from the Wide-field Infrared Survey Explorer (WISE), we
have discovered transient 3.4 micron emission from several previously known
tidal disruption flares. The observations can be explained by dust heated to
its sublimation temperature due to the intense radiation of the tidal flare.
From the break in the infrared light curve we infer that this hot dust is
located ~0.1 pc from the supermassive black hole. Since the dust has been
heated by absorbing UV and (potentially) soft X-ray photons of the flare, the
reprocessing light curve yields an estimate of the bolometric flare luminosity.
For the flare PTF-09ge, we infer that the most likely value of the luminosity
integrated over frequencies at which dust can absorb photons is $8\times
10^{44}$ erg/s, with a factor of 3 uncertainty due to the unknown temperature
of the dust. This bolometric luminosity is a factor ~10 larger than the
observed black body luminosity. Our work is the first to probe dust in the
nuclei of non-active galaxies on sub-parsec scales. The observed infrared
luminosity implies a covering factor ~1% for the nuclear dust in the host
galaxies.
</p>
{{{{ARTICLE_PARSER}}}}<p>We propose an explanation of the galactic center gamma ray excess by
supersymmetric WIMPs as heavy as 500 GeV. The lightest neutralino annihilates
into vector-like leptons or quarks which cascade decay through intermediate
Higgs bosons. Due to the long decay chains, the gamma ray spectrum is much
softer than naively expected and peaks at GeV energies. The model predicts
correlated diboson and dijet signatures to be tested at the LHC.
</p>
{{{{ARTICLE_PARSER}}}}<p>The light curve of many supernovae (SNe) and gamma-ray bursts (GRBs) can be
explained by a sustained injection of extra energy from its possible central
engine, a rapidly rotating strongly magnetic neutron star (i.e. magnetar). The
magnetic dipole radiation power that the magnetar supplies comes at the expense
of the star's rotational energy. However, radiation by gravitational waves
(GWs) can be more efficient than magnetic dipole radiation because of its
stronger dependence on neutron star spin rate Omega, i.e. Omega^6 (for a static
'mountain') or Omega^8 (for a r-mode fluid oscillation) versus Omega^4 for
magnetic dipole radiation. Here, we use the magnetic field B and initial spin
period P_0 inferred from SN and GRB observations to obtain simple constraints
on the dimensionless amplitude of the mountain of epsilon &lt; 0.01 and r-mode
oscillation of alpha &lt; 1, the former being similar to that obtained by recent
works. We then include GW emission within the magnetar model. We show that when
epsilon &gt; 10^-4 (B/10^14 G) (P_0/1 ms) or alpha &gt; 0.01 (B/10^14 G) (P_0/1
ms)^2, light curves are strongly affected, with significant decrease in peak
luminosity and increase in time to peak luminosity. Thus the GW effects studied
here are more pronounced for low B and short P_0 but are unlikely to be
important in modelling SN and GRB light curves since the amplitudes needed for
noticeable changes are quite large.
</p>
{{{{ARTICLE_PARSER}}}}<p>We investigated the typical environment and physical properties of \"red
discs\" and \"blue bulges\", comparing those to the \"normal\" objects in the blue
cloud and red sequence. Our sample is composed of cluster members and field
galaxies at $z \le 0.1$, so that we can assess the impact of the local and
global environment. We find that disc galaxies display a strong dependence on
environment, becoming redder for higher densities. This effect is more
pronounced for objects within the virial radius, being also strong related to
the stellar mass. We find that local and global environment affect galaxy
properties, but the most effective parameter is stellar mass. We find evidence
for a scenario where \"blue discs\" are transformed into \"red discs\" as they grow
in mass and move to the inner parts of clusters. From the metallicity
differences of red and blue discs, and the analysis of their star formation
histories, we suggest the quenching process is slow. We estimate a quenching
time scale of $\sim $ 2$-$3 Gyr. We also find from the sSFR$-$M$_*$ plane that
\"red discs\" gradually change as they move into clusters. The \"blue bulges\" have
many similar properties than \"blue discs\", but some of the former show strong
signs of asymmetry. The high asymmetry \"blue bulges\" display enhanced recent
star formation compared to their regular counterparts. That indicates some of
these systems may have increased their star formation due to mergers.
Nonetheless, there may not be a single evolutionary path for these blue
early-type objects.
</p>
{{{{ARTICLE_PARSER}}}}<p>We report the discovery of Qatar-3b, Qatar-4b, and Qatar-5b, three new
transiting planets identified by the Qatar Exoplanet Survey (QES). The three
planets belong to the hot Jupiter family, with orbital periods of
$P_{Q3b}$=2.50792 days, $P_{Q4b}$=1.80539 days, and $P_{Q5b}$=2.87923 days.
Follow-up spectroscopic observations reveal the masses of the planets to be
$M_{Q3b}$=4.31$\pm0.47$ $M_{\rm J}$, $M_{Q4b}$=5.85$\pm0.47$ $M_{\rm J}$, and
$M_{Q5b}$=4.32$\pm0.18$ $M_{\rm J}$, while model fits to the transit light
curves yield radii of $R_{Q3b}$=1.096$\pm0.14$ $R_{\rm J}$,
$R_{Q4b}$=1.552$\pm0.057$ $R_{\rm J}$, and $R_{Q5b}$=1.107$\pm0.064$ $R_{\rm
J}$. No evidence for an eccentric orbit is seen in the radial velocity curve of
any of the planets. In each case an F-test analysis shows that the p-values for
an eccentric orbit scenario (e=0.1) are too high to support it. The host stars
are low-mass main sequence stars with masses and radii $M_{Q3}$=1.145$\pm0.064$
$M_{\odot}$, $M_{Q4}$=0.954$\pm0.048$ $M_{\odot}$, $M_{Q5}$=1.128$\pm0.056$
$M_{\odot}$ and $R_{Q3}$=1.272$\pm0.14$ $R_{\odot}$, $R_{Q4}$=1.115$\pm0.037$
$R_{\odot}$ and $R_{Q5}$=1.076$\pm0.051$ $R_{\odot}$ for Qatar-3, 4 and 5
respectively. The V magnitudes of the three host stars are $V_{Q3}$=12.88,
$V_{Q4}$=13.60, and $V_{Q5}$=12.62. All three new planets can be classified as
heavy hot Jupiters (M &gt; 4 $M_{J}$).
</p>
{{{{ARTICLE_PARSER}}}}<p>The Hydrogen Epoch of Reionization Array (HERA) is a staged experiment to
measure 21 cm emission from the primordial intergalactic medium (IGM)
throughout cosmic reionization ($z=6-12$), and to explore earlier epochs of our
Cosmic Dawn ($z\sim30$). During these epochs, early stars and black holes
heated and ionized the IGM, introducing fluctuations in 21 cm emission. HERA is
designed to characterize the evolution of the 21 cm power spectrum to constrain
the timing and morphology of reionization, the properties of the first
galaxies, the evolution of large-scale structure, and the early sources of
heating. The full HERA instrument will be a 350-element interferometer in South
Africa consisting of 14-m parabolic dishes observing from 50 to 250 MHz.
Currently, 19 dishes have been deployed on site and the next 18 are under
construction. HERA has been designated as an SKA Precursor instrument.
</p>
<p>In this paper, we summarize HERA's scientific context and provide forecasts
for its key science results. After reviewing the current state of the art in
foreground mitigation, we use the delay-spectrum technique to motivate
high-level performance requirements for the HERA instrument. Next, we present
the HERA instrument design, along with the subsystem specifications that ensure
that HERA meets its performance requirements. Finally, we summarize the
schedule and status of the project. We conclude by suggesting that, given the
realities of foreground contamination, current-generation 21 cm instruments are
approaching their sensitivity limits. HERA is designed to bring both the
sensitivity and the precision to deliver its primary science on the basis of
proven foreground filtering techniques, while developing new subtraction
techniques to unlock new capabilities. The result will be a major step toward
realizing the widely recognized scientific potential of 21 cm cosmology.
</p>
{{{{ARTICLE_PARSER}}}}<p>Super Earths are the largest population of exoplanets and are seen to exhibit
a rich diversity of compositions as inferred through their mean densities. Here
we present a model that combines equilibrium chemistry in evolving disks with
core accretion that tracks materials accreted onto planets during their
formation. In doing so, we aim to explain why super Earths form so frequently
and how they acquire such a diverse range of compositions. A key feature of our
model is disk inhomogeneities, or planet traps, that act as barriers to rapid
type-I migration. The traps we include are the dead zone, which can be caused
by either cosmic ray or X-ray ionization, the ice line, and the heat
transition. We find that in disks with sufficiently long lifetimes ($\gtrsim$ 4
Myr), all traps produce Jovian planets. In these disks, planet formation in the
heat transition and X-ray dead zone produces hot Jupiters while the ice line
and cosmic ray dead zones produce Jupiters at roughly 1 AU. Super Earth
formation takes place within short-lived disks ($\lesssim$ 2 Myr), whereby the
disks are photoevaporated while planets are in a slow phase of gas accretion.
We find that super Earth compositions range from dry and rocky ($&lt;$ 6 % ice by
mass) to those with substantial water contents ($&gt;$ 30 % ice by mass). The
traps play a crucial role in our results, as they dictate where in the disk
particular planets can accrete from, and what compositions they are able to
acquire.
</p>
{{{{ARTICLE_PARSER}}}}<p>The active galactic nucleus (AGN) NGC 7213 shows a complex correlation
between the monochromatic radio luminosity $L_R$ and the 2--10 keV X-ray
luminosity $L_X$, i.e. the correlation is unusually weak with $p\sim 0$ (in the
form $L_R\propto L_X^p$) when $L_X$ is below a critical luminosity, and steep
with $p&gt;1$ when $L_X$ is above that luminosity. Such a hybrid correlation in
individual AGNs is unexpected as it deviates from the Fundamental Plane of AGN
activity. Interestingly, a similar correlation pattern is observed in the black
hole X-ray binary H1743--322, where it has been modelled by switching between
different modes of accretion. We propose that the flat $L_R$--$L_X$ correlation
of NGC 7213 is due to the presence of a luminous hot accretion flow, an
accretion model whose radiative efficiency is sensitive to the accretion rate.
Given the low luminosity of the source, $L_X\sim 10^{-4}$ of the Eddington
luminosity, the viscosity parameter is determined to be small, $\alpha\approx
0.01$. We also modelled the broad-band spectrum from radio to $\gamma$-rays,
the time lag between the radio and X-ray light curves, and the implied size and
the Lorentz factor of the radio jet. We predict that NGC 7213 will enter into a
two-phase accretion regime when $L_X &gt; 1.5 \times 10^{42}\, {\rm erg\,s^{-1}}$.
When this happens, we predict a softening of the X-ray spectrum with the
increasing flux and a steep radio/X-ray correlation.
</p>
{{{{ARTICLE_PARSER}}}}<p>We relate the information entropy and the mass variance of any distribution
in the regime of small fluctuations. We use a set of Monte Carlo simulations of
different homogeneous and inhomogeneous distributions to verify the relation
and also test it in a set of cosmological N-body simulations. We find that the
relation is in excellent agreement with the simulations and is independent of
number density and the nature of the distributions. We show that the relation
between information entropy and mass variance can be used to determine the
linear bias on large scales and detect the signatures of non-Gaussianity on
small scales in galaxy distributions.
</p>
{{{{ARTICLE_PARSER}}}}<p>The origin of the observed steep rotation curves of blue compact dwarf
galaxies (BCDs) remains largely unexplained by theoretical models of BCD
formation. We therefore investigate the rotation curves in BCDs formed from
mergers between gas-rich dwarf irregular galaxies based on the results of
numerical simulations for BCD formation. The principal results are as follows.
The dark matter of merging dwarf irregulars undergoes a central concentration
so that the central density can become up to six times higher than those of the
initial dwarf irregulars. However, the more compact dark matter halo alone can
not reproduce the gradient differences observed between dwarf irregulars and
BCDs. We provide further support that the central concentration of gas due to
rapid gas-transfer to the central regions of dwarf--dwarf mergers is
responsible for the observed difference in rotation curve gradients. The BCDs
with central gas concentration formed from merging can thus show steeply rising
rotation curves in their central regions. Such gas concentration is also
responsible for central starbursts of BCDs and the high central surface
brightness and is consistent with previous BCD studies. We discuss the
relationship between rotational velocity gradient and surface brightness, the
dependence of BCD rotation curves on star formation threshold density,
progenitor initial profile, interaction type and merger mass ratio, as well as
potential evolutionary links between dwarf irregulars, BCDs and compact dwarf
irregulars.
</p>
{{{{ARTICLE_PARSER}}}}<p>While spiral and lenticular galaxies have large-scale disks extending beyond
their bulges, and most local early-type galaxies with 10^{10} &lt; M_*/M_Sun &lt;
2x10^{11} contain a disk (e.g., ATLAS^3D), the early-type galaxies do possess a
range of disk sizes. The edge-on, `intermediate-scale' disk in the `disky
elliptical' galaxy NGC 1271 has led to some uncertainty as to what its
spheroidal component is. Walsh et al. reported a directly measured black hole
mass of 3x10^9 M_Sun for this galaxy; which they remarked was an order of
magnitude greater than what they expected based on their derivation of the host
spheroid's luminosity. Our near-infrared image analysis supports a small
embedded disk within a massive spheroidal component with M_{sph,*} =
(0.9+/-0.2)x10^{11} M_Sun (using M_*/L_H = 1.4 from Walsh et al.). This places
NGC 1271 just 1.6-sigma above the near-linear M_bh-M_{sph,*} relation for
early-type galaxies. Therefore, past speculation that there may be a systematic
difference in the black hole scaling relations between compact massive
early-type galaxies with intermediate-scale disks, i.e. ES galaxies such as NGC
1271, and early-type galaxies with either no substantial disk (E) or a
large-scale disk (S0) is not strongly supported by NGC 1271. We additionally
(i) show how ES galaxies fit naturally in the (`bulge'-to-total)-(morphological
type) diagram, while noting a complication with recent revisions to the
Hubble-Jeans tuning-fork diagram, (ii) caution about claims of over-massive
black holes in other ES galaxies if incorrectly modelled as S0 galaxies, and
(iii) reveal that the compact massive spheroid in NGC 1271 has properties
similar to bright bulges in other galaxies which have grown larger-scale disks.
</p>
{{{{ARTICLE_PARSER}}}}<p>We report the Chandra/HRC-S and Swift/XRT observations for the 2015 outburst
of the high-mass X-ray binary (HMXB) pulsar in the Small Magellanic Cloud, SMC
X-2. While previous studies suggested that either an O star or a Be star in the
field is the high-mass companion of SMC X-2, our Chandra/HRC-S image
unambiguously confirms the O-type star as the true optical counterpart. Using
the Swift/XRT observations, we extracted accurate orbital parameters of the
pulsar binary through a time of arrivals (TOAs) analysis. In addition, there
were two X-ray dips near the inferior conjunction, which are possibly caused by
eclipses or an ionized high-density shadow wind near the companion's surface.
Finally, we propose that an outflow driven by the radiation pressure from day
~10 played an important role in the X-ray/optical evolution of the outburst.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this work, we investigate the abundance and distribution of metals in the
intergalactic medium (IGM) at $\langle z \rangle \simeq 2.8$ through the
analysis of an ultra-high signal-to-noise ratio UVES spectrum of the quasar
HE0940-1050. In the CIV forest, our deep spectrum is sensitive at $3\,\sigma$
to lines with column density down to $\log N_{\rm CIV} \simeq 11.4$ and in 60
per cent of the considered redshift range down to $\simeq11.1$. In our sample,
all HI lines with $\log N_{\rm HI} \ge 14.8$ show an associated CIV absorption.
In the range $14.0 \le \log N_{\rm HI} &lt;14.8$, 43 per cent of HI lines has an
associated CIV absorption. At $\log N_{\rm HI} &lt; 14.0$, the detection rates
drop to $&lt;10$ per cent, possibly due to our sensitivity limits and not to an
actual variation of the gas abundance properties. In the range $\log N_{\rm HI}
\ge 14$, we observe a fraction of HI lines with detected CIV a factor of 2
larger than the fraction of HI lines lying in the circum-galactic medium (CGM)
of relatively bright Lyman-break galaxies hosted by dark matter haloes with
$\langle M\rangle \sim10^{12}$ M$_{\odot}$. The comparison of our results with
the output of a grid of photoionization models and of two cosmological
simulations implies that the volume filling factor of the IGM gas enriched to a
metallicity $\log Z/Z_{\odot} \ge -3$ should be of the order of $\sim 10-13$
percent. In conclusion, our results favour a scenario in which metals are found
also outside the CGM of bright star-forming galaxies, possibly due to pollution
by lower mass objects and/or to an early enrichment by the first sources.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present the results of near-infrared (2.5--5.4um) long-slit spectroscopy
of the extended green object (EGO) G318.05+0.09 with AKARI. Two distinct
sources are found in the slit. The brighter source has strong red continuum
emission with H2O ice, CO2 ice, and CO gas and ice absorption features at 3.0,
4.25um, 4.67um, respectively, while the other greenish object shows peculiar
emission that has double peaks at around 4.5 and 4.7um. The former source is
located close to the ultra compact HII region IRAS 14498-5856 and is identified
as an embedded massive young stellar object. The spectrum of the latter source
can be interpreted by blue-shifted (-3000 ~ -6000km/s) optically-thin emission
of the fundamental ro-vibrational transitions (v=1-0) of CO molecules with
temperatures of 12000--3700K without noticeable H2 and HI emission. We discuss
the nature of this source in terms of outflow associated with the young stellar
object and supernova ejecta associated with a supernova remnant.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present an asteroseismic analysis of 33 solar-type stars observed in short
cadence during Campaigns (C) 1-3 of the NASA K2 mission. We were able to
extract both average seismic parameters and individual mode frequencies for
stars with dominant frequencies up to ~3300{\mu}Hz, and we find that data for
some targets are good enough to allow for a measurement of the rotational
splitting. Modelling of the extracted parameters is performed by using
grid-based methods using average parameters and individual frequencies together
with spectroscopic parameters. For the target selection in C3, stars were
chosen as in C1 and C2 to cover a wide range in parameter space to better
understand the performance and noise characteristics. For C3 we still detected
oscillations in 73% of the observed stars that we proposed. Future K2 campaigns
hold great promise for the study of nearby clusters and the chemical evolution
and age-metallicity relation of nearby field stars in the solar neighbourhood.
We expect oscillations to be detected in ~388 short-cadence targets if the K2
mission continues until C18, which will greatly complement the ~500 detections
of solar-like oscillations made for short-cadence targets during the nominal
Kepler mission. For ~30-40 of these, including several members of the Hyades
open cluster, we furthermore expect that inference from interferometry should
be possible.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present an extension to the general relativistic radiation
magnetohydrodynamic code HARMRAD to account for emission and absorption by
thermal cyclo-synchrotron, double Compton, bremsstrahlung, low-temperature OPAL
opacities as well as Thomson and Compton scattering. We approximate the
radiation field as a Bose-Einstein distribution and evolve it using the
radiation number-energy-momentum conservation equations in order to track
photon hardening. We perform various simulations to study how these extensions
affect the radiative properties of magnetically-arrested disks accreting at
Eddington to super-Eddington rates. We find that double Compton dominates
bremsstrahlung in the disk within a radius of $r\sim 15r_g$ (gravitational
radii) at a hundred times the Eddington accretion rate, and within smaller
radii at lower accretion rates. Double Compton and cyclo-synchrotron regulate
radiation and gas temperatures in the corona, while cyclo-synchrotron regulates
temperatures in the jet. Interestingly, as the accretion rate drops to
Eddington, an optically thin corona develops whose gas temperature of $T\sim
10^9$K is $\sim 100$ times higher than the disk's black body temperature. Our
results show the importance of double Compton and synchrotron in
super-Eddington disks, magnetized coronae, and jets.
</p>
{{{{ARTICLE_PARSER}}}}<p>Fast radio bursts (FRBs) are highly dispersed, sporadic radio pulses that are
likely extragalactic in nature. Here we investigate the constraints on the
source population from surveys carried out at frequencies $&lt;1$~GHz. All but one
FRB has so far been discovered in the 1--2~GHz band, but new and emerging
instruments look set to become valuable probes of the FRB population at sub-GHz
frequencies in the near future. In this paper, we consider the impacts of
free-free absorption and multi-path scattering in our analysis via a number of
different assumptions about the intervening medium. We consider previous low
frequency surveys alongwith an ongoing survey with the University of Technology
digital backend for the Molonglo Observatory Synthesis Telescope (UTMOST) as
well as future observations with the Canadian Hydrogen Intensity Mapping
Experiment (CHIME) and the Hydrogen Intensity and Real-Time Analysis Experiment
(HIRAX). We predict that CHIME and HIRAX will be able to observe $\sim$ 30 or
more FRBs per day, even in the most extreme scenarios where free-free
absorption and scattering can significantly impact the fluxes below 1~GHz. We
also show that UTMOST will detect 1--2 FRBs per month of observations. For
CHIME and HIRAX, the detection rates also depend greatly on the assumed FRB
distance scale. Some of the models we investigated predict an increase in the
FRB flux as a function of redshift at low frequencies. If FRBs are truly
cosmological sources, this effect may impact future surveys in this band,
particularly if the FRB population traces the cosmic star formation rate.
</p>
{{{{ARTICLE_PARSER}}}}<p>Magnetic fields (B-fields) play a key role in the formation and evolution of
protoplanetary disks, but their properties are poorly understood due to the
lack of observational constraints. Using CanariCam at the 10.4-m Gran
Telescopio Canarias, we have mapped out the mid-infrared polarization of the
protoplanetary disk around the Herbig Ae star AB Aur. We detect ~0.44%
polarization at 10.3 micron from AB Aur's inner disk (r &lt; 80 AU), rising to
~1.4% at larger radii. Our simulations imply that the mid-infrared polarization
of the inner disk arises from dichroic emission of elongated particles aligned
in a disk B-field. The field is well ordered on a spatial scale commensurate
with our resolution (~50 AU), and we infer a poloidal shape tilted from the
rotational axis of the disk. The disk of AB Aur is optically thick at 10.3
micron, so polarimetry at this wavelength is probing the B-field near the disk
surface. Our observations therefore confirm that this layer, favored by some
theoretical studies for developing magneto-rotational instability and its
resultant viscosity, is indeed very likely to be magnetized. At radii beyond
~80 AU, the mid-infrared polarization results primarily from scattering by dust
grains with sizes up to ~1 micron, a size indicating both grain growth and,
probably, turbulent lofting of the particles from the disk mid-plane.
</p>
{{{{ARTICLE_PARSER}}}}<p>The census of the Solar neighbourhood is still incomplete, as demonstrated by
recent discoveries of many objects within 5-10 pc from the Sun. The area around
the mid-plane and bulge of the Milky Way presents the most difficulties in
searches for such nearby objects, and is therefore deficient in the known
population. This is largely due to high stellar densities encountered.
Spectroscopic, photometric and kinematic characterization of these objects
allows better understand the local mass function, the binary fraction, and
provides new interesting targets for more detailed studies. We report the
spectroscopic follow-up and characterisation of 12 bright high PM objects,
identified from the VISTA Variables in Via Lactea survey (VVV). We used the
1.9-m telescope of the South African Astronomical Observatory (SAAO) for
low-resolution optical spectroscopy and spectral classification, and the
MPG/ESP 2.2m telescope Fiber-fed Extended Range Optical Spectrograph (FEROS)
high-resolution optical spectroscopy to obtain the radial and space velocities
for three of them. Six of our objects have co-moving companions. We derived
optical spectral types and photometric distances, and classified all of them as
K and M dwarfs within 27-264 pc of the Sun. Finally, we found that one of the
sources, VVV J141421.23-602326.1 (a co-moving companion of VVV
J141420.55-602337.1), appears to be a rare massive white dwarf that maybe close
to the ZZ Ceti instability strip. Many of the objects in our list are
interesting targets for exoplanet searches.
</p>
{{{{ARTICLE_PARSER}}}}<p>The Gaia-Tycho release, scheduled for 14 September, is forecast to yield
parallax errors of $\sigma(\pi)\sim 300\,\mu$as for about 2 million Tycho
stars. We show analytically that the actual performance should be $$
\sigma(\pi) = {\rm max}(\sigma_{1991}/96,20\,\mu{\rm as}) $$ where
$\sigma_{1991}$ is the positional error from the Hipparcos mission. For typical
Tycho stars, $\sigma_{1991}\sim 30\,$mas, so this reproduces the usual claims.
However, for the 100,000 star Hipparcos subset of this sample, $\sigma_{1991}$
is a factor 15 or more smaller. These much lower Hipparcos positional errors
apply even to stars at the Hipparcos-Tycho limit, $V\sim 12$. This is
especially important for RR Lyrae stars, as well as other special classes, that
were systematically included in the Hipparcos catalog down to this limit
because of their exceptional scientific importance. This predicted performance
will provide an early test of the Gaia algorithms.
</p>
{{{{ARTICLE_PARSER}}}}<p>Virial shocks at edges of cosmic-web structures are a clear prediction of
standard structure formation theories. We derive a criterion for the stability
of the post-shock gas and of the virial shock itself in spherical, filamentary
and planar infall geometries. When gas cooling is important, we find that
shocks become unstable, and gas flows uninterrupted towards the center of the
respective halo, filament or sheet. For filaments, we impose this criterion on
self-similar infall solutions. We find that instability is expected for
filament masses between $10^{11}-10^{13}M_\odot Mpc^{-1}.$ Using a simplified
toy model, we then show that these filaments will likely feed halos with
$10^{10}M_{\odot}\lesssim M_{halo}\lesssim 10^{13}M_{\odot}$ at redshift $z=3$,
as well as $10^{12}M_{\odot}\lesssim M_{halo}\lesssim 10^{15}M_{\odot}$ at
$z=0$.
</p>
<p>The instability will affect the survivability of the filaments as they
penetrate gaseous halos in a non-trivial way. Additionally, smaller halos
accreting onto non-stable filaments will not be subject to ram-pressure inside
the filaments. The instreaming gas will continue towards the center, and stop
either once its angular momentum balances the gravitational attraction, or when
its density becomes so high that it becomes self-shielded to radiation.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a detailed investigation of the Large Magellanic Cloud (LMC) disk
using classical Cepheids. Our analysis is based on optical (I,V; OGLE-IV),
near-infrared (NIR: J,H,Ks) and mid-infrared (MIR: w1; WISE) mean magnitudes.
By adopting new templates to estimate the NIR mean magnitudes from single-epoch
measurements, we build the currently most accurate, largest and homogeneous
multi-band dataset of LMC Cepheids. We determine Cepheid individual distances
using optical and NIR Period-Wesenheit relations (PWRs), to measure the
geometry of the LMC disk and its viewing angles. Cepheid distances based on
optical PWRs are precise at 3%, but accurate to 7, while the ones based on NIR
PWRs are more accurate (to 3%), but less precise (2%-15%), given the higher
photometric error on the observed magnitudes. We found an inclination i=25.05
$\pm$ 0.02 (stat.) $\pm$ 0.55 (syst.) deg, and a position angle of the lines of
nodes P.A.=150.76 $\pm$ 0.02(stat.) $\pm$ 0.07(syst.) deg. These values agree
well with estimates based either on young (Red Supergiants) or on
intermediate-age (Asymptotic Giant Branch, Red Clump) stellar tracers, but they
significantly differ from evaluations based on old (RR Lyrae) stellar tracers.
This indicates that young/intermediate and old stellar populations have
different spatial distributions. Finally, by using the reddening-law fitting
approach, we provide a reddening map of the LMC disk which is ten times more
accurate and two times larger than similar maps in the literature. We also
found an LMC true distance modulus of $\mu_{0,LMC}=18.48 \pm 0.10$ (stat. and
syst.) mag, in excellent agreement with the currently most accurate measurement
(Pietrzynski et al. 2013).
</p>
{{{{ARTICLE_PARSER}}}}<p>Modeling of gravitational waves (GWs) from binary black hole inspiral brings
together early post-Newtonian waveforms and late quasinormal ringing waveforms.
Attempts to bridge the two limits without recourse to numerical relativity
involve predicting the time of the peak GW amplitude. This prediction will
require solving the question of why the peak of the \"source,\" i.e., the peak of
the binary angular velocity, does not correspond to the peak of the GW
amplitude. We show here that this offset can be understood as due to the
existence two distinct components of the radiation: the \"direct\" radiation
analogous to that in flat spacetime, and \"scattered\" radiation associated with
curved spacetime. The time dependence of these two components, and of their
relative phases determines the location of the peak amplitude. We use a highly
simplified model to clarify the twocomponent nature of the source, then
demonstrate that the explanation is valid also for an extreme mass ratio binary
inspiral.
</p>
{{{{ARTICLE_PARSER}}}}<p>Modelling of gravitational waves from binary black hole inspiral has played
an important role in the recent observations of such signals. The late-stage
ringdown phase of the gravitational waveform is often associated with the null
particle orbit (\"light ring\") of the black hole spacetime. With simple models
we show that this link between the light ring and spacetime ringing is based
more on the history of specific models than on an actual constraining
relationship. We also show, in particular, that a better understanding of the
dissociation of the two may be relevant to the astrophysically interesting case
of rotating (Kerr) black holes.
</p>
{{{{ARTICLE_PARSER}}}}<p>It has been conjectured that the speed of sound in holographic models with UV
fixed points has an upper bound set by the value of the quantity in conformal
field theory. If true, this would set stringent constraints for the presence of
strongly coupled quark matter in the cores of physical neutron stars, as the
existence of two-solar-mass stars appears to demand a very stiff Equation of
State. In this article, we present a family of counterexamples to the speed of
sound conjecture, consisting of strongly coupled theories at finite density.
The theories we consider include ${\cal N}=4$ super Yang-Mills at finite
R-charge density and non-zero gaugino masses, while the holographic duals are
Einstein-Maxwell theories with a minimally coupled scalar in a charged black
hole geometry. We show that for a small breaking of conformal invariance, the
speed of sound approaches the conformal value from above at large chemical
potentials.
</p>
{{{{ARTICLE_PARSER}}}}<p>The production of electroweak bosons ($Z,\gamma$ and $W$) and charmonia is
sensitive to the initial-state geometry of heavy-ion collisions and to the
parton distribution function with its potential nuclear modification. Since
their leptonic decay products do not interact strongly, their kinematics are
unmodified by the strongly interacting medium, which can be created in a
heavy-ion collision. We report on the latest results of the ATLAS Collaboration
on electroweak boson and charmonia production in $p$+Pb collisions at
$\sqrt{s_{\rm NN}} = 5.02$ TeV. Production yields of $Z$ and $W$ bosons are
presented as a function of (pseudo-)rapidity in different centrality bins. The
forward-backward ratio of J/$\psi$ is shown as a function of transverse
momentum and center-of-mass rapidity.
</p>
{{{{ARTICLE_PARSER}}}}<p>$^{\rm {180m}}$Ta is the rarest naturally occurring quasi-stable isotope and
the longest lived metastable state which is known. Its possible decay via the
$\beta^-$ or the electron capture channel has never been observed. This article
presents a search for the decay of $^{\rm {180m}}$Ta with an ultra low
background Sandwich HPGe gamma spectrometry setup in the HADES underground
laboratory. No signal is observed and improved lower partial half-life limits
are set with a Bayesian analysis to $5.8\cdot10^{16}$ yr for the $\beta^-$
channel and $2.0\cdot10^{17}$ yr for the electron capture channel (90%
credibility). The total half-life of $^{\rm {180m}}$Ta is longer than
$4.5\cdot10^{16}$ yr. This is more than a factor of two improvement compared to
previous searches.
</p>
{{{{ARTICLE_PARSER}}}}<p>The COBRA collaboration investigates 0{\nu}\beta\beta-decays (neutrinoless
double beta-decays). Therefore, a demonstrator setup using coplanar-grid CdZnTe
detectors is operated at the LNGS underground laboratory. In this work, the
demonstrator was commissioned and completed, which is discussed extensively.
The demonstrator works reliably and collects low-background physics data. One
result of the analysis of the data is that surface events are the dominating
background component. To better understand and possibly discriminate this
background, surface events were studied in detail. This was done mainly using
laboratory measurements. For a better interpretation of these measurements,
simulations of particle trajectories and ranges were done. The surface
sensitivity tests showed large differences between the individual detectors.
Often, a dead-layer was determined, especially at the surfaces where the
non-collecting anode (NCA) is the outermost anode rail. Due to this, the
sensitivity of the surfaces where the collecting anode (CA) is adjacent was
typically about a factor of three larger than the NCA sensitivity. A comparison
of the pulse shape analysis methods LSE and A/E was done. Laboratory
measurements indicate, that the latter performs better. Alpha scanning
measurements were done to spatially investigate the surface sensitivity.
Plausible variations were measured. However, no hints were found how to improve
the surface event recognition. The instrumentation of the guard ring, which
surrounds the anode structure, was tested and improved the surface event
discrimination significantly. The fraction of surviving alpha events was at a
per-mill level. Important steps for a future large-scale COBRA experiment are
discussed briefly, mainly the use of an integrated read-out system. Overall,
the results indicate a large potential in background reduction for the COBRA
experiment.
</p>
{{{{ARTICLE_PARSER}}}}<p>Exclusive photoproduction of a $\gamma\,\rho$ pair in the kinematics where
the pair has a large invariant mass and the final nucleon has a small
transverse momentum is described in the collinear factorization framework. The
scattering amplitude is calculated at leading order in $\alpha_s$ and the
differential cross sections for the process where the $\rho-$meson is either
longitudinally or transversely polarized are estimated in the kinematics of the
JLab 12-GeV experiments.
</p>
{{{{ARTICLE_PARSER}}}}<p>Results on the production of $\pi^{\pm}$, $\textrm{K}^{\pm}$,
$\textrm{p}(\bar{\textrm{p}})$, $\Lambda(\bar{\Lambda})$, $\Xi^{-}
\left(\bar{\Xi}^{+}\right)$ and $\Omega^{-} \left(\bar{\Omega}^{+}\right)$ at
midrapidity (${|y|&lt;0.5}$) as a function of multiplicity in
$\sqrt{s}~=~7~\textrm{TeV}$ pp collisions are reported. Transverse momentum
distributions and integrated yields are compared to expectations from
statistical hadronization models along with results from different colliding
systems and center-of-mass energies. The evolution of spectral shapes with
multiplicity show similar patterns to those seen in p-Pb and Pb-Pb collisions.
The $p_{\textrm{T}}$-integrated baryon yields relative to pions exhibit a
significant strangeness-related enhancement in both pp and p-Pb collisions.
</p>
{{{{ARTICLE_PARSER}}}}<p>We report the first di-jet transverse momentum asymmetry measurements from
Au+Au and p+p collisions at RHIC. The two highest-energy back-to-back jets
reconstructed from fragments with transverse momenta above 2 GeV/c display a
significantly stronger momentum imbalance in heavy-ion collisions than in the
p+p reference. When re-examined with correlated soft particles included, we
observe that these di-jets then exhibit a unique new feature -- momentum
balance is restored to that observed in p+p for a jet resolution parameter of
R=0.4, while re-balancing is not attained with a smaller value of R=0.2.
</p>
{{{{ARTICLE_PARSER}}}}<p>We analyze recent $K^+$ meson photoproduction data from the CLAS
collaboration for the reactions $\gamma p\rightarrow K^+\Lambda$ and $\gamma
p\rightarrow K^+\Sigma_0$, fitting measured forward-angle differential cross
sections to the form $Ae^{Bt}$. We develop a quantitative scheme for
determining the kinematic region where the fit is to be done, and, from the
extracted $t$-slope $B$, determine whether single-Reggeon exchange can explain
the production mechanism. We find that, in the region $5 &lt; s &lt; 8.1$ GeV$^2$,
production of the $K^+\Lambda$ channel can be explained by single $K^+$ Reggeon
exchange, but the $K^+\Sigma_0$ production channel cannot. We verify these
conclusions by fitting the data to a differential cross section produced by the
interfering sum of two exponential amplitudes.
</p>
{{{{ARTICLE_PARSER}}}}<p>We introduce a new technology of detectors for the search of the neutrinoless
double beta decay of $^{82}$Se. Based on the present literature, imaging
devices from amorphous $^{82}$Se evaporated on a complementary
metal-oxide-semiconductor (CMOS) active pixel array are expected to have the
energy and spatial resolution to produce two-dimensional images of ionizing
tracks of utmost quality, effectively akin to an electronic bubble chamber in
the double beta decay energy regime. Still to be experimentally demonstrated, a
detector consisting of a large array of these devices could have very low
backgrounds, possibly reaching $10^{-7}$/(kg y) in the neutrinoless decay
region of interest (ROI), as it may be required for the full exploration of the
neutrinoless double beta decay parameter space in the most unfavorable
condition of a strongly quenched nucleon axial coupling constant.
</p>
{{{{ARTICLE_PARSER}}}}<p>The production of beauty hadrons was measured via semi-leptonic decays at
mid-rapidity with the ALICE detector at the LHC in the transverse momentum
interval $1&lt;p_{\rm T}&lt;8$ GeV/$c$ in minimum-bias p-Pb collisions at
$\sqrt{s_{\rm NN}}=5.02$ TeV and in $1.3&lt;p_{\rm T}&lt;8$ GeV/$c$ in the 20% most
central Pb-Pb collisions at $\sqrt{s_{\rm NN}}=2.76$ TeV. The pp reference
spectra at $\sqrt{s}=5.02$ TeV and $\sqrt{s}=2.76$ TeV, needed for the
calculation of the nuclear modification factors $R_{\rm pPb}$ and $R_{\rm
PbPb}$, were obtained by a pQCD-driven scaling of the cross section of
electrons from beauty-hadron decays measured at $\sqrt{s}=7$ TeV. The $R_{\rm
PbPb}$ is about 0.7 with an uncertainty of about 30% in the interval $3&lt;p_{\rm
T}&lt;6$ GeV/$c$ and 0.47 with an uncertainty of 25% in $6&lt;p_{\rm T}&lt;8$ GeV/$c$ in
Pb-Pb collisions. Below $p_{\rm T}=3$ GeV/$c$, the $R_{\rm PbPb}$ values
increase with decreasing transverse momentum with systematic uncertainties of
30-45%. The $R_{\rm pPb}$ is consistent with unity within systematic
uncertainties of about 20% at high $p_{\rm T}$, increasing at low $p_{\rm T}$,
and is well described by theoretical calculations that include cold nuclear
matter effects in p-Pb collisions. The measured $R_{\rm pPb}$ and these
calculations indicate that cold nuclear matter effects are small at high
transverse momentum also in Pb-Pb collisions. Therefore, the observed reduction
of $R_{\rm PbPb}$ below unity for high $p_{\rm T}$ can be ascribed to an effect
of the hot and dense medium formed in Pb-Pb collisions.
</p>
{{{{ARTICLE_PARSER}}}}<p>We perform a combined analysis of recent NEOS and Daya Bay data on the
reactor antineutrino spectrum. This analysis includes approximately 1.5 million
antineutrino events, which is the largest neutrino event sample analyzed to
date. We use a double ratio which cancels flux model dependence and related
uncertainties as well as the effects of the detector response model. We find at
3-4 standard deviation significance level, that plutonium-239 and plutonium-241
are disfavored as the single source for the the so-called 5 MeV bump. This
analysis method has general applicability and in particular with higher
statistics data sets will be able to shed significant light on the issue of the
bump. With some caveat this also should allow to improve the sensitivity for
sterile neutrino searches in NEOS.
</p>
{{{{ARTICLE_PARSER}}}}<p>Searches for rare nuclear processes, such as neutrinoless double beta-decay
and the interactions of WIMP dark matter, are motivating experiments with
ever-decreasing levels of radioactive backgrounds. These background reductions
are achieved using various techniques, but amongst the most important is
minimizing radioactive contamination in the materials from which the experiment
is constructed. To this end there have been decades of advances in material
sourcing, manufacture and certification, during which researchers have
accumulated many thousands of measurements of material radiopurity. Some of
these assays are described in publications, others are in databases, but many
are still communicated informally. Until this work, there has been no standard
format for encoding assay results and no effective, central location for
storing them. The aim of this work is to address these long-standing problems
by creating a concise and flexible material assay data format and powerful
software application to manipulate it. A public installation of this software,
available at <a href=\"http://www.radiopurity.org\">this http URL</a>, is the largest database of assay
results ever compiled and is intended as a long-term repository for the
community's data.
</p>
{{{{ARTICLE_PARSER}}}}<p>We analyse the rare kaon decays $K_S \to \gamma\gamma$ and $K_S \to
\gamma\ell^+\ell^-$ $(\ell = e \mbox{ or } \mu)$ in a dispersive framework in
which the weak Hamiltonian carries momentum. Our analysis extends predictions
from lowest order $SU(3)_L\times SU(3)_R$ chiral perturbation theory
($\chi$PT$_3$) to fully account for effects from final-state interactions, and
is free from ambiguities associated with extrapolating the kaon off-shell.
Given input from $K_S \to \pi\pi$ and $\gamma\gamma^{(*)}\to\pi\pi$, we solve
the once-subtracted dispersion relations numerically to predict the rates for
$K_S \to \gamma\gamma$ and $K_S \to \gamma\ell^+\ell^-$. In the leptonic modes,
we find sizeable corrections to the $\chi$PT$_3$ predictions for the integrated
rates.
</p>
{{{{ARTICLE_PARSER}}}}<p>An analysis is presented of the expectations of the thermal model for
particle production in collisions of small nuclei. The maxima observed in
particle ratios of strange particles to pions as a function of beam energy in
heavy ion collisions, are reduced when considering smaller nuclei. Of
particular interest is the $\Lambda/\pi^+$ ratio shows the strongest maximum
which survives even in collisions of small nuclei.
</p>
{{{{ARTICLE_PARSER}}}}<p>Gerry Brown was a godfather of our hidden local symmetry (HLS) for the vector
meson from the birth of the theory throughout his life. The HLS is originated
from very nature of the nonlinear realization of the symmetry G based on the
manifold G/H, and thus is universal to any physics based on the nonlinear
realization. Here I focus on the Higgs Lagrangian of the Standard Model (SM),
which is shown to be equivalent to the nonlinear sigma model based on G/H=
SU(2)_L x SU(2)_R/SU(2)_V with additional symmetry, the nonlinearly realized
scale symmetry. Then the SM does have a dynamical gauge boson of the SU(2)_V
HLS, \"SM rho meson\", in addition to the Higgs as a pseudo dilaton as well as
the NG bosons to be absorbed into the W and Z. Based on the recent work done
with S. Matsuzaki and H. Ohki, I discuss a novel possibility that the SM rho
meson acquires kinetic term by the SM dynamics itself, which then stabilizes
the skyrmion dormant in the SM as a viable candidate for the dark matter, what
we call \"Dark SM skyrmion (DSMS)\".
</p>
{{{{ARTICLE_PARSER}}}}<p>A method is presented to evaluate the particle-phonon coupling (PC)
corrections to the single-particle energies (SPEs) in semi-magic nuclei. In
such nuclei always there is a collective low-lying $2^+$ phonon, and a strong
mixture of single-particle and particle-phonon states often occurs. As in magic
nuclei, the so-called $g^2_L$ approximation, where $g_L$ is the vertex of the
$L$-phonon creation, can be used for finding the PC correction $\delta
\Sigma^{\rm PC}(\varepsilon)$ to the initial mass operator $\Sigma_0$. In
addition to the usual pole diagram, the phonon \"tadpole\" diagram is also taken
into account. In semi-magic nuclei, the perturbation theory in $\delta
\Sigma^{\rm PC}(\varepsilon)$ with respect to $\Sigma_0$ is often invalid for
finding the PC corrected SPEs. Instead, the Dyson equation with the mass
operator $\Sigma(\varepsilon){=}\Sigma_0{+}\delta \Sigma^{\rm PC}(\varepsilon)$
is solved directly, without any use of the perturbation theory. Results for a
chain of semi-magic Pb isotopes are presented.
</p>
{{{{ARTICLE_PARSER}}}}<p>Poincare' covariant definitions for the spin-dependent spectral function and
for the momentum distributions within the light-front Hamiltonian dynamics are
proposed for a three-fermion bound system, starting from the light-front wave
function of the system. The adopted approach is based on the Bakamjian-Thomas
construction of the Poincare' generators, that allows one to easily import the
familiar and wide knowledge on the nuclear interaction into a light-front
framework. The proposed formalism can find useful applications in refined
nuclear calculations, like the ones needed for evaluating the EMC effect or the
semi-inclusive deep inelastic cross sections with polarized nuclear targets,
since remarkably the light-front unpolarized momentum distribution by
definition fulfills both normalization and momentum sum rules. It is also shown
a straightforward generalization of the definition of the light-front spectral
function to an A-nucleon system.
</p>
{{{{ARTICLE_PARSER}}}}<p>Exclusive photoproduction of a $\gamma\,\rho$ pair in the kinematics where
the pair has a large invariant mass and the final nucleon has a small
transverse momentum is described in the collinear factorization framework. The
scattering amplitude is calculated at leading order in $\alpha_s$ and the
differential cross sections for the process where the $\rho-$meson is either
longitudinally or transversely polarized are estimated in the kinematics of the
JLab 12-GeV experiments.
</p>
{{{{ARTICLE_PARSER}}}}<p>We investigate the dynamics of a quantized vortex and a nuclear impurity
immersed in a neutron superfluid within a fully microscopic time-dependent
three-dimensional approach. The magnitude and even the sign of the force
between the quantized vortex and the nuclear impurity have been a matter of
debate for over four decades. We determine that the vortex and the impurity
repel at neutron densities, 0.014 fm$^{-3}$ and 0.031 fm$^{-3}$, which are
relevant to the neutron star crust and the origin of glitches, while previous
calculations have concluded that the force changes its sign between these two
densities and predicted contradictory signs. The magnitude of the force
increases with the density of neutron superfluid, while the magnitude of the
pairing gap decreases in this density range.
</p>
{{{{ARTICLE_PARSER}}}}<p>The $\phi$-meson properties in cold nuclear matter are investigated by
implementing resonant $\phi N$ interactions as described in effective
approaches including the unitarization of scattering amplitudes. Several
$N^*$-like states are dynamically generated in these models around $2$ GeV, in
the vicinity of the $\phi N$ threshold. We find that both these states and the
non-resonant part of the amplitude contribute sizably to the $\phi$ collisional
self-energy at finite nuclear density. These contributions are of a similar
strength as the widely studied medium effects from the $\bar K K$ cloud.
Depending on model details (position of the resonances and strength of the
coupling to $\phi N$) we report a $\phi$ broadening up to about $40$-$50$ MeV,
to be added to the $\phi\to\bar K K$ in-medium decay width, and an attractive
optical potential at threshold up to about $35$ MeV at normal matter density.
The $\phi$ spectral function develops a double peak structure as a consequence
of the mixing of resonance-hole modes with the $\phi$ quasi-particle peak. The
former results point in the direction of making up for missing absorption as
reported in $\phi$ nuclear production experiments.
</p>
{{{{ARTICLE_PARSER}}}}<p>We perform a combined analysis of recent NEOS and Daya Bay data on the
reactor antineutrino spectrum. This analysis includes approximately 1.5 million
antineutrino events, which is the largest neutrino event sample analyzed to
date. We use a double ratio which cancels flux model dependence and related
uncertainties as well as the effects of the detector response model. We find at
3-4 standard deviation significance level, that plutonium-239 and plutonium-241
are disfavored as the single source for the the so-called 5 MeV bump. This
analysis method has general applicability and in particular with higher
statistics data sets will be able to shed significant light on the issue of the
bump. With some caveat this also should allow to improve the sensitivity for
sterile neutrino searches in NEOS.
</p>
{{{{ARTICLE_PARSER}}}}<p>It is shown that the excellent scaling of the elliptic flow found for all
centralities, species and energies from RHIC to the LHC for $p_{T}$ less than
the saturation momentum is a consequence of the energy lost by a parton
interacting with the color field produced in a nucleus-nucleus collision. In
particular, the deduced shape of the scaling curve describes correctly all the
data. We discuss the possible extensions to higher $p_{T}$, proton-nucleus and
proton-proton collisions as well as higher harmonics.
</p>
{{{{ARTICLE_PARSER}}}}<p>An extension of the ideal non-interacting hadron resonance gas (HRG) model is
constructed which includes the attractive and repulsive van der Waals (VDW)
interactions between baryons. This VDW-HRG model yields the nuclear liquid-gas
transition at low temperatures and high baryon densities. The VDW parameters
$a$ and $b$ are fixed by the ground state properties of nuclear matter, and the
temperature dependence of various thermodynamic observables at zero chemical
potential are calculated within VDW-HRG model. Compared to the ideal
non-interacting HRG, the inclusion of VDW interactions between baryons leads to
a qualitatively different behavior of 2nd and higher moments of fluctuations of
conserved charges, in particular in the so-called crossover region $T \sim 140
\div 190$ MeV. For many observables this behavior resembles closely the results
obtained from lattice QCD simulations. These results imply that VDW
interactions play a crucial role in thermodynamics of hadron gas. Thus, the
commonly performed comparisons of the ideal HRG model with the lattice data may
lead to misconceptions and misleading conclusions, and should therefore be
treated with extreme care.
</p>
{{{{ARTICLE_PARSER}}}}<p>Lattice QCD calculations with background magnetic fields are used to
determine the magnetic moments of the octet baryons. Computations are performed
at the physical value of the strange quark mass, and two values of the light
quark mass, one corresponding to the SU(3) flavor-symmetric point, where the
pion mass is ~ 800 MeV, and the other corresponding to a pion mass ~ 450 MeV.
The moments are found to exhibit only mild pion-mass dependence when expressed
in terms of appropriately chosen magneton units---the natural baryon magneton.
This suggests that simple extrapolations can be used to determine magnetic
moments at the physical point, and extrapolated results are found to agree with
experiment within uncertainties. A curious pattern is revealed among the
anomalous baryon magnetic moments which is linked to the constituent quark
model, however, careful scrutiny exposes additional features. Relations
expected to hold in the large-Nc limit of QCD are studied; and, in one case,
the quark model prediction is significantly closer to the extracted values than
the large-Nc prediction. The magnetically coupled Lambda-Sigma system is
treated in detail at the SU(3) point, with the lattice QCD results comparing
favorably with predictions based on SU(3) symmetry. This analysis enables the
first extraction of the isovector transition magnetic polarizability. The
possibility that large magnetic fields stabilize strange matter is explored,
but such a scenario is found to be unlikely.
</p>
{{{{ARTICLE_PARSER}}}}<p>In the context of f(R) gravity theories, we show that the apparent mass of a
neutron star as seen from an observer at infinity is numerically calculable but
requires careful matching, first at the star's edge, between interior and
exterior solutions, none of them being totally Schwarzschild-like but
presenting instead small oscillations of the curvature scalar R; and second at
large radii, where the Newtonian potential is used to identify the mass of the
neutron star. We find that for the same equation of state, this mass definition
is always larger than its general relativistic counterpart. We exemplify this
with quadratic $R^2$ and Hu-Sawicki-like modifications of the standard General
Relativity action. Therefore, the finding of two-solar mass neutron stars
basically imposes no constraint on stable f(R) theories. However, star radii
are in general smaller than in General Relativity, which can give an
observational handle on such classes of models at the astrophysical level. Both
larger masses and smaller matter radii are due to much of the apparent
effective energy residing in the outer metric for scalar-tensor theories.
Finally, because the f(R) neutron star masses can be much larger than General
Relativity counterparts, the total energy available for radiating gravitational
waves could be of order several solar masses, and thus a merger of these stars
constitutes an interesting wave source.
</p>
{{{{ARTICLE_PARSER}}}}<p>It has been conjectured that the speed of sound in holographic models with UV
fixed points has an upper bound set by the value of the quantity in conformal
field theory. If true, this would set stringent constraints for the presence of
strongly coupled quark matter in the cores of physical neutron stars, as the
existence of two-solar-mass stars appears to demand a very stiff Equation of
State. In this article, we present a family of counterexamples to the speed of
sound conjecture, consisting of strongly coupled theories at finite density.
The theories we consider include ${\cal N}=4$ super Yang-Mills at finite
R-charge density and non-zero gaugino masses, while the holographic duals are
Einstein-Maxwell theories with a minimally coupled scalar in a charged black
hole geometry. We show that for a small breaking of conformal invariance, the
speed of sound approaches the conformal value from above at large chemical
potentials.
</p>
{{{{ARTICLE_PARSER}}}}<p>We address small volume-fraction asymptotic properties of a nonlocal
isoperimetric functional with a confinement term, derived as the sharp
interface limit of a variational model for self-assembly of diblock copolymers
under confinement by nanoparticle inclusion. Following Choksi and Peletier, we
introduce a small parameter $\eta$ to represent the size of the domains of the
minority phase, and study the resulting droplet regime as $\eta\to 0$. By
considering confinement densities which are spatially variable and attain a
nondegenerate maximum, we present a two-stage asymptotic analysis in the sense
of $\Gamma$-convergence wherein a separation of length scales is captured due
to competition between the nonlocal repulsive and confining attractive effects
in the energy. A key role is played by a parameter $M$ which gives the total
volume of the droplets at order $\eta^3$ and its relation to existence and
non-existence of a recently well-studied nonlocal isoperimetric functional on
$\mathbb{R}^3$. For large values of $M$, the minority phase splits into several
droplets at an intermediate scale $\eta^{1/3}$, while for small $M$ minimizers
form a single droplet converging to the maximum of the confinement density.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study the radial part of the Dunkl-Coulomb problem in two dimensions and
show that this problem possesses the $su(1,1)$ symmetry. We introduce two
different realizations for the $su(1,1)$ Lie algebra and use the theory of
irreducible representations to obtain the energy spectrum and the
eigenfunctions. For the first algebra realization, we apply the Schr\\"odinger
factorization to the radial part of the Dunkl-Coulomb problem to construct the
algebra generators. In the second realization, we introduce three operators,
been one of them proportional to the radial Hamiltonian. Finally, we use the
$su(1,1)$ Sturmian basis of one of the two algebras to construct the radial
coherent states in a closed form.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider smooth moduli spaces of semistable vector bundles of fixed rank
and determinant on a compact Riemann surface $X$ of genus at least $3$. The
choice of a Poincar\'e bundle for such a moduli space $M$ induces an
isomorphism between $X$ and a component of the moduli space of semistable
sheaves over $M$. We prove that $h^0(M, \text{End}({\mathcal E})\otimes TM)= 1$
for a vector bundle $\mathcal E$ on $M$ coming from this component.
Furthermore, there are no nonzero integrable co-Higgs fields on $\mathcal E$.
</p>
{{{{ARTICLE_PARSER}}}}<p>Grad's moment models for Boltzmann equation were recently regularized to
globally hyperbolic systems, and thus the regularized models attain local
well-posedness for Cauchy data. The hyperbolic regularization is only related
to the convection term in Boltzmann equation. We in this paper studied the
regularized models with the presentation of collision terms. It is proved that
the regularized models are linear stability at the local equilibrium and
satisfy Yong's first stability condition with commonly used approximate
collision terms, and particularly with Boltzmann's binary collision model.
</p>
{{{{ARTICLE_PARSER}}}}<p>We reformulate gauge theories in analogy with the vierbein formalism of
general relativity. More specifically, we reformulate gauge theories such that
their gauge dynamical degrees of freedom are local fields that transform
linearly under the dual representation of the charged matter field. These local
fields, which naively have the interpretation of non-local operators similar to
Wilson lines, satisfy constraint equations. A set of basis tensor fields are
used to solve these constraint equations, and their field theory is
constructed. A new local symmetry in terms of the basis tensor fields is used
to make this field theory local and maintain a Hamiltonian that is bounded from
below. The field theory of the basis tensor fields is what we call the basis
tensor gauge theory.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider the statistical problem of `compressive' estimation of low rank
states with random basis measurements, where the estimation error is expressed
terms of two metrics - the Frobenius norm and quantum infidelity. It is known
that unlike the case of general full state tomography, low rank states can be
identified from a reduced number of observables' expectations. Here we
investigate whether for a fixed sample size $N$, the estimation error
associated to a `compressive' measurement setup is `close' to that of the
setting where a large number of bases are measured. In terms of the Frobenius
norm, we demonstrate that for all states the error attains the optimal rate
$rd/N$ with only $O(r \log{d})$ random basis measurements. We provide an
illustrative example of a single qubit and demonstrate a concentration in the
Frobenius error about its optimal for all qubit states. In terms of the quantum
infidelity, we show that such a concentration does not exist uniformly over all
states. Specifically, we show that for states that are nearly pure and close to
the surface of the Bloch sphere, the mean infidelity scales as $1/\sqrt{N}$ but
the constant converges to zero as the number of settings is increased. This
demonstrates a lack of `compressive' recovery for nearly pure states in this
metric.
</p>
{{{{ARTICLE_PARSER}}}}<p>We show that for $\beta \ge 1$ the semigroups of $\beta$ Laguerre and $\beta$
Jacobi processes of different dimensions are intertwined in analogy to a
similar result for $\beta$ Dyson Brownian motion recently obtained by Ramanan
and Shkolnikov. These intertwining relations generalize to arbitrary $\beta \ge
1$ the ones obtained for $\beta=2$ by the author, O'Connell and Warren between
$h$-transformed Karlin-McGregor semigroups. Moreover they form the key first
step towards constructing a multilevel process in a Gelfand Tsetlin pattern.
Finally as a by product we obtain a relation between general $\beta$ Jacobi
ensembles of different dimensions.
</p>
{{{{ARTICLE_PARSER}}}}<p>Let $P$ be a parabolic subgroup of a connected simply connected complex
semisimple Lie group $G$. Given a compact K\\"ahler manifold $X$, the
dimensional reduction of $G$-equivariant holomorphic vector bundles over
$X\times G/P$ was carried out by the first and third authors. This raises the
question of dimensional reduction of holomorphic principal bundles over
$X\times G/P$. The method used for equivariant vector bundles does not
generalize to principal bundles. In this paper, we adapt to equivariant
principal bundles the Tannakian approach of Nori, to describe the dimensional
reduction of $G$-equivariant principal bundles over $X\times G/P$, and to
establish a Hitchin--Kobayashi type correspondence. In order to be able to
apply the Tannakian theory, we need to assume that $X$ is a complex projective
manifold.
</p>
{{{{ARTICLE_PARSER}}}}<p>We show that using the family of adapted K\\"ahler polarizations of the phase
space of an irreducible, simply connected, compact, Riemannian symmetric space,
the field $H^{corr}$ of quantum Hilbert spaces produced by geometric
quantization including the half-form correction is flat if $M$ is a compact Lie
group equipped with a biinvariant metric and not even projectively flat
otherwise. The result generalizes a similar statement for rank-1 spaces that
was proved in a joint paper with L. Lempert.
</p>
{{{{ARTICLE_PARSER}}}}<p>We apply a definition of generalised super Calabi-Yau variety (SCY) to
supermanifolds of complex dimension one. We get that the class of all SCY's of
bosonic dimension one and reduced manifold equal to $\mathbb{P}^1$ is given by
$\mathbb{P}^{1|2} $ and the weighted projective super space
$\mathbb{WP}^{1|1}_{(2)}$. Then we compute the corresponding sheaf cohomology
of superforms, showing that the cohomology with picture number one is infinite
dimensional, while the de Rham cohomology remains finite dimensional. Moreover,
we provide the complete real and holomorphic de Rham cohomology for generic
projective super spaces $\mathbb P^{n|m}$. We also determine the automorphism
groups, which for $\mathbb{P}^{1|2} $ results to be larger than the projective
supergroup. Finally, we show that $\mathbb{P}^{1|2} $ is self mirror, whereas
$\mathbb{WP} ^{1|1}_{(2)}$ has a zero dimensional mirror. The mirror map for
$\mathbb{P}^{1|2}$ endows it with a structure of $N=2$ super Riemann surface.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider general N-particle wave functions which have the form of a
product of the Laughlin state with filling factor $1/\ell$ and an analytic
function of the N variables. This is the most general form of a wave function
that can arise through a perturbation of the Laughlin state by external
potentials or impurities, while staying in the lowest Landau level and
maintaining the strong correlations of the original state. We show that the
perturbation can only shift or lower the 1-particle density but nowhere
increase it above a maximum value. Regardless of the analytic prefactor, the
density satisfies the same bound as the Laughlin function itself, i.e. $(\pi
\ell) ^{-1}$ in the limit of large particle number. Consequences of this
incompressibility bound for the response of the Laughlin state to external
fields are discussed. Our theorems apply equally to bosonic and fermionic
states.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we study a coupled nonlinear Schr{\\"o}dinger system with small
initial data in a product space. We establish a modified scattering of the
solutions of this system and we construct a modified wave operator. The study
of the resonant system, which provides the asymptotic dynamics, allows us to
highlight a control of the Sobolev norms and interesting dynamics with the
beating effect. The proof uses a recent work of Hani, Pausader, Tzvetkov and
Visciglia for the modified scattering, and a recent work of Gr{\'e}bert,
Paturel and Thomann for the study of the resonant system.
</p>
{{{{ARTICLE_PARSER}}}}<p>For general dimension $d$ we prove the equidistribution of energy at the
micro-scale in $\mathbb R^d$, for the optimal point configurations appearing in
Coulomb gases at zero temperature. At the microscopic scale, i.e. after blow-up
at the scale corresponding to the interparticle distance, in the case of
Coulomb gases we show that the energy concentration is precisely determined by
the macroscopic density of points, independently of the scale. This uses the
\"jellium energy\" which was previously shown to control the next-order term in
the large particle number asymptotics of the minimum energy. As a corollary, we
obtain sharp error bounds on the discrepancy between the number of points and
its expected average of optimal point configurations for Coulomb gases,
extending previous results valid only for $2$-dimensional log-gases. For Riesz
gases with interaction potentials $g(x)=|x|^{-s}, s\in]\min\{0,d-2\},d[$ and
one-dimensional log-gases, we prove the same equidistribution result under an
extra hypothesis on the decay of the localized energy, which we conjecture to
hold for minimizing configurations. In this case we use the
Caffarelli-Silvestre description of the non-local fractional Laplacians in
$\mathbb R^d$ to localize the problem.
</p>
{{{{ARTICLE_PARSER}}}}<p>Second-order conformal quantum superintegrable systems in 2 dimensions are
Laplace equations on a manifold, with an added scalar potential and 3
independent 2nd order conformal symmetry operators. They encode all the
information about 2D Helmholtz or time-independent Schroedinger superintegrable
systems in an efficient manner: Each of these systems admits a quadratic
symmetry algebra (not usually a Lie algebra) and is multiseparable. The
separation equations comprise all of the various types of hypergeometric and
Heun equations in full generality. In particular, they yield all of the 1D
Schroedinger exactly solvable (ES) and quasi-exactly solvable (QES) systems
related to the Heun operator. The separable solutions of these equations are
the special functions of mathematical physics. The different systems are
related by Staeckel transforms, by the symmetry algebras and by Bocher
contractions of the conformal algebra so(4,C) to itself, which enables all
systems to be derived from a single one: the generic potential on the complex
2-sphere. Distinct separable bases for a single Laplace system are related by
interbasis expansion coefficients which are themselves special functions, such
as the Wilson polynomials. Applying Bocher contractions to expansion
coefficients for ES systems one can derive the Askey scheme for hypergeometric
orthogonal polynomials. This approach facilitates a unified view of special
function theory, incorporating hypergeometric and Heun functions in full
generality.
</p>
{{{{ARTICLE_PARSER}}}}<p>Recent inapproximability results of Sly (2010), together with an
approximation algorithm presented by Weitz (2006) establish a beautiful picture
for the computational complexity of approximating the partition function of the
hard-core model. Let $\lambda_c(T_\Delta)$ denote the critical activity for the
hard-model on the infinite $\Delta$-regular tree. Weitz presented an FPTAS for
the partition function when $\lambda&lt;\lambda_c(T_\Delta)$ for graphs with
constant maximum degree $\Delta$. In contrast, Sly showed that for all
$\Delta\geq 3$, there exists $\epsilon_\Delta&gt;0$ such that (unless RP=NP) there
is no FPRAS for approximating the partition function on graphs of maximum
degree $\Delta$ for activities $\lambda$ satisfying
$\lambda_c(T_\Delta)&lt;\lambda&lt;\lambda_c(T_\Delta)+\epsilon_\Delta$.
</p>
<p>We prove that a similar phenomenon holds for the antiferromagnetic Ising
model. Recent results of Li et al. and Sinclair et al. extend Weitz's approach
to any 2-spin model, which includes the antiferromagnetic Ising model, to yield
an FPTAS for the partition function for all graphs of constant maximum degree
$\Delta$ when the parameters of the model lie in the uniqueness regime of the
infinite tree $T_\Delta$. We prove the complementary result that for the
antiferrogmanetic Ising model without external field that, unless RP=NP, for
all $\Delta\geq 3$, there is no FPRAS for approximating the partition function
on graphs of maximum degree $\Delta$ when the inverse temperature lies in the
non-uniqueness regime of the infinite tree $T_\Delta$. Our results extend to a
region of the parameter space for general 2-spin models. Our proof works by
relating certain second moment calculations for random $\Delta$-regular
bipartite graphs to the tree recursions used to establish the critical points
on the infinite tree.
</p>
{{{{ARTICLE_PARSER}}}}<p>The Polyakov relation, which in the sphere topology gives the changes of the
Liouville action under the variation of the position of the sources, in the
case of higher genus is related also to the dependence of the action on the
moduli of the surface. We write and prove such a relation for genus 1 and for
all hyperelliptic surfaces.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider the problem of non degenerate in energy metastable states forming
a series in the framework of reversible finite state space Markov chains. We
assume that starting from the state at higher energy the system necessarily
visits the second one before reaching the stable state. In this framework, we
give a sharp estimate of the exit time from the metastable state at higher
energy and, on the proper exponential time scale, we prove an addition rule. As
an application of the theory, we study the Blume-Capel model in the zero
chemical potential case.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider Yang-Mills theory with $N=2$ super translation group in $d=10$
auxiliary dimensions as the structure group. The gauge theory is defined on a
direct product manifold $\Sigma_2\times H^2$, where $\Sigma_2$ is a
two-dimensional Lorentzian manifold and $H^2$ is the open disc in
$\mathbb{R}^2$ with the boundary $S^1=\partial H^2$. We show that in the
adiabatic limit, when the metric on $H^2$ is scaled down, the Yang-Mills action
supplemented by the $d=5$ Chern-Simons term becomes the Green-Schwarz
superstring action. More concretely, the Yang-Mills action in the infrared
limit flows to the kinetic part of the superstring action and the $d=5$
Chern-Simons action, defined on a 5-manifold with the boundary $\Sigma_2\times
H^2$, flows to the Wess-Zumino part of the superstring action. The same kind of
duality between gauge fields and strings is established for type IIB
superstring on AdS$_5\times S^5$ background and a supergroup gauge theory with
PSU(2,2$|$4) as the structure group.
</p>
{{{{ARTICLE_PARSER}}}}<p>The state space of an operator system of $n$-by-$n$ matrices has, in a sense,
many normal cones. Merely this convex geometrical property implies smoothness
qualities and a clustering property of exposed faces. The latter holds since
each exposed face is an intersection of maximal exposed faces. An isomorphism
translates these results to the lattice of ground state projections of the
operator system. We work on minimizing the assumptions under which a convex set
has the mentioned properties.
</p>
{{{{ARTICLE_PARSER}}}}<p>Using the worldline formalism of the Dirac field with a non-Abelian gauge
symmetry we show how to describe the matter field transforming in an arbitrary
representation of the gauge group. Colour degrees of freedom are carried on the
worldline by auxiliary fields, responsible for providing path ordering and the
Wilson-loop coupling. The Hilbert space of these fields is reducible but we
make use of recent work in order to project onto a single, arbitrary,
irreducible representation. By functionally quantising the resulting theory we
show that this procedure correctly generates the Wilson-loop interaction
between the gauge field and the matter field taken to transform in a chosen
representation. This work has direct application to physical observables such
as scattering amplitudes in the presence of such a matter multiplet and lifts
the restriction on the type of matter that has previously featured in worldline
calculations.
</p>
{{{{ARTICLE_PARSER}}}}<p>We call \emph{Alphabet model} a generalization to N types of particles of the
classic ABC model. We have particles of different types stochastically evolving
on a one dimensional lattice with an exchange dynamics. The rates of exchange
are local but under suitable conditions the dynamics is reversible with a
Gibbsian like invariant measure with long range interactions. We discuss
geometrically the conditions of reversibility on a ring that correspond to a
gradient condition on the graph of configurations or equivalently to a
divergence free condition on a graph structure associated to the types of
particles. We show that much of the information on the interactions between
particles can be encoded in associated \emph{Tournaments} that are a special
class of oriented directed graphs. In particular we show that interactions that
corresponds to reversible model are corresponding to strongly connected
tournaments. The possible minimizers of the energies are in correspondence with
the Hamiltonian cycles of the tournaments. We can then determine how many and
which are the possible minimizers of the energy looking at the structure of the
associated tournament. As a byproduct we obtain a probabilistic proof of a
classic Theorem of Camion \cite{Camion} on the existence of Hamiltonian cycles
for strongly connected tournaments. Using these results we obtain in the case
of an equal number of k types of particles new representations of the
Hamiltonians in terms of $k$-body long range interactions. We show that when
$k=3,4$ the minimizer of the energy is always unique up to translations.
Starting from the case $k=5$ it is possible to have more than one minimizer. In
particular it is possible to have minimizers for which particles of the same
type are not joined together in single clusters.
</p>
{{{{ARTICLE_PARSER}}}}<p>We introduce a notion of Real bundle gerbes on manifolds equipped with an
involution. We elucidate their relation to Jandl gerbes and prove that they are
classified by their Real Dixmier-Douady class in Grothendieck's equivariant
sheaf cohomology. We show that the Grothendieck group of Real bundle gerbe
modules is isomorphic to twisted KR-theory for a torsion Real Dixmier-Douady
class. Building on the Baum-Douglas model for K-homology and the orientifold
construction in string theory, we introduce geometric cycles for twisted
KR-homology groups using Real bundle gerbe modules. We prove that this defines
a real-oriented generalised homology theory dual to twisted KR-theory for Real
closed manifolds, and more generally for Real finite CW-complexes, for any Real
Dixmier-Douady class. This is achieved by defining an explicit natural
transformation to analytic twisted KR-homology and proving that it is an
isomorphism. Our constructions give a new framework for the classification of
orientifolds in string theory, providing precise conditions for orientifold
lifts of H-fluxes and for orientifold projections of open string states.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this work, we introduce classical holographic codes. These can be
understood as concatenated probabilistic codes and can be represented as
networks uniformly covering hyperbolic space. In particular, classical
holographic codes can be interpreted as maps from bulk degrees of freedom to
boundary degrees of freedom. Interestingly, they are shown to exhibit features
similar to those expected from the AdS/CFT correspondence. Among these are a
version of the Ryu-Takayanagi formula and intriguing properties regarding bulk
reconstruction and boundary representations of bulk operations. We discuss the
relation of our findings with expectations from AdS/CFT and, in particular,
with recent results from quantum error correction.
</p>
{{{{ARTICLE_PARSER}}}}<p>We calculate Berry's phase when the driving field, to which a spin-1/2 is
coupled adiabatically, rather than the familiar classical magnetic field, is a
quantum vector operator, of noncommuting, in general, components, e.g., the
angular momentum of another particle, or another spin. The geometric phase of
the entire system, spin plus \"quantum driving field\", is first computed, and is
then subdivided into the two subsystems, using the Schmidt decomposition of the
total wave function -the resulting expression shows a marked, purely quantum
effect, involving the commutator of the field components. We also compute the
corresponding mean \"classical\" phase, involving a precessing magnetic field in
the presence of noise, up to terms quadratic in the noise amplitude -the
results are shown to be in excellent agreement with numerical simulations in
the literature. Subtleties in the relation between the quantum and classical
case are pointed out, while three concrete examples illustrate the scope and
internal consistency of our treatment.
</p>
{{{{ARTICLE_PARSER}}}}<p>Quantum technology based on cold-atom interferometers is showing great
promise for fields such as inertial sensing and fundamental physics. However,
the best precision achievable on Earth is limited by the free-fall time of the
atoms, and their full potential can only be realized in Space where
interrogation times of many seconds will lead to unprecedented sensitivity.
Various mission scenarios are presently being pursued which plan to implement
matter-wave inertial sensors. Toward this goal, we realize the first onboard
operation of simultaneous $^{87}$Rb $-$ $^{39}$K interferometers in the
weightless environment produced during parabolic flight. The large vibration
levels ($10^{-2}~g/\sqrt{\rm Hz}$), acceleration range ($0-1.8~g$) and rotation
rates ($5$ deg/s) during flight present significant challenges. We demonstrate
the capability of our dual-quantum sensor by measuring the E\\"{o}tv\\"{o}s
parameter with systematic-limited uncertainties of $1.1 \times 10^{-3}$ and
$3.0 \times 10^{-4}$ during standard- and micro-gravity, respectively. This
constitutes the first test of the equivalence principle in a free-falling
vehicle with quantum sensors. Our results are applicable to inertial
navigation, and can be extended to the trajectory of a satellite for future
Space missions.
</p>
{{{{ARTICLE_PARSER}}}}<p>Fixed-point quantum search algorithms succeed at finding one of $M$ target
items among $N$ total items even when the run time of the algorithm is longer
than necessary. While the famous Grover's algorithm can search quadratically
faster than a classical computer, it lacks the fixed-point property --- the
fraction of target items must be known precisely to know when to terminate the
algorithm. Recently, Yoder, Low, and Chuang gave an optimal gate-model search
algorithm with the fixed-point property. Meanwhile, it is known that an
adiabatic quantum algorithm, operating by continuously varying a Hamiltonian,
can reproduce the quadratic speedup of gate-model Grover search. We ask, can an
adiabatic algorithm also reproduce the fixed-point property? We show that the
answer depends on what interpolation schedule is used, so as in the gate model,
there are both fixed-point and non-fixed-point versions of adiabatic search,
only some of which attain the quadratic quantum speedup. Guided by geometric
intuition on the Bloch sphere, we rigorously justify our claims with an
explicit upper bound on the error in the adiabatic approximation. We also show
that the fixed-point adiabatic search algorithm can be simulated in the gate
model with neither loss of the quadratic Grover speedup nor of the fixed-point
property. Finally, we discuss natural uses of fixed-point algorithms such as
preparation of a relatively prime state and oblivious amplitude amplification.
</p>
{{{{ARTICLE_PARSER}}}}<p>Classical stochastic processes can be generated by quantum simulators instead
of the more standard classical ones, such as hidden Markov models. One reason
for using quantum simulators is that they generally require less memory than
their classical counterparts. Here, we examine this quantum advantage for
strongly coupled spin systems---the Dyson-like one-dimensional Ising spin chain
with variable interaction length. We find that the advantage scales with both
interaction range and temperature, growing without bound as interaction
increases. Thus, quantum systems can very efficiently simulate strongly coupled
classical systems.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper investigates the relationship between subsystems and time in a
closed nonrelativistic system of interacting bosons and fermions. It is
possible to write any state vector in such a system as an unentangled tensor
product of subsystem vectors, and to do so in infinitely many ways. This
requires the superposition of different numbers of particles, but the theory
can describe in full the equivalence relation that leads to a particle-number
superselection rule in conventionally defined subsystems. Time is defined as a
functional of subsystem changes, thus eliminating the need for any reference to
an external time variable. The dynamics of the unentangled subsystem
decomposition is derived from a variational principle of dynamical stability,
which requires the decomposition to change as little as possible in any given
infinitesimal time interval, subject to the constraint that the state of the
total system satisfy the Schroedinger equation. The resulting subsystem
dynamics is deterministic. This determinism is regarded as a conceptual tool
that observers can use to make inferences about the outside world, not as a law
of nature. The experiences of each observer define some properties of that
observer's subsystem during an infinitesimal interval of time (i.e., the
present moment); everything else must be inferred from this information. The
overall structure of the theory has some features in common with quantum
Bayesianism, the Everett interpretation, and dynamical reduction models, but it
differs significantly from all of these. The theory of information described
here is largely qualitative, as the most important equations have not yet been
solved. The quantitative level of agreement between theory and experiment thus
remains an open question.
</p>
{{{{ARTICLE_PARSER}}}}<p>Device-independent self-testing allows a verifier to certify that potentially
malicious parties hold on to a specific quantum state, based only on the
observed correlations. Parallel self-testing has recently been explored, aiming
to self-test many copies (i.e. a tensor product) of the target state
concurrently. In this work, we show that $n$ EPR pairs can be self-tested in
parallel through $n$ copies of the well known CHSH game. We generalise this
result further to a parallel self-test of $n$ tilted EPR pairs with arbitrary
angles, and finally we show how our results and calculations can also be
applied to obtain a parallel self-test of $2n$ EPR pairs via $n$ copies of the
Mermin-Peres magic square game.
</p>
{{{{ARTICLE_PARSER}}}}<p>We examine the efficiency of an effective two-terminal thermoelectric device
under broken time-reversal symmetry. The setup is derived from a three-terminal
thermoelectric device comprising a thermal terminal and two electronic
contacts, under a magnetic field. We find that breaking time-reversal symmetry
in the presence of the inelastic electron-phonon processes can significantly
enhance the figure of merit for delivering electric power by supplying heat
from a phonon bath, beyond the one for producing the electric power by
investing thermal power from the electronic heat current. The efficiency of
such a device is bounded by the non-negativity of the entropy production of the
original three-terminal junction. The efficiency at maximal power can be quite
close to the Carnot efficiency, but then the electric power vanishes.
</p>
{{{{ARTICLE_PARSER}}}}<p>Shortcuts to adiabaticity are alternative fast processes that reproduce the
same final populations, or even the same final state, as the adiabatic process
in a finite, shorter time. Many of the existing approaches to design shortcuts,
and even new ones have been used throughout this Thesis in the double well
potential. These techniques have allowed us to obtain a great benefit in
processes as varied as matter-wave splitting, multiplexing/ demultiplexing,
beam splitters, cotunneling, population inversion, waveguide optimization,
Tonks-Girardeau gas manipulation and bias inversion.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study the effect of an external harmonic trapping potential on an outcome
of the non-adiabatic quantum phase transition from an antiferromagnetic to a
phase-separated state in a spin-1 atomic condensate. Previously, we
demonstrated that the dynamics of an untrapped system exhibits double
universality with two different scaling laws appearing due to conservation of
magnetization. We show that in the presence of a trap double universality
persists. However, the corresponding scaling exponents are strongly modified by
transfer of local magnetization across the system. The values of these
exponents cannot be explained by the effect of causality alone, as in the
spinless case. We derive the appropriate scaling laws based on a slow
diffusive-drift relaxation process in the local density approximation.
</p>
{{{{ARTICLE_PARSER}}}}<p>Two-level systems are one of the most important quantum systems and they form
the basis of quantum computers. We briefly look at the traditional approach to
two-level systems with an external driving field as well as those subjected to
noise. This project is aimed at studying two specific methods for obtaining
analytic solutions for two-level systems. One of the methods enables us to
obtain analytic solutions for driven time-dependent two-level systems while the
other attempts to give exact solution of qubit decoherence using a transfer
matrix method. A thorough study of both papers is done and results are
reproduced. The latter method is generalized for a qutrit system as well as a
two qubit system subjected to noise. A general method is formally derived for
an N-dimensional quantum system and the difficulties in applying the method in
real life systems is discussed.
</p>
{{{{ARTICLE_PARSER}}}}<p>The general Weyl -- Wigner formalism in finite dimensional phase spaces is
investigated. Then this formalism is specified to the case of symmetric
ordering of operators in an odd -- dimensional Hilbert space. A respective
Wigner function on the discrete phase space is found and the limit, when the
dimension of Hilbert space tends to infinity, is considered. It is shown that
this limit gives the number -- phase Wigner function in quantum optics.
Analogous results for the `almost' symmetric ordering in an even -- dimensional
Hilbert space are obtained. Relations between the discrete Wigner functions
introduced in our paper and some other discrete Wigner functions appearing in
literature are studied.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider the statistical problem of `compressive' estimation of low rank
states with random basis measurements, where the estimation error is expressed
terms of two metrics - the Frobenius norm and quantum infidelity. It is known
that unlike the case of general full state tomography, low rank states can be
identified from a reduced number of observables' expectations. Here we
investigate whether for a fixed sample size $N$, the estimation error
associated to a `compressive' measurement setup is `close' to that of the
setting where a large number of bases are measured. In terms of the Frobenius
norm, we demonstrate that for all states the error attains the optimal rate
$rd/N$ with only $O(r \log{d})$ random basis measurements. We provide an
illustrative example of a single qubit and demonstrate a concentration in the
Frobenius error about its optimal for all qubit states. In terms of the quantum
infidelity, we show that such a concentration does not exist uniformly over all
states. Specifically, we show that for states that are nearly pure and close to
the surface of the Bloch sphere, the mean infidelity scales as $1/\sqrt{N}$ but
the constant converges to zero as the number of settings is increased. This
demonstrates a lack of `compressive' recovery for nearly pure states in this
metric.
</p>
{{{{ARTICLE_PARSER}}}}<p>In 2013, Boneh and Zhandry introduced the notion of indistinguishability
(IND) in chosen plaintext (CPA) and chosen ciphertext (CCA) attacks by a
quantum adversary which is given superposition access to an oracle for
encryption and decryption queries but is restricted to classical queries in the
challenge phase. In this paper we define IND-CPA and IND-CCA notions for
symmetric encryption schemes where the adversary has full quantum superposition
access to the oracle, and give constructions that achieve these security
notions. Our results are formulated in the concrete security framework.
</p>
{{{{ARTICLE_PARSER}}}}<p>Non-stoquastic Hamiltonians have both positive and negative signs in
off-diagonal elements in their matrix representation in the standard basis and
thus cannot be simulated efficiently by the standard quantum Monte Carlo method
due to the sign problem. We review our analytical studies of this type of
Hamiltonians with infinite-range non-random and random interactions from the
perspective of possible enhancement of the efficiency of quantum annealing or
adiabatic quantum computing. It is shown that non-stoquastic terms, of the type
of multi-body transverse interactions like XX and XXX with positive
coefficients, appended to the stoquastic Hamiltonian reduce a first-order
quantum phase transition in the simple transverse-field Ising model to a
second-order transition. This implies that the efficiency of quantum annealing
is exponentially enhanced, because a first-order transition has an
exponentially small energy gap (and therefore exponentially long computation
time) whereas a second-order transition has a polynomially decaying gap
(polynomial computation time). The examples presented here are the first
instances where strong quantum effects, in the sense that they cannot be
efficiently simulated in the standard quantum Monte Carlo, have analytically
been shown to exponentially enhance the efficiency of quantum annealing for
combinatorial optimization problems.
</p>
{{{{ARTICLE_PARSER}}}}<p>We calculate the emission spectra, the Glauber $g^{(2)}$ function, and the
entanglement of formation for a few two-level emitters coupled to a single
cavity mode and subject to an external laser-excitation. To evaluate these
quantities we couple the system to environmental degrees of freedom which leads
to dissipative dynamics. Because of the periodic time-dependence of the system
Hamiltonian, the coefficients of the (Markovian) master equation are constant
if Floquet states are used as the computational basis. Studying the emission
spectra we show that the dynamic Stark effect, i.e., the shift of spectral
lines, first appears in the second order of the laser intensity. For the
Glauber function, we find clearly distinguished parameter regimes of super- and
sub-Poissonian light emission and explain the additional features appearing for
finite laser intensity in terms of the quasienergy spectrum of the driven
emitter-cavity system. Finally, we analyze the temperature and emitter-cavity
coupling regimes where entanglement among the emitters is generated, and show
that the laser-excitation leads to a decrease of entanglement.
</p>
{{{{ARTICLE_PARSER}}}}<p>We outline the general construction of three-players games with incomplete
information which fulfil the following conditions: (i) symmetry with respect to
the exchange of the players; (ii) the existence of the upper bound for total
payoff resulting from Bell inequalities; (iii) the existence of both fair and
unfair Nash equilibria saturating this bound. Conditions ((i)$\div$(iii)) imply
that we are dealing with conflicting interest games. An explicit example of
such a game is given. A quantum counterpart of this game is considered which is
obtained by keeping the same utilities but replacing classical advisor by a
quantum one. It is shown that the quantum game possesses only fair equilibria
with strictly higher payoffs than in the classical case. This implies that
quantum nonlocality can be used to resolve a conflict between players.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider the ground-state properties of an extended one-dimensional Bose
gas with pointwise attractive interactions. We take the limit where the
interaction strength goes to zero as the system size increases at fixed
particle density. In this limit the gas exhibits a quantum phase transition. We
compute local correlation functions at zero temperature, both at finite and
infinite size. We provide analytic formulas for the experimentally relevant
one-point functions $g_2$, $g_3$ and analyze their finite-size corrections. Our
results are compared to the mean-field approach based on the Gross-Pitaevskii
equation which yields the exact results in the infinite system size limit, but
not for finite systems.
</p>
{{{{ARTICLE_PARSER}}}}<p>The Bohm/de Broglie theory of deterministic non-relativistic quantum
mechanics is broadened to accommodate the free-particle Dirac equation. As with
the spin-0 theory, an effective particle rest-mass scalar field in the presence
of the spin-1/2 pilot wave is allowed, together with the assumption that the
convective current component describes ensemble dynamics. Non-positive
excursions of the ensemble density for extreme cases of positive-energy
solutions of the Dirac equation are interpreted in terms of virtual-like pair
creation and annihilation beneath the Compton wavelength. A specific
second-rank tensor is defined in terms of the Dirac spinors for generalizing
from simply a quantum potential to a stress tensor required to account for the
force of pilot wave on particle. A simple dependence of the stress tensor on a
two-component spin pseudovector field is determined. Consistency is found with
an earlier non-relativistic theory of objects with spin.
</p>
{{{{ARTICLE_PARSER}}}}<p>Dzyaloshinskii-Moriya (DM) interaction has been proven to excite entanglement
of spin systems, enhancing the capability of realizing various quantum tasks
such as teleportation. In this work, we consider the DM interaction -to the
best of our knowledge, for the first time in quantum game theory. We study the
winning probability of magic square game played with the thermal entangled
state of spin models under external magnetic fields and DM interaction. We
analytically show that although DM interaction excites the entanglement of the
system as expected, it surprizingly reduces the winning probability of the
game, acting like temperature or magnetic fields, and also show that the
effects of DM interaction and inhomogeneous magnetic field on the winning
probability are identical. In addition, we show that XXZ model is considerably
more robust than XX model against destructive effects. Our results can open new
insights for quantum information processing with spin systems.
</p>
{{{{ARTICLE_PARSER}}}}<p>While being optimally compensated for spatial phase variations, the
two-photon state produced by the two-crystal emission exhibits spatial and
spectral decoherence off the central emission modes. In this paper, we present
an experimentally convenient method to optimize the ultra-wide spatial and
spectral windows; allowing the minimum spatial-spectral decoherence for a
required two-photon flux.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this work, we provide an overview of how well-established concepts in the
fields of quantum chemistry and material sciences have to be adapted when the
quantum nature of light becomes important in correlated matter-photon problems.
Therefore, we analyze model systems in optical cavities, where the
matter-photon interaction is considered from the weak- to the strong coupling
limit and for individual photon modes as well as for the multi-mode case. We
identify fundamental changes in Born-Oppenheimer surfaces, spectroscopic
quantities, conical intersections and efficiency for quantum control. We
conclude by applying our novel recently developed quantum-electrodynamical
density-functional theory to single-photon emission and show how a
straightforward approximation accurately describes the correlated
electron-photon dynamics. This paves the road to describe matter-photon
interactions from first-principles and addresses the emergence of new states of
matter in chemistry and material science.
</p>
{{{{ARTICLE_PARSER}}}}<p>The security analysis of quantum key distribution (QKD) is difficult to
perform when there is efficiency mismatch between various threshold detectors
involved in an experimental setup. Even the verification that the device
actually performs in the quantum domain, referred to as the task of
entanglement verification, is hard to perform. In this article we provide such
an entanglement-verification method for characterized detection-efficiency
mismatch. Our method does not rely on cut-off of photon numbers in the optical
signal. It can be applied independently of the degrees of freedom involved,
thus covering, for example, efficiency mismatch in polarization and time-bin
modes, but also in spatial modes. The evaluation of typical experimental
scenarios suggests that an increase of detection-efficiency mismatch will drive
the performance of a given setup out of the quantum domain.
</p>
{{{{ARTICLE_PARSER}}}}<p>Point defects in silicon carbide are rapidly becoming a platform of great
interest for single photon generation, quantum sensing, and quantum information
science. Photonic crystal cavities (PCC) can serve as an efficient light-matter
interface both to augment the defect emission and to aid in studying the
defects' properties. In this work, we fabricate 1D nanobeam PCCs in 4H-silicon
carbide with embedded silicon vacancy centers. These cavities are used to
achieve Purcell enhancement of two closely spaced defect zero-phonon lines
(ZPL). Enhancements of &gt;80-fold are measured using multiple techniques.
Additionally, the nature of the cavity coupling to the different ZPLs is
examined.
</p>
{{{{ARTICLE_PARSER}}}}<p>The simultaneous presence of two competing inter-particle interactions can
lead to the emergence of new phenomena in a many-body system. Among others,
such effects are expected in dipolar Bose-Einstein condensates, subject to
dipole-dipole interaction and short-range repulsion. Magnetic quantum gases and
in particular Dysprosium gases, offering a comparable short-range contact and a
long-range dipolar interaction energy, remarkably exhibit such emergent
phenomena. In addition an effective cancellation of mean-field effects of the
two interactions results in a pronounced importance of quantum-mechanical
beyond mean-field effects. For a weakly-dominant dipolar interaction the
striking consequence is the existence of a new state of matter equilibrated by
the balance between weak mean-field attraction and beyond mean-field repulsion.
Though exemplified here in the case of dipolar Bose gases, this state of matter
should appear also with other microscopic interactions types, provided a
competition results in an effective cancellation of the total mean-field. The
macroscopic state takes the form of so-called quantum droplets. We present the
effects of a long-range dipolar interaction between these droplets.
</p>
{{{{ARTICLE_PARSER}}}}<p>We observe the nonlinearity of the Jaynes-Cummings (JC) ladder in the
Autler-Townes spectroscopy of the hyperfine ground states for a Rydberg-dressed
two-atom system. Here the role of the two-level system in the JC model is
played by the presence or absence of a collective Rydberg excitation, and the
bosonic mode manifests as the number $n$ of single atom spin flips,
symmetrically distributed between the atoms. We measure the normal-mode
splitting and $\sqrt{n}$ nonlinearity as a function of detuning and Rabi
frequency, thereby experimentally establishing the isomorphism with the JC
model.
</p>
{{{{ARTICLE_PARSER}}}}<p>The ability to distribute entanglement over complex quantum networks is an
important step towards a quantum Internet. Recently, there has been significant
theoretical effort by mainly focusing on the distribution of bipartite
entanglement via a simple quantum network composed only of bipartite quantum
channels. There are, however, a number of quantum information processing
protocols based on multipartite entanglement, rather than bipartite one, which
should be served more naturally by a quantum broadcast network, rather than
such a simple network. In this work, we present a general framework for
deriving upper bounds on the rates at which GHZ states or multipartite private
states can be distributed among a number of different parties over any quantum
broadcast network. We also discuss how lower bounds can be obtained by
combining generalisation of an aggregated quantum repeater protocol with graph
theoretic concepts.
</p>
{{{{ARTICLE_PARSER}}}}<p>We derive a microscopic model for dissipative dynamics in a system of
mutually interacting qubits coupled to a thermal bath that generalises the
dissipative model of Landau-Lifschitz-Gilbert to the case of anisotropic bath
couplings. We show that the dissipation acts to bias the quantum trajectories
towards a reduced phase space. This model applies to a system of
superconducting flux qubits whose coupling to the environment is necessarily
anisotropic. We study the model in the context of the D-Wave computing device
and show that the form of environmental coupling in this case produces dynamics
that are closely related to several models proposed on phenomenological
grounds.
</p>
{{{{ARTICLE_PARSER}}}}<p>We develop rigorous notions of causality and causal separability in the
process framework introduced in [Oreshkov, Costa, Brukner, Nat. Commun. 3, 1092
(2012)], which describes correlations between separate local experiments
without a prior assumption of causal order between them. We consider the
general multipartite case and take into account the possibility for dynamical
causal order, where the order of a set of events can depend on other events in
the past. Starting from a general definition of causality, we derive an
iteratively formulated canonical decomposition of multipartite causal
processes, and show that for a fixed number of settings and outcomes for each
party, the respective correlations form a polytope whose facets define causal
inequalities. In the case of quantum processes, we investigate the link between
causality and the theory-dependent notion of causal separability, which we here
extend to the multipartite case based on concrete principles. We show that
causality and causal separability are not equivalent in general by giving an
example of a physically admissible tripartite quantum process that is causal
but not causally separable. We also show that there exist causally separable
(and hence causal) quantum processes that become non-causal if extended by
supplying the parties with entangled ancillas. This example of activation of
non-causality motivates the concepts of extensibly causal and extensibly
causally separable (ECS) processes, for which the respective property remains
invariant under extension with arbitrary ancillas. We characterize the class of
tripartite ECS processes in terms of simple conditions on the form of the
process matrix, which generalize the form of bipartite causally separable
process matrices. We show that the processes realizable by classically
controlled quantum circuits are ECS and conjecture that the reverse also holds.
</p>
{{{{ARTICLE_PARSER}}}}<p>We apply microcanonical ensemble considerations to suggest that, whenever it
may thermalize, a general disorder-free many-body Hamiltonian of a typical
atomic system has solid-like eigenstates at low energies and fluid-type (and
gaseous, plasma) eigenstates associated with energy densities exceeding those
present in the melting (and, respectively, higher energy) transition(s). In
particular, the lowest energy density at which the eigenstates of such a clean
many body atomic system undergo a non-analytic change is that of the melting
(or freezing) transition. We invoke this observation to analyze the evolution
of a liquid upon supercooling (i.e., cooling rapidly enough to avoid
solidification below the freezing temperature). Expanding the wavefunction of a
supercooled liquid in the complete eigenbasis of the many-body Hamiltonian,
only the higher energy liquid-type eigenstates contribute significantly to
measurable hydrodynamic relaxations (e.g., those probed by viscosity) while
static thermodynamic observables become weighted averages over both solid- and
liquid-type eigenstates. Consequently, when extrapolated to low temperatures,
hydrodynamic relaxation times of deeply supercooled liquids (i.e., glasses) may
seem to diverge at nearly the same temperature at which the extrapolated
entropy of the supercooled liquid becomes that of the solid. In this formal
quantum framework, the increasingly sluggish (and spatially heterogeneous)
dynamics in supercooled liquids as their temperature is lowered stems from the
existence of the single non-analytic change of the eigenstates of the clean
many-body Hamiltonian at the equilibrium melting transition present in low
energy solid-type eigenstates. We derive a single (possibly computable)
dimensionless parameter fit to the viscosity and suggest other testable
predictions of our approach.
</p>
{{{{ARTICLE_PARSER}}}}<p>Optical dipole-traps are used in various scientific fields, including
classical optics, quantum optics and biophysics. Here, we propose and implement
a dipole-trap for nanoparticles that is based on focusing from the full solid
angle with a deep parabolic mirror. The key aspect is the generation of a
linear-dipole mode which is predicted to provide a tight trapping potential. We
demonstrate the trapping of rod-shaped nanoparticles and validate the trapping
frequencies to be on the order of the expected ones. The described realization
of an optical trap is applicable for various other kinds of solid-state
targets. The obtained results demonstrate the feasibility of optical
dipole-traps which simultaneously provide high trap stiffness and allow for
efficient interaction of light and matter in free space.
</p>
{{{{ARTICLE_PARSER}}}}<p>Measurement and estimation of parameters are essential for science and
engineering, where the main quest is to find out the highest achievable
precision with given resources and design schemes to attain it. Two schemes,
the sequential feedback scheme and the parallel scheme, are usually studied in
quantum parameter estimation. While the sequential feedback scheme represents
the most general scheme, it remains unknown whether it can outperform the
parallel scheme for any quantum estimation tasks. In this Letter we show that
the sequential feedback scheme has a 3-fold improvement over the parallel
scheme for Hamiltonian parameter estimations on 2-dimensional systems, and an
order of $O(d+1)$ improvement for Hamiltonian parameter estimation on
$d-$dimensional systems. We also show that, contrary to the conventional
belief, it is possible to simultaneously achieve the highest precision for
estimating all three components of a magnetic field, which sets a benchmark on
the local precision limit for the estimation of a magnetic field.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a new technique to reduce the expected number of measurements to
declare an unknown quantum state as entangled. Our method is based on the
geometric criterion and so requires only local Pauli measurements. Using
concentration of measure, we provide a heuristic which allows us to reinterpret
a previous decision tree algorithm due to Laskowski et al and that forms the
basis for our new algorithm. Numerical simulations show that for three to six
qubits we use fewer measurements than either the trivial algorithm or the
decision tree algorithm. In addition, our method is easy to construct, assumes
no prior knowledge of the system and works for any number of qubits.
</p>
{{{{ARTICLE_PARSER}}}}<p>Optical interfaces for quantum emitters are a prerequisite for implementing
quantum networks. Here, we couple single molecules to the guided modes of an
optical nanofiber. The molecules are embedded within a crystal that provides
photostability and, due to its inhomogeneous environment, a means to spectrally
address single molecules. Single molecules are excited and detected solely via
the nanofiber interface without the requirement of additional optical access.
In this way, we realize a fully fiber-integrated system that is scalable and
may become a versatile constituent for quantum hybrid systems.
</p>
{{{{ARTICLE_PARSER}}}}<p>The capacity of classical channels is convex. This is not the case for the
quantum capacity of a channel: the capacity of a mixture of different quantum
channels exceeds the mixture of the individual capacities and thus is
non-convex. Here we show that this effect goes beyond the quantum capacity and
holds for the private and classical environment-assisted capacities of quantum
channels.
</p>
{{{{ARTICLE_PARSER}}}}<p>Removing exactly one photon from an arbitrary input pulse is an elementary
operation in quantum optics and enables applications in quantum information
processing and quantum simulation. Here we demonstrate a deterministic
single-photon absorber based on the saturation of an optically thick free-space
medium by a single photon due to Rydberg blockade. Single-photon subtraction
adds a new component to the Rydberg quantum optics toolbox, which already
contains photonic logic building-blocks such as single-photon sources,
switches, transistors, and conditional $\pi$-phase shifts. Our approach is
scalable to multiple cascaded absorbers, essential for preparation of
non-classical light states for quantum information and metrology applications,
and, in combination with the single-photon transistor, high-fidelity
number-resolved photon detection.
</p>
{{{{ARTICLE_PARSER}}}}<p>Recently it was shown that the quantum behavior of an intense cavity field
can be revealed by measuring the steady atomic correlations between two ideal
atoms, which interact with the same leaking cavity mode. Considering a weak
atom-field coupling regime and large average number of photons in the cavity
mode ($\bar{n}$), one expects that a semiclassical theory could explain the
whole dynamics of the system. However, this system presents the generation of
correlations between the atoms, which is a signature of the quantumness of the
cavity field even in the limit of $\bar{n} \gg 1$ [Phys. Rev. Lett.
\textbf{107}, 153601 (2011)]. Here, we extend this result by investigating the
relaxation time for such a system. We have shown that the relaxation time of
the system varies proportionally to $\bar{n}$ for a coherent driving, but it is
inversely proportional to $\bar{n}$ for an incoherent pumping. Thus, the time
required to observe the manifestation of the quantum aspects of a cavity field
on the atomic correlations diverges as $\bar{n}$ tends to macroscopic values
due to a coherent driving, while it goes to zero for incoherent pumping. For a
coherent driving, we can also see that this system presents metastability,
i.e., firstly the atomic system reaches a quasi-stationary state which last for
a long time interval, but eventually it reaches the real steady state. We have
also discussed the effects of small atomic decay. In this case, the steady
correlations between the atoms disappear for long times, but the intense cavity
field is still able to generate atomic correlations at intermediate times.
Then, considering a real scenario, we would be able to monitor the quantumness
of a cavity field in a certain time interval.
</p>
{{{{ARTICLE_PARSER}}}}<p>We show that known entropy bounds constrain the information carried off by
radiation to null infinity. We consider distant, planar null hypersurfaces in
asymptotically flat spacetime. Their focussing and area loss can be computed
perturbatively on a Minkowski background, yielding entropy bounds in terms of
the energy flux of the outgoing radiation. In the asymptotic limit, we obtain
boundary versions of the Quantum Null Energy Condition, of the Generalized
Second Law, and of the Quantum Bousso Bound.
</p>
{{{{ARTICLE_PARSER}}}}<p>Using the worldline formalism of the Dirac field with a non-Abelian gauge
symmetry we show how to describe the matter field transforming in an arbitrary
representation of the gauge group. Colour degrees of freedom are carried on the
worldline by auxiliary fields, responsible for providing path ordering and the
Wilson-loop coupling. The Hilbert space of these fields is reducible but we
make use of recent work in order to project onto a single, arbitrary,
irreducible representation. By functionally quantising the resulting theory we
show that this procedure correctly generates the Wilson-loop interaction
between the gauge field and the matter field taken to transform in a chosen
representation. This work has direct application to physical observables such
as scattering amplitudes in the presence of such a matter multiplet and lifts
the restriction on the type of matter that has previously featured in worldline
calculations.
</p>
{{{{ARTICLE_PARSER}}}}<p>Quantum critical states exhibit strong quantum fluctuations and are therefore
highly susceptible to perturbations. In this work we study the dynamical
stability of such states against a sudden coupling to these strong fluctuations
by quenching the order parameter of the underlying transition. We find that
such a quench can generate superextensive energy fluctuations. This leads to a
dynamical quantum phase transition in the resulting decay of the initial state.
At the corresponding critical time the dynamically-evolved state becomes
orthogonal to the initial one making both states optimally distinguishable
which is the defining property of quantum speed limits. Due to the
superextensive energy fluctuations the critical time of orthogonality exhibits
an unconventional system-size dependence yielding a yet unrecognized quantum
speed limit. We illustrate these findings for the spin-1/2 XXZ chain and the
infinitely-connected Ising model. The main concepts, however, are general and
can be applied also to other critical states. An outlook is given onto the
implications of the superextensive energy fluctuations on potential restricted
thermalization despite of nonintegrability.
</p>
{{{{ARTICLE_PARSER}}}}<p>While two-dimensional symmetry-enriched topological phases ($\mathsf{SET}$s)
have been studied intensively and systematically, three-dimensional ones are
still open issues. We propose an algorithmic approach of imposing global
symmetry $G_s$ on gauge theories (denoted by $\mathsf{GT}$) with gauge group
$G_g$. The resulting symmetric gauge theories are dubbed \"symmetry-enriched
gauge theories\" ($\mathsf{SEG}$), which may be served as low-energy effective
theories of three-dimensional symmetric topological quantum spin liquids. We
focus on $\mathsf{SEG}$s with gauge group
$G_g=\mathbb{Z}_{N_1}\times\mathbb{Z}_{N_2}\times\cdots$ and on-site unitary
symmetry group $G_s=\mathbb{Z}_{K_1}\times\mathbb{Z}_{K_2}\times\cdots$ or
$G_s=\mathrm{U(1)}\times \mathbb{Z}_{K_1}\times\cdots$. Each
$\mathsf{SEG}(G_g,G_s)$ is described in the path integral formalism associated
with certain symmetry assignment. From the path-integral expression, we propose
how to physically diagnose the ground state properties (i.e., $\mathsf{SET}$
orders) of $\mathsf{SEG}$s in experiments of charge-loop braidings (patterns of
symmetry fractionalization) and the \emph{mixed} multi-loop braidings among
deconfined loop excitations and confined symmetry fluxes. From these
symmetry-enriched properties, one can obtain the map from $\mathsf{SEG}$s to
$\mathsf{SET}$s. By giving full dynamics to background gauge fields,
$\mathsf{SEG}$s may be eventually promoted to a set of new gauge theories
(denoted by $\mathsf{GT}^*$). Based on their gauge groups, $\mathsf{GT}^*$s may
be further regrouped into different classes each of which is labeled by a gauge
group ${G}^*_g$. Finally, a web of gauge theories involving $\mathsf{GT}$,
$\mathsf{SEG}$, $\mathsf{SET}$ and $\mathsf{GT}^*$ is achieved. We demonstrate
the above symmetry-enrichment physics and the web of gauge theories through
many concrete examples.
</p>
{{{{ARTICLE_PARSER}}}}<p>Quantum mechanics suggests that nature is discrete, with one state per phase
space volume $\hbar^{3N}$. This appears to contradict the idea that the state
of an N-particle system can have infinite precision and is described by a set
of exponentially many complex numbers. Using a finite-temperature gas confined
in a box as an example, this short paper argues that there are indeed limits to
the precision of wave functions, and that this may help at understanding the
quantum-to-classical transition.
</p>
{{{{ARTICLE_PARSER}}}}<p>Autoscaling system can reconfigure cloud-based applications and services,
through various cloud software configurations and hardware provisioning, to
adapt to the changing environment at runtime. Such a behaviour offers the
foundation to achieve elasticity in modern cloud computing paradigm. Given the
importance of autoscaling in cloud, computational intelligence has been widely
applied for engineering autoscaling system, leading to self-aware,
self-adaptive and more dependable runtime scaling. In this paper, we present
the brief background and history for autoscaling in the cloud, as well as their
associations with self-awareness and self-adaptivity of a system. Subsequently,
we conduct detailed survey and taxonomy of the key related work and identify
the gaps in this area of research.
</p>
{{{{ARTICLE_PARSER}}}}<p>Future multiple-input multiple-output (MIMO) wireless communications systems
will use beamforming to re- alize the capacity requirements necessitated by the
exponential increase in data demand. Taking advantage of these potentially
large beamforming gains will require the transmitter and receiver to align
their beams in an optimal manner as a function of the wireless channel. We
propose two novel (yet low-complexity) techniques for beam alignment in Time
Division Duplexing (TDD) MIMO wireless communications systems. Both algorithms
seek to obtain good estimates of the optimal beamformer/combiner pair, which
are the dominant singular vectors of the channel matrix. These techniques are
motivated by the power method, an iterative algorithm to determine eigenvalues
and eigenvectors through re- peated matrix multiplications. The proposed
techniques improve on this simple idea by providing a better performance in the
low- SNR regime. This is achieved by considering information from all the
previous iterations of the algorithm and combining them in different ways, in
contrast to the basic power method which considers only the most recent
iteration and assumes noiseless links. The first technique sequentially
constructs a least-squares estimate of the channel matrix, which is then used
to calculate the optimal beamformer/combiner pair. The second technique aims to
mitigate the effect of additive noise by using a linear combination of the
previously tried beams to calculate the next beam in the iteration. Simulation
results provide insight on the performance of both algorithms in the presence
of noise and compare them with similar techniques from the literature.
</p>
{{{{ARTICLE_PARSER}}}}<p>We propose a novel Connectionist Text Proposal Network (CTPN) that accurately
localizes text lines in natural image. The CTPN detects a text line in a
sequence of fine-scale text proposals directly in convolutional feature maps.
We develop a vertical anchor mechanism that jointly predicts location and
text/non-text score of each ?fixed-width proposal, considerably improving
localization accuracy. The sequential proposals are naturally connected by a
recurrent neural network, which is seamlessly incorporated into the
convolutional network, resulting in an end-to-end trainable model. This allows
the CTPN to explore rich context information of image, making it powerful to
detect extremely ambiguous text. The CTPN works reliably on multi-scale and
multi- language text without further post-processing, departing from previous
bottom-up methods requiring multi-step post-processing. It achieves 0.88 and
0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, surpass- ing recent
results [8, 35] by a large margin. The CTPN is computationally efficient with
0:14s/image, by using the very deep VGG16 model [27]. Online demo is available
at: <a href=\"http://textdet.com/.\">this http URL</a>
</p>
{{{{ARTICLE_PARSER}}}}<p>Context: Developers use bad code smells to guide code reorganization. Yet
developers, text books, tools, and researchers disagree on which bad smells are
important. Objective: To evaluate the likelihood that a code reorganization to
address bad code smells will yield improvement in the defect-proneness of the
code. Method: We introduce XTREE, a tool that analyzes a historical log of
defects seen previously in the code and generates a set of useful code changes.
Any bad smell that requires changes outside of that set can be deprioritized
(since there is no historical evidence that the bad smell causes any problems).
Evaluation: We evaluate XTREE's recommendations for bad smell improvement
against recommendations from previous work (Shatnawi, Alves, and Borges) using
multiple data sets of code metrics and defect counts. Results: Code modules
that are changed in response to XTREE's recommendations contain significantly
fewer defects than recommendations from previous studies. Further, XTREE
endorses changes to very few code metrics, and the bad smell recommendations
(learned from previous studies) are not universal to all software projects.
Conclusion: Before undertaking a code reorganization based on a bad smell
report, use a tool like XTREE to check and ignore any such operations that are
useless; i.e. ones which lack evidence in the historical record that it is
useful to make that change. Note that this use case applies to both manual code
reorganizations proposed by developers as well as those conducted by automatic
methods. This recommendation assumes that there is an historical record. If
none exists, then the results of this paper could be used as a guide.
</p>
{{{{ARTICLE_PARSER}}}}<p>Personal Health Records (PHR) emerge as an alternative to integrate patient's
health information to give a global view of patients' status. However,
integration is not a trivial feature when dealing with a variety electronic
health systems from healthcare centers. Access to PHR sensitive information
must comply with privacy policies defined by the patient. Architecture PHR
design should be in accordance to these, and take advantage of nowadays
technology. Cloud computing is a current technology that provides scalability,
ubiquity, and elasticity features. This paper presents a scoping review related
to PHR systems that achieve three characteristics: integrated, reliable and
cloud-based. We found 101 articles that addressed those characteristics. We
identified four main research topics: proposal/developed systems, PHR
recommendations for development, system integration and standards, and security
and privacy. Integration is tackled with HL7 CDA standard. Information
reliability is based in ABE security-privacy mechanism. Cloud-based technology
access is achieved via SOA.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider the problem of object recognition in 3D using an ensemble of
attribute-based classifiers. We propose two new concepts to improve
classification in practical situations, and show their implementation in an
approach implemented for recognition from point-cloud data. First, the viewing
conditions can have a strong influence on classification performance. We study
the impact of the distance between the camera and the object and propose an
approach to fuse multiple attribute classifiers, which incorporates distance
into the decision making. Second, lack of representative training samples often
makes it difficult to learn the optimal threshold value for best positive and
negative detection rate. We address this issue, by setting in our attribute
classifiers instead of just one threshold value, two threshold values to
distinguish a positive, a negative and an uncertainty class, and we prove the
theoretical correctness of this approach. Empirical studies demonstrate the
effectiveness and feasibility of the proposed concepts.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper we address the problem of robot movement adaptation under
various environmental constraints interactively. Motion primitives are
generally adopted to generate target motion from demonstrations. However, their
generalization capability is weak while facing novel environments.
Additionally, traditional motion generation methods do not consider the
versatile constraints from various users, tasks, and environments. In this
work, we propose a co-active learning framework for learning to adapt robot
end-effector's movement for manipulation tasks. It is designed to adapt the
original imitation trajectories, which are learned from demonstrations, to
novel situations with various constraints. The framework also considers user's
feedback towards the adapted trajectories, and it learns to adapt movement
through human-in-the-loop interactions. The implemented system generalizes
trained motion primitives to various situations with different constraints
considering user preferences. Experiments on a humanoid platform validate the
effectiveness of our approach.
</p>
{{{{ARTICLE_PARSER}}}}<p>This thesis focuses on the wireless security in the physical layer with
beamforming technique. One of the emerging areas is the security enhancement in
the physical layer, which exploits the intrinsic properties of the wireless
medium. Beamforming, which has been proved to have many advantages, can also be
applied to enhance the wireless security.
</p>
<p>One of the most common threats, i.e., passive eavesdropping, is addressed in
this thesis. To reduce the risk of leaking information to the eavesdroppers,
the physical region where the transmission is exposed to eavesdropping has been
studied. In this thesis, the exposure region based beamforming technique is
proposed to combat the threat from the randomly located eavesdroppers in a
fading channel. In this system model, the large-scale path loss and a general
Rician fading channel model are considered. The exposure region is established
to describe the secrecy performance of the system, based on which the
probability that the secrecy outage event occurs is defined to evaluate the
security level of the exposure region.
</p>
<p>The antenna array is one of the most important factors that affect the
secrecy performance of the exposure region based beamforming technique. The
potential of using different array geometry and array configuration to improve
security is explored. In this thesis, two common arrays, i.e., linear and
circular arrays, are considered.Analytic expressions for general array geometry
and for the linear and circular arrays are derived. Based on the empirical
results, numerical optimization algorithms are developed to exploit the array
configuration to enhance the system security level. In addition, experiments
are carried out to study the performance of the beamformer with linear and
circular arrays. Especially, the impact of the mutual coupling on the security
performance is investigated.
</p>
{{{{ARTICLE_PARSER}}}}<p>Events and entities are closely related; entities are often actors or
participants in events and events without entities are uncommon. The
interpretation of events and entities is highly contextually dependent.
Existing work in information extraction typically models events separately from
entities, and performs inference at the sentence level, ignoring the rest of
the document. In this paper, we propose a novel approach that models the
dependencies among variables of events, entities, and their relations, and
performs joint inference of these variables across a document. The goal is to
enable access to document-level contextual information and facilitate
context-aware predictions. We demonstrate that our approach substantially
outperforms the state-of-the-art methods for event extraction as well as a
strong baseline for entity extraction.
</p>
{{{{ARTICLE_PARSER}}}}<p>Kahn process networks are a model of computation based on a collection of
sequential, deterministic processes that communicate by sending messages
through unbounded channels. They are well suited for modelling stream-based
computations, but are in no way restricted to this application. Interaction
nets are graph rewriting systems that have many interesting properties for
implementation. In this paper we show how to encode process networks using
interaction nets, where we model both networks and messages in the same
framework.
</p>
{{{{ARTICLE_PARSER}}}}<p>An algorithm is in-place, or runs in-situ, when it does not need any
additional memory to execute beyond a small constant amount. There are many
algorithms that are efficient because of this feature, therefore it is an
important aspect of an algorithm. In most programming languages, it is not
obvious when an algorithm can run in-place, and moreover it is often not clear
that the implementation respects that idea. In this paper we study interaction
nets as a formalism where we can see directly, visually, that an algorithm is
in-place, and moreover the implementation will respect that it is in-place. Not
all algorithms can run in-place however. We can nevertheless still use the same
language, but now we can annotate parts of the algorithm that can run in-place.
We suggest an annotation for rules, and give an algorithm to find this
automatically through analysis of the interaction rules.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper we study termination of term graph rewriting, where we restrict
our attention to acyclic term graphs. Motivated by earlier work by Plump we aim
at a definition of the notion of simplification order for acyclic term graphs.
For this we adapt the homeomorphic embedding relation to term graphs. In
contrast to earlier extensions, our notion is inspired by morphisms. Based on
this, we establish a variant of Kruskal's Tree Theorem formulated for acyclic
term graphs. In proof, we rely on the new notion of embedding and follow
Nash-Williams' minimal bad sequence argument. Finally, we propose a variant of
the lexicographic path order for acyclic term graphs.
</p>
{{{{ARTICLE_PARSER}}}}<p>GP 2 is a non-deterministic programming language for computing by graph
transformation. One of the design goals for GP 2 is syntactic and semantic
simplicity, to facilitate formal reasoning about programs. In this paper, we
demonstrate with four case studies how programmers can prove termination and
partial correctness of their solutions. We argue that GP 2's graph
transformation rules, together with induction on the length of program
executions, provide a convenient framework for program verification.
</p>
{{{{ARTICLE_PARSER}}}}<p>We introduce a new interaction net implementation of optimal reduction for
the pure untyped lambda calculus. Unlike others, our implementation allows to
reach normal form regardless of the interaction net reduction strategy using
the approach of so-called token-passing nets and a non-deterministic extension
for interaction nets. Another new feature is the read-back mechanism
implemented without leaving the formalism of interaction nets.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider directed graphs with edge labels from a semiring. We present an
algorithm that allows efficient execution of queries for existence and weights
of paths, and allows updates of the graph: adding nodes and edges, and changing
weights of existing edges.
</p>
<p>We apply this method in the construction of matchbound certificates for
automatically proving termination of string rewriting. We re-implement the
decomposition/completion algorithm of Endrullis et al. (2006) in our framework,
and achieve comparable performance.
</p>
{{{{ARTICLE_PARSER}}}}<p>A common feature across many science and engineering applications is the
amount and diversity of data and computation that must be integrated to yield
insights. Data sets are growing larger and becoming distributed; and their
location, availability and properties are often time-dependent. Collectively,
these characteristics give rise to dynamic distributed data-intensive
applications. While \"static\" data applications have received significant
attention, the characteristics, requirements, and software systems for the
analysis of large volumes of dynamic, distributed data, and data-intensive
applications have received relatively less attention. This paper surveys
several representative dynamic distributed data-intensive application
scenarios, provides a common conceptual framework to understand them, and
examines the infrastructure used in support of applications.
</p>
{{{{ARTICLE_PARSER}}}}<p>This volume contains the proceedings of the Seventh International Symposium
on Games, Automata, Logic and Formal Verification (GandALF 2016). The symposium
took place in Catania, Italy, from the 14th to the 16th of September 2016. The
proceedings of the symposium contain abstracts of the 3 invited talks and 21
full papers that were accepted after a careful evaluation for presentation at
the conference. The topics of the accepted papers cover algorithmic game
theory, automata theory, synthesis, formal verification, and dynamic, modal and
temporal logics.
</p>
{{{{ARTICLE_PARSER}}}}<p>Classical stochastic processes can be generated by quantum simulators instead
of the more standard classical ones, such as hidden Markov models. One reason
for using quantum simulators is that they generally require less memory than
their classical counterparts. Here, we examine this quantum advantage for
strongly coupled spin systems---the Dyson-like one-dimensional Ising spin chain
with variable interaction length. We find that the advantage scales with both
interaction range and temperature, growing without bound as interaction
increases. Thus, quantum systems can very efficiently simulate strongly coupled
classical systems.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we study the distributed control of networked cyber-physical
systems when a much more energy-efficient distributed communication management
strategy is proposed to solve the well-studied consensus problem. In contrast
to the existing potential-based network topology control method, the proposed
topology control method is based on the variation of communication ranges such
that each agent can control its ad hoc communication range. The proposed
network topology control technique can not only guarantee network connectivity
but also reduce the communication energy. We apply the new network topology
control technique, based on variable communication ranges, in a well-studied
consensus problem, where the communication range for each agent is designed
locally along with a new bounded control algorithm. Theoretical analysis is
then provided to show that the proposed network topology control technique can
guarantee consensus with bounded communication energy consumption. Finally,
simulation examples are provided to show the effectiveness of the proposed
energy-efficient distributed topology control technique.
</p>
{{{{ARTICLE_PARSER}}}}<p>Object skeletons are useful for object representation and object detection.
They are complementary to the object contour, and provide extra information,
such as how object scale (thickness) varies among object parts. But object
skeleton extraction from natural images is very challenging, because it
requires the extractor to be able to capture both local and non-local image
context in order to determine the scale of each skeleton pixel. In this paper,
we present a fully convolutional network with multiple scale-associated side
outputs to address this problem. By observing the relationship between the
receptive field sizes of the different layers in the network and the skeleton
scales they can capture, we introduce two scale-associated side outputs to each
stage of the network. The network is trained by multi-task learning, where one
task is skeleton localization to classify whether a pixel is a skeleton pixel
or not and the other is skeleton scale prediction to regress the scale of each
skeleton pixel. Supervision is imposed at different stages by guiding the
scale-associated side outputs toward the groundtruth skeletons at the
appropriate scales. The responses of the multiple scale-associated side outputs
are then fused in a scale-specific way to detect skeleton pixels using multiple
scales effectively. Our method achieves promising results on two skeleton
extraction datasets, and significantly outperforms other competitors.
Additionally, the usefulness of the obtained skeletons and scales (thickness)
are verified on two object detection applications: Foreground object
segmentation and object proposal detection.
</p>
{{{{ARTICLE_PARSER}}}}<p>Text simplification (TS) aims to reduce the lexical and structural complexity
of a text, while still retaining the semantic meaning. Current automatic TS
techniques are limited to either lexical-level applications or manually
defining a large amount of rules. Since deep neural networks are powerful
models that have achieved excellent performance over many difficult tasks, in
this paper, we propose to use the Long Short-Term Memory (LSTM) Encoder-Decoder
model for sentence level TS, which makes minimal assumptions about word
sequence. We conduct preliminary experiments to find that the model is able to
learn operation rules such as reversing, sorting and replacing from sequence
pairs, which shows that the model may potentially discover and apply rules such
as modifying sentence structure, substituting words, and removing words for TS.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider a class of pursuit-evasion problems where an evader enters a
directed acyclic graph and attempts to reach one of the terminal nodes. A
pursuer enters the graph at a later time and attempts to capture the evader
before it reaches a terminal node. The pursuer can only obtain information
about the evader's path via sensors located at each node in the graph; the
sensor measurements are either green or red (indicating whether or not the
evader has passed through that node). We first show that it is NP-hard to
determine whether the pursuer can enter with some nonzero delay and still be
guaranteed to capture the evader, even for the simplest case when the
underlying graph is a tree. This also implies that it is NP-hard to determine
the largest delay at which the pursuer can enter and still have a guaranteed
capture policy. We further show that it is NP-hard to approximate (within any
constant factor) the largest delay at which the pursuer can enter. Finally, we
provide an algorithm to compute the maximum pursuer delay for a class of
node-sweeping policies on tree networks and show that this algorithm runs in
linear-time for bounded-degree trees.
</p>
{{{{ARTICLE_PARSER}}}}<p>Several recent deep neural networks experiments leverage the
generalist-specialist paradigm for classification. However, no formal study
compared the performance of different clustering algorithms for class
assignment. In this paper we perform such a study, suggest slight modifications
to the clustering procedures, and propose a novel algorithm designed to
optimize the performance of of the specialist-generalist classification system.
Our experiments on the CIFAR-10 and CIFAR-100 datasets allow us to investigate
situations for varying number of classes on similar data. We find that our
\emph{greedy pairs} clustering algorithm consistently outperforms other
alternatives, while the choice of the confusion matrix has little impact on the
final performance.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider the longest common subsequence (LCS) problem with the restriction
that the common subsequence is required to consist of at least $k$ length
substrings. First, we show an $O(mn)$ time algorithm for the problem that
improves the worst-case running time of the existing algorithm, where $m$ and
$n$ are lengths of the input strings. Furthermore, we mainly consider the LCS
in at least $k$ length order-isomorphic substrings problem. We show that the
problem can also be solved in $O(mn)$ worst-case time by an easy-to-implement
algorithm.
</p>
{{{{ARTICLE_PARSER}}}}<p>Recommender systems often use latent features to explain the behaviors of
users and capture the properties of items. As users interact with different
items over time, user and item features can influence each other, evolve and
co-evolve over time. To accurately capture the fine grained nonlinear
coevolution of these features, we propose a recurrent coevolutionary feature
embedding process model, which combines recurrent neural network (RNN) with a
multidimensional point process model. The RNN learns a nonlinear representation
of user and item features which take into account mutual influence between user
and item features, and the feature evolution over time. We also develop an
efficient stochastic gradient algorithm for learning the model parameters,
which can readily scale up to millions of events. Experiments on diverse
real-world datasets demonstrate significant improvements in user behavior
prediction compared to state-of-the-arts.
</p>
{{{{ARTICLE_PARSER}}}}<p>Learning based methods have shown very promising results for the task of
depth estimation in single images. However, most existing approaches treat
depth prediction as a supervised regression problem and as a result, require
vast quantities of corresponding ground truth depth data for training. Just
recording quality depth data in a range of environments is a challenging
problem. In this paper, we innovate beyond existing approaches, replacing the
use of explicit depth data during training with easier-to-obtain binocular
stereo footage.
</p>
<p>We propose a novel training objective that enables our convolutional neural
network to learn to perform single image depth estimation, despite the absence
of ground truth depth data. By exploiting epipolar geometry constraints, we
generate disparity images by training our networks with an image reconstruction
loss. We show that solving for image reconstruction alone results in poor
quality depth images. To overcome this problem, we propose a novel training
loss that enforces consistency between the disparities produced relative to
both the left and right images, leading to improved performance and robustness
compared to existing approaches. Our method produces state of the art results
for monocular depth estimation on the KITTI driving dataset, even outperforming
supervised methods that have been trained with ground truth depth.
</p>
{{{{ARTICLE_PARSER}}}}<p>We present a theoretically grounded approach to train deep neural networks,
including recurrent networks, subject to class-dependent label noise. Our
method only performs a correction on the loss function, and is agnostic to both
the application domain and network architecture. We propose two procedures for
loss correction: they simply amount to at most a matrix inversion and
multiplication, provided that we know the probability of each class being
corrupted into another. We further show how one can estimate these
probabilities, adapting a recent technique for noise estimation to the
multi-class setting, and thus providing an end-to-end framework. Extensive
experiments on MNIST, IMDB, CIFAR-10, CIFAR-100 employing a diversity of
architectures --- stacking dense, convolutional, pooling, dropout, batch
normalization, word embedding, LSTM and residual layers --- demonstrate the
noise robustness of our proposals. Incidentally, we also prove that, when ReLU
is the only non-linearity, the loss curvature is immune to class-dependent
label noise.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper we consider Type 1 Gray maps and Type 2 Gray maps for groups of
order 16. First, we confirm that we can construct Type 1 Gray maps for all
groups of order 16, and we actually construct them. Next, we confirm that we
can construct Type 2 Gray maps for several groups of order 16, and we construct
all such maps. Finally we reveal why the other groups do not admit Gray maps.
</p>
{{{{ARTICLE_PARSER}}}}<p>The virtuality continuum describes the degrees of positive virtuality under
the umbrella term mixed reality. Besides adding virtual information within a
mixed environment, diminished reality aims at reducing real world information.
Mann defined the term mediated reality (MR), which also considered diminished
reality, but without the possibility to describe different degrees of fusion
between a mixed and a diminished reality. That is why this work defines the new
term blending entropy that captures the relations between a mixed and a
diminished reality. The blending entropy is based on the information density of
the mediated reality and the actual area the user has to comprehend, which is
named perceptual frustum. We describe the blending entropy's twodimensional
dependencies and detail important points in the blending entropy's space.
</p>
{{{{ARTICLE_PARSER}}}}<p>In the relay based telecommunications with $K$ relays between the source and
destination, $K+1$ time or frequency slots are required for a single frame
transmission. However, without the relays, only one time or frequency slot is
used for a single frame transmission. Therefore, despite the benefits of
relaying systems, this type of communications is not efficient from the
spectral efficiency viewpoint. One solution to reduce this issue might be the
full-duplex (FD) relays. An old technique which is reconsidered recently to
improve the spectral efficiency of telecommunication systems. However, FD
relays have a certain complexity, so, some similar techniques such as
successive relays with nearly the same performance but less complexity is taken
into account now. In successive relaying systems, two relays between the source
and destination are employed which receive the transmitted frames from the
source and relay it to the destination successively. This structure generally
acts like an FD relays. In this paper, the effective capacity performance of an
amplify and forward (AF) successive relaying systems with power allocation
strategy at the relays are studied perfectly. However, while the inter-rely
interference (IRI) between two successive relays has to be managed well, the
power allocation and the effective capacity is derived under different
assumptions about the IRI. In this way, we assume weak or strong, short or
long-term constraints on the IRI. Then we extract the optimal transmitted power
at the relay to maximize the effective capacity under these constraints.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we study diffusion social learning over weakly-connected
graphs. We show that the asymmetric flow of information hinders the learning
abilities of certain agents regardless of their local observations. Under some
circumstances that we clarify in this work, a scenario of total influence (or
\"mind-control\") arises where a set of influential agents ends up shaping the
beliefs of non-influential agents. We derive useful closed-form expressions
that characterize this influence, and which can be used to motivate design
problems to control it. We provide simulation examples to illustrate the
results.
</p>
{{{{ARTICLE_PARSER}}}}<p>The revelation principle is a fundamental theorem in many economics fields
such as game theory, mechanism design and auction theory etc. In this paper, I
construct an example to show that a social choice function which can be
implemented in Bayesian Nash equilibrium is not truthfully implementable. The
key point is that agents pay cost in the indirect mechanism, but pay nothing in
the direct mechanism. As a result, the revelation principle may not hold when
agent's cost cannot be neglected in the indirect mechanism.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, a novel proximity and load-aware resource allocation for
vehicle-to-vehicle (V2V) communication is proposed. The proposed approach
exploits the spatio-temporal traffic patterns, in terms of load and vehicles'
physical proximity, to minimize the total network cost which captures the
tradeoffs between load (i.e., service delay) and successful transmissions while
satisfying vehicles's quality-of-service (QoS) requirements. To solve the
optimization problem under slowly varying channel information, it is decoupled
the problem into two interrelated subproblems. First, a dynamic clustering
mechanism is proposed to group vehicles in zones based on their traffic
patterns and proximity information. Second, a matching game is proposed to
allocate resources for each V2V pair within each zone. The problem is cast as
many-to-one matching game in which V2V pairs and resource blocks (RBs) rank one
another in order to minimize their service delay. The proposed game is shown to
belong to the class of matching games with externalities. To solve this game, a
distributed algorithm is proposed using which V2V pairs and RBs interact to
reach a stable matching. Simulation results for a Manhattan model shown that
the proposed scheme yields a higher percentage of V2V pairs satisfying QoS as
well as significant gain in terms of the signal-to-interference-plus-noise
ratio (SINR) as compared to a state-of-art resource allocation baseline.
</p>
{{{{ARTICLE_PARSER}}}}<p>A map $\varphi:K\to R^2$ of a graph $K$ is approximable by embeddings, if for
each $\varepsilon&gt;0$ there is an $\varepsilon$-close to $\varphi$ embedding
$f:K\to R^2$. Analogous notions were studied in computer science under the
names of cluster planarity and weak simplicity. In this survey we present
criteria for approximability by embeddings (P. Minc, 1997, M. Skopenkov, 2003)
and their algorithmic corollaries. We introduce the van Kampen (or
Hanani-Tutte) obstruction for approximability by embeddings and discuss its
completeness. We discuss analogous problems of moving graphs in the plane apart
(cf. S. Spiez and H. Torunczyk, 1991) and finding closest embeddings (H.
Edelsbrunner). We present higher dimensional van Kampen obstruction, its
completeness result and algorithmic corollary (D. Repovs and A. Skopenkov,
1998).
</p>
{{{{ARTICLE_PARSER}}}}<p>In this thesis, we develop multiscale models for particle simulations in
population dynamics. These models are characterised by prescribing particle
motion on two spatial scales: microscopic and macroscopic. At the microscopic
level, each particle has its own mass, position and velocity, while at the
macroscopic level the particles are interpolated to a continuum quantity whose
evolution is governed by a system of transport equations. This way, one can
prescribe various types of interactions on a global scale, whilst still
maintaining high simulation speed for a large number of particles. In addition,
the interplay between particle motion and interaction is well tuned in both
regions of low and high densities.
</p>
<p>We analyse links between models on these two scales and prove that under
certain conditions, a system of interacting particles converges to a nonlinear
coupled system of transport equations. We use this as a motivation to derive a
model defined on both modelling scales and prescribe the intercommunication
between them. Simulation takes place in inhomogeneous domains with arbitrary
conditions at inflow and outflow boundaries. We realise this by modelling
obstacles, sources and sinks. Integrating these aspects into the simulation
requires a route planning algorithm for the particles. Several algorithms are
considered and evaluated on accuracy, robustness and efficiency.
</p>
<p>All aspects mentioned above are combined in a novel open source prototyping
simulation framework called Mercurial. This computational framework allows the
design of geometries and is built for high performance when large numbers of
particles are involved. Mercurial supports various types of inhomogeneities and
global systems of equations.
</p>
<p>We apply our framework to simulate scenarios in crowd dynamics. We compare
our results with test cases from literature to assess the quality of the
simulations.
</p>
{{{{ARTICLE_PARSER}}}}<p>One of the major issues of cryptography is the cryptanalysis of cipher
algorithms. Cryptanalysis is the study of methods for obtaining the meaning of
encrypted information, without access to the secret information that is
normally required. Some mechanisms for breaking codes include differential
cryptanalysis, advanced statistics and brute-force.
</p>
<p>Recent works also attempt to use algebraic tools to reduce the cryptanalysis
of a block cipher algorithm to the resolution of a system of quadratic
equations describing the ciphering structure.
</p>
<p>In our study, we will also use algebraic tools but in a new way: by using
Boolean functions and their properties. A Boolean function is a function from
$F_2^n\to F_2$ with $n&gt;1$, characterized by its truth table. The arguments of
Boolean functions are binary words of length $n$. Any Boolean function can be
represented, uniquely, by its algebraic normal form which is an equation which
only contains additions modulo 2 - the XOR function - and multiplications
modulo 2 - the AND function.
</p>
<p>Our aim is to describe the AES algorithm as a set of Boolean functions then
calculate their algebraic normal forms by using the M\\"obius transforms. After,
we use a specific representation for these equations to facilitate their
analysis and particularly to try a combinatorial analysis. Through this
approach we obtain a new kind of equations system. This equations system is
more easily implementable and could open new ways to cryptanalysis.
</p>
{{{{ARTICLE_PARSER}}}}<p>Initially developed for the min-knapsack problem, the knapsack cover
inequalities are used in the current best relaxations for numerous
combinatorial optimization problems of covering type. In spite of their
widespread use, these inequalities yield linear programming (LP) relaxations of
exponential size, over which it is not known how to optimize exactly in
polynomial time. In this paper we address this issue and obtain LP relaxations
of quasi-polynomial size that are at least as strong as that given by the
knapsack cover inequalities.
</p>
<p>For the min-knapsack cover problem, our main result can be stated formally as
follows: for any $\varepsilon &gt;0$, there is a $(1/\varepsilon)^{O(1)}n^{O(\log
n)}$-size LP relaxation with an integrality gap of at most $2+\varepsilon$,
where $n$ is the number of items. Prior to this work, there was no known
relaxation of subexponential size with a constant upper bound on the
integrality gap.
</p>
<p>Our construction is inspired by a connection between extended formulations
and monotone circuit complexity via Karchmer-Wigderson games. In particular,
our LP is based on $O(\log^2 n)$-depth monotone circuits with fan-in~$2$ for
evaluating weighted threshold functions with $n$ inputs, as constructed by
Beimel and Weinreb. We believe that a further understanding of this connection
may lead to more positive results complementing the numerous lower bounds
recently proved for extended formulations.
</p>
{{{{ARTICLE_PARSER}}}}<p>Data analyses in the life sciences are moving from tools run on a personal
computer to services run on large computing platforms. This creates a need to
package tools and dependencies for easy installation, configuration and
deployment on distributed platforms. In addition, for secure execution there is
a need for process isolation on a shared platform. Existing virtual machine and
container technologies are often more complex than traditional Unix utilities,
like chroot, and often require root privileges in order to set up or use. This
is especially challenging on HPC systems where users typically do not have root
access. We therefore present nsroot, a lightweight Linux namespaces based
process isolation tool. It allows restricting the runtime environment of data
analysis tools that may not have been designed with security as a top priority,
in order to reduce the risk and consequences of security breaches, without
requiring any special privileges. The codebase of nsroot is small, and it
provides a command line interface similar to chroot. It can be used on all
Linux kernels that implement user namespaces. In addition, we propose combining
nsroot with the AppImage format for secure execution of packaged applications.
nsroot is open sourced and available at: https://github.com/uit-no/nsroot
</p>
{{{{ARTICLE_PARSER}}}}<p>We analyze a coupled dataset collecting the mobile phone communication and
bank transactions history of a large number of individuals living in Mexico.
After mapping the social structure and introducing indicators of socioeconomic
status, demographic features, and purchasing habits of individuals we show that
typical consumption patterns are strongly correlated with identified
socioeconomic classes leading to patterns of stratification in the social
structure. In addition we measure correlations between merchant categories and
introduce a correlation network, which emerges with a meaningful community
structure. We detect multivariate relations between merchant categories and
show correlations in purchasing habits of individuals. Our work provides novel
and detailed insight into the relations between social and consuming behaviour
with potential applications in recommendation system design.
</p>
{{{{ARTICLE_PARSER}}}}<p>Intelligent control of robotic arms has huge potential over the coming years,
but as of now will often fail to adapt when presented with new and unfamiliar
environments. Recent trends to solve this problem have seen a shift to
end-to-end solutions using deep reinforcement learning to learn policies from
visual input, rather than relying on a handcrafted, modular pipeline. Building
upon the recent success of deep Q-networks, we present an approach which uses
three-dimensional simulations to train a 7-DOF robotic arm in a robot arm
control task without any prior knowledge. Policies accept images of the
environment as input and output motor actions. However, the high-dimensionality
of the policies as well as the large state space makes policy search difficult.
This is overcome by ensuring interesting states are explored via intermediate
rewards that guide the policy towards higher reward states. Our results
demonstrate that deep Q-networks can be used to learn policies for a task that
involves locating a cube, grasping, and then finally lifting. The agent is able
to learn to deal with a range of starting joint configurations and starting
cube positions when tested in simulation. Moreover, we show that policies
trained via simulation have the potential to be directly applied to real-world
equivalents without any further training. We believe that robot simulations can
decrease the dependency on physical robots and ultimately improve productivity
of training robot control tasks.
</p>
{{{{ARTICLE_PARSER}}}}<p>In underlay heterogeneous networks (HetNets), the distance between a macro
base station (MBS) and a macro user (MU) is crucial for a small-cell based
station (SBS) to control the interference to the MU and achieve the
coexistence. To obtain the distance between the MBS and the MU, the SBS needs a
backhaul link from the macro system, such that the macro system is able to
transmit the information of the distance to the SBS through the backhaul link.
However, there may not exist any backhaul link from the macro system to the SBS
in practical situations. Thus, it is challenging for the SBS to obtain the
distance. To deal with this issue, we propose a median based (MB) estimator for
the SBS to obtain the distance between the MBS and the MU without any backhaul
link. Numerical results show that the estimation error of the MB estimator can
be as small as $4\%$.
</p>
{{{{ARTICLE_PARSER}}}}<p>Graph aggregation is the process of computing a single output graph that
constitutes a good compromise between several input graphs, each provided by a
different source. One needs to perform graph aggregation in a wide variety of
situations, e.g., when applying a voting rule (graphs as preference orders),
when consolidating conflicting views regarding the relationships between
arguments in a debate (graphs as abstract argumentation frameworks), or when
computing a consensus between several alternative clusterings of a given
dataset (graphs as equivalence relations). In this paper, we introduce a formal
framework for graph aggregation grounded in social choice theory. Our focus is
on understanding which properties shared by the individual input graphs will
transfer to the output graph returned by a given aggregation rule. We consider
both common properties of graphs, such as transitivity and reflexivity, and
arbitrary properties expressible in certain fragments of modal logic. Our
results establish several connections between the types of properties preserved
under aggregation and the choice-theoretic axioms satisfied by the rules used.
The most important of these results is a powerful impossibility theorem that
generalises Arrow's seminal result for the aggregation of preference orders to
a large collection of different types of graphs.
</p>
{{{{ARTICLE_PARSER}}}}<p>Creative telescoping is the method of choice for obtaining information about
definite sums or integrals. It has been intensively studied since the early
1990s, and can now be considered as a classical technique in computer algebra.
At the same time, it is still subject of ongoing research. In this paper, we
present a selection of open problems in this context. We would be curious to
hear about any substantial progress on any of these problems.
</p>
{{{{ARTICLE_PARSER}}}}<p>We derive a new proof to show that the incremental resparsification algorithm
proposed by Kelner and Levin (2013) produces a spectral sparsifier in high
probability. We rigorously take into account the dependencies across subsequent
resparsifications using martingale inequalities, fixing a flaw in the original
analysis.
</p>
{{{{ARTICLE_PARSER}}}}<p>We formulate and analyze a graphical model selection method for inferring the
conditional independence graph of a high-dimensional nonstationary Gaussian
random process (time series) from a finite-length observation. The observed
process samples are assumed uncorrelated over time and having a time-varying
marginal distribution. The selection method is based on testing conditional
variances obtained for small subsets of process components. This allows to cope
with the high-dimensional regime, where the sample size can be (drastically)
smaller than the process dimension. We characterize the required sample size
such that the proposed selection method is successful with high probability.
</p>
{{{{ARTICLE_PARSER}}}}<p>Pose estimation, tracking, and action recognition of articulated objects from
depth images are important and challenging problems, which are normally
considered separately. In this paper, a unified paradigm based on Lie group
theory is proposed, which enables us to collectively address these related
problems. Our approach is also applicable to a wide range of articulated
objects. Empirically it is evaluated on lab animals including mouse and fish,
as well as on human hand. On these applications, it is shown to deliver
competitive results compared to the state-of-the-arts, and non-trivial
baselines including convolutional neural networks and regression forest
methods.
</p>
{{{{ARTICLE_PARSER}}}}<p>Recurrent neural network (RNN) based character-level language models (CLMs)
are extremely useful for modeling unseen words by nature. However, their
performance is generally much worse than the word-level language models (WLMs),
since CLMs need to consider longer history of tokens to properly predict the
next one. We address this problem by proposing hierarchical RNN architectures,
which consist of multiple modules with different clock rates. Despite the
multi-clock structures, the input and output layers operate with the
character-level clock, which allows the existing RNN CLM training approaches to
be directly applicable without any modifications. Our CLM models show better
perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word Benchmark
with only 2% of parameters. Also, we present real-time character-level
end-to-end speech recognition examples on the Wall Street Journal (WSJ) corpus,
where replacing traditional mono-clock RNN CLMs with the proposed models
results in better recognition accuracies even though the number of parameters
are reduced to 30%.
</p>
{{{{ARTICLE_PARSER}}}}<p>In 2013, Boneh and Zhandry introduced the notion of indistinguishability
(IND) in chosen plaintext (CPA) and chosen ciphertext (CCA) attacks by a
quantum adversary which is given superposition access to an oracle for
encryption and decryption queries but is restricted to classical queries in the
challenge phase. In this paper we define IND-CPA and IND-CCA notions for
symmetric encryption schemes where the adversary has full quantum superposition
access to the oracle, and give constructions that achieve these security
notions. Our results are formulated in the concrete security framework.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper proposes a decentralized algorithm for solving a consensus
optimization problem defined in a \emph{directed} networked multi-agent system,
where the local objective functions have the smooth+nonsmooth composite form,
and are possibly \emph{nonconvex}. Examples of such problems include
decentralized compressed sensing and constrained quadratic programming
problems, as well as many decentralized regularization problems. We extend the
existing algorithms PG-EXTRA and ExtraPush to a new algorithm
\emph{PG-ExtraPush} for composite consensus optimization over a \emph{directed}
network. This algorithm takes advantage of the proximity operator like in
PG-EXTRA to deal with the nonsmooth term, and employs the push-sum protocol
like in ExtraPush to tackle the bias introduced by the directed network. We
show that PG-ExtraPush converges to an optimal solution under the boundedness
assumption. In numerical experiments, with a proper step size, PG-ExtraPush
performs surprisingly linear rates in most of cases, even in some nonconvex
cases.
</p>
{{{{ARTICLE_PARSER}}}}<p>Hierarchical feature learning based on convolutional neural networks (CNN)
has recently shown significant potential in various computer vision tasks.
While allowing high-quality discriminative feature learning, the downside of
CNNs is the lack of explicit structure in features, which often leads to
overfitting, absence of reconstruction from partial observations and limited
generative abilities. Explicit structure is inherent in hierarchical
compositional models, however, these lack the ability to optimize a
well-defined cost function. We propose a novel analytic model of a basic unit
in a layered hierarchical model with both explicit compositional structure and
a well-defined discriminative cost function. Our experiments on two datasets
show that the proposed compositional model performs on a par with standard CNNs
on discriminative tasks, while, due to explicit modeling of the structure in
the feature units, affording a straight-forward visualization of parts and
faster inference due to separability of the units. Actions
</p>
{{{{ARTICLE_PARSER}}}}<p>The aim of this paper is to develop and test metrics to quantitatively
identify technological discontinuities in a knowledge network. We developed
five metrics based on innovation theories and tested the metrics by a
simulation model-based knowledge network and hypothetically designed
discontinuity. The designed discontinuity is modeled as a node which combines
two different knowledge streams and whose knowledge is dominantly persistent in
the knowledge network. The performances of the proposed metrics were evaluated
by how well the metrics can distinguish the designed discontinuity from other
nodes on the knowledge network. The simulation results show that the
persistence times # of converging main paths provides the best performance in
identifying the designed discontinuity: the designed discontinuity was
identified as one of the top 3 patents with 96~99% probability by Metric 5 and
it is, according to the size of a domain, 12~34% better than the performance of
the second best metric. Beyond the simulation analysis, we tested the metrics
using a patent set representative of the Magnetic information storage domain.
The three representative patents associated with a well-known breakthrough
technology in the domain, the giant magneto-resistance (GMR) spin valve sensor,
were selected based on the qualitative studies, and the metrics were tested by
how well the metrics identify the selected patents as top-ranked patents. The
empirical results fully support the simulation results and therefore the
persistence times # of converging main paths is recommended for identifying
technological discontinuities for any technology.
</p>
{{{{ARTICLE_PARSER}}}}<p>Human age estimation has attracted increasing researches due to its wide
applicability in such as security monitoring and advertisement recommendation.
Although a variety of methods have been proposed, most of them focus only on
the age-specific facial appearance. However, biological researches have shown
that not only gender but also the aging difference between the male and the
female inevitably affect the age estimation. To our knowledge, so far there
have been two methods that have concerned the gender factor. The first is a
sequential method which first classifies the gender and then performs age
estimation respectively for classified male and female. Although it promotes
age estimation performance because of its consideration on the gender semantic
difference, an accumulation risk of estimation errors is unavoidable. To
overcome drawbacks of the sequential strategy, the second is to regress the age
appended with the gender by concatenating their labels as two dimensional
output using Partial Least Squares (PLS). Although leading to promotion of age
estimation performance, such a concatenation not only likely confuses the
semantics between the gender and age, but also ignores the aging discrepancy
between the male and the female. In order to overcome their shortcomings, in
this paper we propose a unified framework to perform gender-aware age
estimation. The proposed method considers and utilizes not only the semantic
relationship between the gender and the age, but also the aging discrepancy
between the male and the female. Finally, experimental results demonstrate not
only the superiority of our method in performance, but also its good
interpretability in revealing the aging discrepancy.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we consider a multiple-input multiple-output wireless powered
communication network (MIMO-WPCN), where multiple users harvest energy from a
dedicated power station in order to be able to transmit their information
signals to an information receiving station. Employing a practical non-linear
energy harvesting (EH) model, we propose a joint time allocation and power
control scheme, which takes into account the uncertainty regarding the channel
state information (CSI) and provides robustness against imperfect CSI
knowledge. In particular, we formulate two non-convex optimization problems for
different objectives, namely system sum throughput maximization and
maximization of the minimum individual throughput across all wireless powered
users. To overcome the non-convexity, we apply several transformations along
with a one-dimensional search to obtain an efficient resource allocation
algorithm. Numerical results reveal that a significant performance gain can be
achieved when the resource allocation is designed based on the adopted
non-linear EH model instead of the conventional linear EH model. Besides,
unlike a non-robust baseline scheme designed for perfect CSI, the proposed
resource allocation schemes are shown to be robust against imperfect CSI
knowledge.
</p>
{{{{ARTICLE_PARSER}}}}<p>Recently, Dohrau et al. studied a zero-player game on switch graphs and
proved that deciding the termination of the game is in NP $\cap$ coNP. In this
short paper, we show that the search version of this game on switch graphs,
i.e., the task of finding a witness of termination (or of non-termination) is
in PLS.
</p>
{{{{ARTICLE_PARSER}}}}<p>PDDL+ planning has its semantics rooted in hybrid automata (HA) and recent
work has shown that it can be modeled as a network of HAs. Addressing the
complexity of nonlinear PDDL+ planning as HAs requires both space and time
efficient reasoning. Unfortunately, existing solvers either do not address
nonlinear dynamics or do not natively support networks of automata.
</p>
<p>We present a new algorithm, called HNSolve, which guides the variable
selection of the dReal Satisfiability Modulo Theories (SMT) solver while
reasoning about network encodings of nonlinear PDDL+ planning as HAs. HNSolve
tightly integrates with dReal by solving a discrete abstraction of the HA
network. HNSolve finds composite runs on the HA network that ignore continuous
variables, but respect mode jumps and synchronization labels. HNSolve
admissibly detects dead-ends in the discrete abstraction, and posts conflict
clauses that prune the SMT solver's search. We evaluate the benefits of our
HNSolve algorithm on PDDL+ benchmark problems and demonstrate its performance
with respect to prior work.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we model the salient object detection problem under a
probabilistic framework encoding the boundary connectivity saliency cue and
smoothness constraints in an optimization problem. We show that this problem
has a closed form global optimum which estimates the salient object. We further
show that along with the probabilistic framework, the proposed method also
enjoys a wide range of interpretations, i.e. graph cut, diffusion maps and
one-class classification. With an analysis according to these interpretations,
we also find that our proposed method provides approximations to the global
optimum to another criterion that integrates local/global contrast and large
area saliency cues. The proposed approach achieves leading performance compared
to the state-of-the-art algorithms over a large set of salient object detection
datasets including around 17k images for several evaluation metrics.
Furthermore, the computational complexity of the proposed method is
favorable/comparable to many state-of-the-art techniques.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper considers how to separate text and/or graphics from smooth
background in screen content and mixed content images and proposes an algorithm
to perform this segmentation task. The proposed methods make use of the fact
that the background in each block is usually smoothly varying and can be
modeled well by a linear combination of a few smoothly varying basis functions,
while the foreground text and graphics create sharp discontinuity. This
algorithm separates the background and foreground pixels by trying to fit pixel
values in the block into a smooth function using a robust regression method.
The inlier pixels that can be well represented with the smooth model will be
considered as background, while remaining outlier pixels will be considered
foreground. We have also created a dataset of screen content images extracted
from HEVC standard test sequences for screen content coding with their ground
truth segmentation result which can be used for this task. The proposed
algorithm has been tested on the dataset mentioned above and is shown to have
superior performance over other methods, such as the hierarchical k-means
clustering algorithm, shape primitive extraction and coding, and the least
absolute deviation fitting scheme for foreground segmentation.
</p>
{{{{ARTICLE_PARSER}}}}<p>Cooperative Intelligent Transport Systems (C-ITS) refers to applications
using vehicle-to-vehicle and vehicle-to-infrastructure communications at a
carrier frequency of 5.9 GHz to increase road traffic safety and road traffic
efficiency in Europe (a.k.a. connected vehicle technology in the US). This
article will shed some light on the current status of C-ITS in Europe and what
is left before deployment can commence in 2019 as announced by C2C-CC. Even
though there is an immense activity for the launch of C-ITS in Europe, the
automotive industry is also planning for the future.
</p>
{{{{ARTICLE_PARSER}}}}<p>Robust face representation is imperative to highly accurate face recognition.
In this work, we propose an open source face recognition method with deep
representation named as VIPLFaceNet, which is a 10-layer deep convolutional
neural network with 7 convolutional layers and 3 fully-connected layers.
Compared with the well-known AlexNet, our VIPLFaceNet takes only 20% training
time and 60% testing time, but achieves 40\% drop in error rate on the
real-world face recognition benchmark LFW. Our VIPLFaceNet achieves 98.60% mean
accuracy on LFW using one single network. An open-source C++ SDK based on
VIPLFaceNet is released under BSD license. The SDK takes about 150ms to process
one face image in a single thread on an i7 desktop CPU. VIPLFaceNet provides a
state-of-the-art start point for both academic and industrial face recognition
applications.
</p>
{{{{ARTICLE_PARSER}}}}<p>Diffusion Magnetic Resonance Imaging (MRI) exploits the anisotropic diffusion
of water molecules in the brain to enable the estimation of the brain's
anatomical fiber tracts at a relatively high resolution. In particular,
tractographic methods can be used to generate whole-brain anatomical
connectivity matrix where each element provides an estimate of the connectivity
strength between the corresponding voxels. Structural brain networks are built
using the connectivity information and a predefined brain parcellation, where
the nodes of the network represent the brain regions and the edge weights
capture the connectivity strengths between the corresponding brain regions.
This paper introduces a number of novel scalable methods to generate and
analyze structural brain networks with a varying number of nodes. In
particular, we introduce a new parallel algorithm to quickly generate large
scale connectivity-based parcellations for which voxels in a region possess
highly similar connectivity patterns to the rest of the regions. We show that
the corresponding regional structural consistency is always superior to
randomly generated parcellations over a wide range of parcellation sizes.
Corresponding brain networks with a varying number of nodes are analyzed using
standard graph-theorectic measures, as well as, new measures derived from
spectral graph theory. Our results indicate increasingly more statistical power
of brain networks with larger numbers of nodes and the relatively unique shape
of the spectral profile of large brain networks relative to other well-known
networks.
</p>
{{{{ARTICLE_PARSER}}}}<p>Convolutional Neural Networks (CNNs) were recently shown to provide
state-of-the-art results for object category viewpoint estimation. However
different ways of formulating this problem have been proposed and the competing
approaches have been explored with very different design choices. This paper
presents a comparison of these approaches in a unified setting as well as a
detailed analysis of the key factors that impact performance. Followingly, we
present a new joint training method with the detection task and demonstrate its
benefit. We also highlight the superiority of classification approaches over
regression approaches, quantify the benefits of deeper architectures and
extended training data, and demonstrate that synthetic data is beneficial even
when using ImageNet training data. By combining all these elements, we
demonstrate an improvement of approximately 5% mAVP over previous
state-of-the-art results on the Pascal3D+ dataset. In particular for their most
challenging 24 view classification task we improve the results from 31.1% to
36.1% mAVP.
</p>
{{{{ARTICLE_PARSER}}}}<p>Development of large computerized systems requires both combinational and
sequential circuits. Registers and counters are two important examples of
sequential circuits, which are widely used in practical applications like CPUs.
The basic element of sequential logic is Flip-Flop, which stores an input value
and returns two outputs (Q and Q_bar). This paper presents an innovative
ternary D Flip-Flap-Flop, which offers circuit designers to customize their
design by eliminating one of the outputs if it is not required. This unique
feature of the new design leads to considerable power reduction in comparison
with the previously presented structures. The proposed design is simulated and
tested by HSPICE and 45 nm CMOS technology.
</p>
{{{{ARTICLE_PARSER}}}}<p>Information theoretic measures (e.g. the Kullback Liebler divergence and
Shannon mutual information) have been used for exploring possibly nonlinear
multivariate dependencies in high dimension. If these dependencies are assumed
to follow a Markov factor graph model, this exploration process is called
structure discovery. For discrete-valued samples, estimates of the information
divergence over the parametric class of multinomial models lead to structure
discovery methods whose mean squared error achieves parametric convergence
rates as the sample size grows. However, a naive application of this method to
continuous nonparametric multivariate models converges much more slowly. In
this paper we introduce a new method for nonparametric structure discovery that
uses weighted ensemble divergence estimators that achieve parametric
convergence rates and obey an asymptotic central limit theorem that facilitates
hypothesis testing and other types of statistical validation.
</p>
{{{{ARTICLE_PARSER}}}}<p>The open source ALPS (Algorithms and Libraries for Physics Simulations)
project provides a collection of physics libraries and applications, with a
focus on simulations of lattice models and strongly correlated systems. The
libraries provide a convenient set of well-documented and reusable components
for developing condensed matter physics simulation code, and the applications
strive to make commonly used and proven computational algorithms available to a
non-expert community. In this paper we present an updated and refactored
version of the core ALPS libraries geared at the computational physics software
development community, rewritten with focus on documentation, ease of
installation, and software maintainability.
</p>
{{{{ARTICLE_PARSER}}}}<p>We apply a novel spectral graph technique, that of locally-biased
semi-supervised eigenvectors, to study the diversity of galaxies. This
technique permits us to characterize empirically the natural variations in
observed spectra data, and we illustrate how this approach can be used in an
exploratory manner to highlight both large-scale global as well as small-scale
local structure in Sloan Digital Sky Survey (SDSS) data. We use this method in
a way that simultaneously takes into account the measurements of spectral lines
as well as the continuum shape. Unlike Principal Component Analysis, this
method does not assume that the Euclidean distance between galaxy spectra is a
good global measure of similarity between all spectra, but instead it only
assumes that local difference information between similar spectra is reliable.
Moreover, unlike other nonlinear dimensionality methods, this method can be
used to characterize very finely both small-scale local as well as large-scale
global properties of realistic noisy data. The power of the method is
demonstrated on the SDSS Main Galaxy Sample by illustrating that the derived
embeddings of spectra carry an unprecedented amount of information. By using a
straightforward global or unsupervised variant, we observe that the main
features correlate strongly with star formation rate and that they clearly
separate active galactic nuclei. Computed parameters of the method can be used
to describe line strengths and their interdependencies. By using a
locally-biased or semi-supervised variant, we are able to focus on typical
variations around specific objects of astronomical interest. We present several
examples illustrating that this approach can enable new discoveries in the data
as well as a detailed understanding of very fine local structure that would
otherwise be overwhelmed by large-scale noise and global trends in the data.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider the problem of fairly dividing a two-dimensional heterogeneous
resource among several agents with different preferences. Potential
applications include dividing land-estates among heirs, museum space among
presenters or space in print and electronic media among advertisers. Classic
cake-cutting procedures either consider a one-dimensional resource, or allocate
each agent a collection of disconnected pieces. In practice, however, the
two-dimensional shape of the allotted piece is of crucial importance in many
applications. For example, when building houses or designing advertisements, in
order to be useful, the allotments should be squares or rectangles with bounded
aspect-ratio. We thus introduce the problem of fair two-dimensional division
wherein the allocated piece must have a pre-specified geometric shape. We
present constructive cake-cutting procedures that satisfy the two most
prominent fairness criteria, namely envy-freeness and proportionality. In
scenarios where proportionality cannot be achieved due to the geometric
constraints, our procedures provide a partially-proportional division,
guaranteeing that the fraction allocated to each agent be at least a certain
positive constant. We prove that in many natural scenarios the envy-freeness
requirement is compatible with the best attainable partial-proportionality.
</p>
{{{{ARTICLE_PARSER}}}}<p>Networks extracted from social media platforms frequently include multiple
types of links that dynamically change over time; these links can be used to
represent dyadic interactions such as economic transactions, communications,
and shared activities. Organizing this data into a dynamic multiplex network,
where each layer is composed of a single edge type linking the same underlying
vertices, can reveal interesting cross-layer interaction patterns. In
coevolving networks, links in one layer result in an increased probability of
other types of links forming between the same node pair. Hence we believe that
a holistic approach in which all the layers are simultaneously considered can
outperform a factored approach in which link prediction is performed separately
in each layer. This paper introduces a comprehensive framework, MLP (Multilayer
Link Prediction), in which link existence likelihoods for the target layer are
learned from the other network layers. These likelihoods are used to reweight
the output of a single layer link prediction method that uses rank aggregation
to combine a set of topological metrics. Our experiments show that our
reweighting procedure outperforms other methods for fusing information across
network layers.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this work, we provide a solution for pre-shaping a human-like robot hand
for grasping based on visual information. Our approach uses convolutional
neural networks (CNNs) to define a mapping between images and grasps. Applying
CNNs to robotics applications is non-trivial for two reasons. First, collecting
enough robot data to train a CNN at the same scale as the models trained in the
vision community is extremely difficult. In this work, we demonstrate that by
using a pre-trained CNN, a small set of grasping examples is sufficient for
generalizing across different objects of similar shapes. Second, the final
output of a CNN contains little location information of the observed object,
which is essential for the robot to manipulate the object. We take advantage of
the hierarchical nature of CNN features and identify the 3D position of a
mid-level feature using an approach we call targeted back propagation. Targeted
back propagation traces the activations of higher level features in a CNN
backwards through the network to discover the locations in the observation that
were responsible for making them fire, thus localizing important manipulatives
in the environment. We showed that this approach outperforms approaches without
targeted backpropagation in a cluttered scene. We further implemented a
hierarchical controller that controls fingers and palms based on features
located in different CNN layers for pre-shaping the robot hand and demonstrated
that this approach outperforms a pointcloud based approach on a grasping task
on Robonaut 2.
</p>
{{{{ARTICLE_PARSER}}}}<p>Although temporal persistence, or permanence, is a well understood
requirement for optimal biometric features, there is no general agreement on
how to assess temporal persistence. We suggest that the best way to assess
temporal persistence is to perform a test-retest study, and assess test-retest
reliability. For ratio-scale features that are normally distributed, this is
best done using the Intraclass Correlation Coefficient (ICC). For 10 distinct
data sets (8 eye-movement related, and 2 gait related), we calculated the
test-retest reliability ('Temporal persistence') of each feature, and compared
biometric performance of high-ICC features to lower ICC features, and to the
set of all features. We demonstrate that using a subset of only high-ICC
features produced superior Rank-1-Identification Rate (Rank-1-IR) performance
in 9 of 10 databases (p = 0.01, one-tailed). For Equal Error Rate (EER), using
a subset of only high-ICC features produced superior performance in 8 of 10
databases (p = 0.055, one-tailed). In general, then, prescreening potential
biometric features, and choosing only highly reliable features will yield
better performance than lower ICC features or than the set of all features
combined. We hypothesize that this would likely be the case for any biometric
modality where the features can be expressed as quantitative values on an
interval or ratio scale, assuming an adequate number of relatively independent
features.
</p>
{{{{ARTICLE_PARSER}}}}<p>Inductive Matrix Completion (IMC) is an important class of matrix completion
problems that allows direct inclusion of available features to enhance
estimation capabilities. These models have found applications in personalized
recommendation systems, multilabel learning, dictionary learning, etc. This
paper examines a general class of noisy matrix completion tasks where the
underlying matrix is following an IMC model i.e., it is formed by a mixing
matrix (a priori unknown) sandwiched between two known feature matrices. The
mixing matrix here is assumed to be well approximated by the product of two
sparse matrices---referred here to as \"sparse factor models.\" We leverage the
main theorem of Soni:2016:NMC and extend it to provide theoretical error bounds
for the sparsity-regularized maximum likelihood estimators for the class of
problems discussed in this paper. The main result is general in the sense that
it can be used to derive error bounds for various noise models. In this paper,
we instantiate our main result for the case of Gaussian noise and provide
corresponding error bounds in terms of squared loss.
</p>
{{{{ARTICLE_PARSER}}}}<p>An important result from psycholinguistics (Griffiths &amp; Kalish, 2005) states
that no language can be learned iteratively by rational agents in a
self-sustaining manner. We show how to modify the learning process slightly in
order to achieve self-sustainability. Our work is in two parts. First, we
characterize iterated learnability in geometric terms and show how a slight,
steady increase in the lengths of the training sessions ensures
self-sustainability for any discrete language class. In the second part, we
tackle the nondiscrete case and investigate self-sustainability for iterated
linear regression. We discuss the implications of our findings to issues of
non-equilibrium dynamics in natural algorithms.
</p>
{{{{ARTICLE_PARSER}}}}<p>Efforts at understanding the computational processes in the brain have met
with limited success, despite their importance and potential uses in building
intelligent machines. We propose a simple new model which draws on recent
findings in Neuroscience and the Applied Mathematics of interacting Dynamical
Systems. The Feynman Machine is a Universal Computer for Dynamical Systems,
analogous to the Turing Machine for symbolic computing, but with several
important differences. We demonstrate that networks and hierarchies of simple
interacting Dynamical Systems, each adaptively learning to forecast its
evolution, are capable of automatically building sensorimotor models of the
external and internal world. We identify such networks in mammalian neocortex,
and show how existing theories of cortical computation combine with our model
to explain the power and flexibility of mammalian intelligence. These findings
lead directly to new architectures for machine intelligence. A suite of
software implementations has been built based on these principles, and applied
to a number of spatiotemporal learning tasks.
</p>
{{{{ARTICLE_PARSER}}}}<p>The attention mechanism is an important part of the neural machine
translation (NMT) where it was reported to produce richer source representation
compared to fixed-length encoding sequence-to-sequence models. Recently, the
effectiveness of attention has also been explored in the context of image
captioning. In this work, we assess the feasibility of a multimodal attention
mechanism that simultaneously focus over an image and its natural language
description for generating a description in another language. We train several
variants of our proposed attention mechanism on the Multi30k multilingual image
captioning dataset. We show that a dedicated attention for each modality
achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT
baseline.
</p>
{{{{ARTICLE_PARSER}}}}<p>Accuracy, descriptor size, and the time required for extraction and matching
are all important factors when selecting local image descriptors. To optimize
over all these requirements, this paper presents a CUDA port for the recent
Learned Arrangement of Three Patches (LATCH) binary descriptors to the GPU
platform. The design of LATCH makes it well suited for GPU processing. Owing to
its small size and binary nature, the GPU can further be used to efficiently
match LATCH features. Taken together, this leads to breakneck descriptor
extraction and matching speeds. We evaluate the trade off between these speeds
and the quality of results in a feature matching intensive application. To this
end, we use our proposed CUDA LATCH (CLATCH) to recover structure from motion
(SfM), comparing 3D reconstructions and speed using different representations.
Our results show that CLATCH provides high quality 3D reconstructions at
fractions of the time required by other representations, with little, if any,
loss of reconstruction quality.
</p>
{{{{ARTICLE_PARSER}}}}<p>A common strategy for improving optimization algorithms is to restart the
algorithm when it is believed to be trapped in an inferior part of the search
space. However, while specific restart strategies have been developed for
specific problems (and specific algorithms), restarts are typically not
regarded as a general tool to speed up an optimization algorithm. In fact, many
optimization algorithms do not employ restarts at all.
</p>
<p>Recently, \"bet-and-run\" was introduced in the context of mixed-integer
programming, where first a number of short runs with randomized initial
conditions is made, and then the most promising run of these is continued. In
this article, we consider two classical NP-complete combinatorial optimization
problems, traveling salesperson and minimum vertex cover, and study the
effectiveness of different bet-and-run strategies. In particular, our restart
strategies do not take any problem knowledge into account, nor are tailored to
the optimization algorithm. Therefore, they can be used off-the-shelf. We
observe that state-of-the-art solvers for these problems can benefit
significantly from restarts on standard benchmark instances.
</p>
{{{{ARTICLE_PARSER}}}}<p>This text reports in detail how SEAL, a modeling framework for the economy
based on individual agents and firms, works. Thus, it aims to be an usage
manual for those wishing to use SEAL or SEAL's results. As a reference work,
theoretical and research studies are only cited. SEAL is thought as a Lab that
enables the simulation of the economy with spatially bounded
microeconomic-based computational agents. Part of the novelty of SEAL comes
from the possibility of simulating the economy in space and the instantiation
of different public offices, i.e. government institutions, with embedded
markets and actual data. SEAL is designed for Public Policy analysis,
specifically those related to Public Finance, Taxes and Real Estate.
</p>
{{{{ARTICLE_PARSER}}}}<p>Recent inapproximability results of Sly (2010), together with an
approximation algorithm presented by Weitz (2006) establish a beautiful picture
for the computational complexity of approximating the partition function of the
hard-core model. Let $\lambda_c(T_\Delta)$ denote the critical activity for the
hard-model on the infinite $\Delta$-regular tree. Weitz presented an FPTAS for
the partition function when $\lambda&lt;\lambda_c(T_\Delta)$ for graphs with
constant maximum degree $\Delta$. In contrast, Sly showed that for all
$\Delta\geq 3$, there exists $\epsilon_\Delta&gt;0$ such that (unless RP=NP) there
is no FPRAS for approximating the partition function on graphs of maximum
degree $\Delta$ for activities $\lambda$ satisfying
$\lambda_c(T_\Delta)&lt;\lambda&lt;\lambda_c(T_\Delta)+\epsilon_\Delta$.
</p>
<p>We prove that a similar phenomenon holds for the antiferromagnetic Ising
model. Recent results of Li et al. and Sinclair et al. extend Weitz's approach
to any 2-spin model, which includes the antiferromagnetic Ising model, to yield
an FPTAS for the partition function for all graphs of constant maximum degree
$\Delta$ when the parameters of the model lie in the uniqueness regime of the
infinite tree $T_\Delta$. We prove the complementary result that for the
antiferrogmanetic Ising model without external field that, unless RP=NP, for
all $\Delta\geq 3$, there is no FPRAS for approximating the partition function
on graphs of maximum degree $\Delta$ when the inverse temperature lies in the
non-uniqueness regime of the infinite tree $T_\Delta$. Our results extend to a
region of the parameter space for general 2-spin models. Our proof works by
relating certain second moment calculations for random $\Delta$-regular
bipartite graphs to the tree recursions used to establish the critical points
on the infinite tree.
</p>
{{{{ARTICLE_PARSER}}}}<p>Image enhancement is an important image processing technique that processes
images suitably for a specific application e.g. image editing. The conventional
solutions of image enhancement are grouped into two categories which are
spatial domain processing method and transform domain processing method such as
contrast manipulation, histogram equalization, homomorphic filtering. This
paper proposes a new image enhance method based on dictionary learning.
Particularly, the proposed method adjusts the image by manipulating the rarity
of dictionary atoms. Firstly, learn the dictionary through sparse coding
algorithms on divided sub-image blocks. Secondly, compute the rarity of
dictionary atoms on statistics of the corresponding sparse coefficients.
Thirdly, adjust the rarity according to specific application and form a new
dictionary. Finally, reconstruct the image using the updated dictionary and
sparse coefficients. Compared with the traditional techniques, the proposed
method enhances image based on the image content not on distribution of pixel
grey value or frequency. The advantages of the proposed method lie in that it
is in better correspondence with the response of the human visual system and
more suitable for salient objects extraction. The experimental results
demonstrate the effectiveness of the proposed image enhance method.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study the multi-armed bandit problem with arms which are Markov chains
with rewards. In the finite-horizon setting, the celebrated Gittins indices do
not apply, and the exact solution is intractable. We provide approximation
algorithms for a more general model which includes Markov decision processes
and non-unit transition times. When preemption is allowed, we provide a
(1/2-eps)-approximation, along with an example showing this is tight. When
preemption isn't allowed, we provide a 1/12-approximation, which improves to a
4/27-approximation when transition times are unity. Our model encompasses the
Markovian Bandits model of Gupta et al, the Stochastic Knapsack model of Dean,
Goemans, and Vondrak, and the Budgeted Learning model of Guha and Munagala, and
our algorithms improve existing results in all three areas. In our analysis, we
encounter and overcome to our knowledge a novel obstacle - an algorithm that
provably exists via polyhedral arguments, but cannot be found in polynomial
time.
</p>
{{{{ARTICLE_PARSER}}}}<p>We introduce a new recursive aggregation procedure called Bernstein Online
Aggregation (BOA). The exponential weights include an accuracy term and a
second order term that is a proxy of the quadratic variation as in Hazan and
Kale (2010). This second term stabilizes the procedure that is optimal in
different senses. We first obtain optimal regret bounds in the deterministic
context. Then, an adaptive version is the first exponential weights algorithm
that exhibits a second order bound with excess losses that appears first in
Gaillard et al. (2014). The second order bounds in the deterministic context
are extended to a general stochastic context using the cumulative predictive
risk. Such conversion provides the main result of the paper, an inequality of a
novel type comparing the procedure with any deterministic aggregation procedure
for an integrated criteria. Then we obtain an observable estimate of the excess
of risk of the BOA procedure. To assert the optimality, we consider finally the
iid case for strongly convex and Lipschitz continuous losses and we prove that
the optimal rate of aggregation of Tsybakov (2003) is achieved. The batch
version of the BOA procedure is then the first adaptive explicit algorithm that
satisfies an optimal oracle inequality with high probability.
</p>
{{{{ARTICLE_PARSER}}}}<p>In binary classification and regression problems, it is well understood that
Lipschitz continuity and smoothness of the loss function play key roles in
governing generalization error bounds for empirical risk minimization
algorithms. In this paper, we show how these two properties affect
generalization error bounds in the learning to rank problem. The learning to
rank problem involves vector valued predictions and therefore the choice of the
norm with respect to which Lipschitz continuity and smoothness are defined
becomes crucial. Choosing the $\ell_\infty$ norm in our definition of Lipschitz
continuity allows us to improve existing bounds. Furthermore, under smoothness
assumptions, our choice enables us to prove rates that interpolate between
$1/\sqrt{n}$ and $1/n$ rates. Application of our results to ListNet, a popular
learning to rank method, gives state-of-the-art performance guarantees.
</p>
{{{{ARTICLE_PARSER}}}}<p>We prove that finding an $\epsilon$-approximate Nash equilibrium is
PPAD-complete for constant $\epsilon$ and a particularly simple class of games:
polymatrix, degree 3 graphical games, in which each player has only two
actions.
</p>
<p>As corollaries, we also prove similar inapproximability results for Bayesian
Nash equilibrium in a two-player incomplete information game with a constant
number of actions, for relative $\epsilon$-Well Supported Nash Equilibrium in a
two-player game, for market equilibrium in a non-monotone market, for the
generalized circuit problem defined by Chen, Deng, and Teng [CDT'09], and for
approximate competitive equilibrium from equal incomes with indivisible goods.
</p>
{{{{ARTICLE_PARSER}}}}<p>Unlike the matrix case, computing low-rank approximations of tensors is
NP-hard and numerically ill-posed in general. Even the best rank-1
approximation of a tensor is NP-hard. In this paper, we use convex optimization
to develop polynomial-time algorithms for low-rank approximation and completion
of positive tensors. Our approach is to use algebraic topology to define a new
(numerically well-posed) decomposition for positive tensors, which we show is
equivalent to the standard tensor decomposition in important cases. Though
computing this decomposition is a nonconvex optimization problem, we prove it
can be exactly reformulated as a convex optimization problem. This allows us to
construct polynomial-time randomized algorithms for computing this
decomposition and for solving low-rank tensor approximation problems. Among the
consequences is that best rank-1 approximations of positive tensors can be
computed in polynomial time. Our framework is next extended to the tensor
completion problem, where noisy entries of a tensor are observed and then used
to estimate missing entries. We provide a polynomial-time algorithm that for
specific cases requires a polynomial (in tensor order) number of measurements,
in contrast to existing approaches that require an exponential number of
measurements. These algorithms are extended to exploit sparsity in the tensor
to reduce the number of measurements needed. We conclude by providing a novel
interpretation of statistical regression problems with categorical variables as
tensor completion problems, and numerical examples with synthetic data and data
from a bioengineered metabolic network show the improved performance of our
approach on this problem.
</p>
{{{{ARTICLE_PARSER}}}}<p>Many applications collect a large number of time series, for example, the
financial data of companies quoted in a stock exchange, the health care data of
all patients that visit the emergency room of a hospital, or the temperature
sequences continuously measured by weather stations across the US. These data
are often referred to as unstructured. A first task in its analytics is to
derive a low dimensional representation, a graph or discrete manifold, that
describes well the interrelations among the time series and their
intrarelations across time. This paper presents a computationally tractable
algorithm for estimating this graph that structures the data. The resulting
graph is directed and weighted, possibly capturing causal relations, not just
reciprocal correlations as in many existing approaches in the literature. A
convergence analysis is carried out. The algorithm is demonstrated on random
graph datasets and real network time series datasets, and its performance is
compared to that of related methods. The adjacency matrices estimated with the
new method are close to the true graph in the simulated data and consistent
with prior physical knowledge in the real dataset tested.
</p>
{{{{ARTICLE_PARSER}}}}<p>Pathloss is typically modeled using a log-distance power law with a
large-scale fading term that is log-normal. However, the received signal is
affected by the dynamic range and noise floor of the measurement system used to
sound the channel, which can cause measurement samples to be truncated or
censored. If the information about the censored samples are not included in the
estimation method, as in ordinary least squares estimation, it can result in
biased estimation of both the pathloss exponent and the large scale fading.
This can be solved by applying a Tobit maximum-likelihood estimator, which
provides consistent estimates for the pathloss parameters. This letter provides
information about the Tobit maximum-likelihood estimator and its asymptotic
variance under certain conditions.
</p>
{{{{ARTICLE_PARSER}}}}<p>We show that if $\mathbb A$ is a core relational structure such that
$CSP(\mathbb{A})$ can be solved by a linear Datalog program, and $\mathbb A$ is
$n$-permutable for some $n$, then $CSP(\mathbb A)$ can be solved by a symmetric
Datalog program (and thus $CSP(\mathbb{A})$ lies in deterministic logspace). At
the moment, it is not known for which structures $\mathbb A$ will
$CSP(\mathbb{A})$ be solvable by a linear Datalog program. However, once
somebody obtains a characterization of linear Datalog, our result immediately
gives a characterization of symmetric Datalog.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider repeated zero-sum games with incomplete information on the side
of Player 2 with the total payoff given by the non-normalized sum of stage
gains. In the classical examples the value $V_N$ of such an $N$-stage game is
of the order of $N$ or $\sqrt{N}$ as $N\to \infty$.
</p>
<p>Our aim is to find what is causing another type of asymptotic behavior of the
value $V_N$ observed for the discrete version of the financial market model
introduced by De Meyer and Saley. For this game Domansky and independently De
Meyer with Marino found that $V_N$ remains bounded as $N\to\infty$ and
converges to the limit value. This game is almost-fair, i.e., if Player 1
forgets his private information the value becomes zero.
</p>
<p>We describe a class of almost-fair games having bounded values in terms of an
easy-checkable property of the auxiliary non-revealing game. We call this
property the piecewise property, and it says that there exists an optimal
strategy of Player 2 that is piecewise-constant as a function of a prior
distribution $p$. Discrete market models have the piecewise property. We show
that for non-piecewise almost-fair games with an additional non-degeneracy
condition $V_N$ is of the order of $\sqrt{N}$.
</p>
{{{{ARTICLE_PARSER}}}}<p>We analyze the number of tile types $t$, bins $b$, and stages necessary to
assemble $n \times n$ squares and scaled shapes in the staged tile assembly
model. For $n \times n$ squares, we prove $\mathcal{O}(\frac{\log{n} - tb -
t\log t}{b^2} + \frac{\log \log b}{\log t})$ stages suffice and
$\Omega(\frac{\log{n} - tb - t\log t}{b^2})$ are necessary for almost all $n$.
For shapes $S$ with Kolmogorov complexity $K(S)$, we prove
$\mathcal{O}(\frac{K(S) - tb - t\log t}{b^2} + \frac{\log \log b}{\log t})$
stages suffice and $\Omega(\frac{K(S) - tb - t\log t}{b^2})$ are necessary to
assemble a scaled version of $S$, for almost all $S$. We obtain similarly tight
bounds when the more powerful flexible glues are permitted.
</p>
{{{{ARTICLE_PARSER}}}}<p>Clarithmetics are number theories based on computability logic (see
<a href=\"http://www.csc.villanova.edu/~japaridz/CL/\">this http URL</a> ). Formulas of these theories
represent interactive computational problems, and their \"truth\" is understood
as existence of an algorithmic solution. Various complexity constraints on such
solutions induce various versions of clarithmetic. The present paper introduces
a parameterized/schematic version CLA11(P1,P2,P3,P4). By tuning the three
parameters P1,P2,P3 in an essentially mechanical manner, one automatically
obtains sound and complete theories with respect to a wide range of target
tricomplexity classes, i.e. combinations of time (set by P3), space (set by P2)
and so called amplitude (set by P1) complexities. Sound in the sense that every
theorem T of the system represents an interactive number-theoretic
computational problem with a solution from the given tricomplexity class and,
furthermore, such a solution can be automatically extracted from a proof of T.
And complete in the sense that every interactive number-theoretic problem with
a solution from the given tricomplexity class is represented by some theorem of
the system. Furthermore, through tuning the 4th parameter P4, at the cost of
sacrificing recursive axiomatizability but not simplicity or elegance, the
above extensional completeness can be strengthened to intensional completeness,
according to which every formula representing a problem with a solution from
the given tricomplexity class is a theorem of the system. This article is
published in two parts. The present Part I introduces the system and proves its
completeness, while Part II is devoted to proving soundness.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this note, we extend the algorithms Extra and subgradient-push to a new
algorithm ExtraPush for consensus optimization with convex differentiable
objective functions over a directed network. When the stationary distribution
of the network can be computed in advance, we propose a simplified algorithm
called Normalized ExtraPush. Just like Extra, both ExtraPush and Normalized
ExtraPush can iterate with a fixed step size. But unlike Extra, they can take a
column-stochastic mixing matrix, which is not necessarily doubly stochastic.
Therefore, they remove the undirected-network restriction of Extra.
Subgradient-push, while also works for directed networks, is slower on the same
type of problem because it must use a sequence of diminishing step sizes.
</p>
<p>We present preliminary analysis for ExtraPush under a bounded sequence
assumption. For Normalized ExtraPush, we show that it naturally produces a
bounded, linearly convergent sequence provided that the objective function is
strongly convex.
</p>
<p>In our numerical experiments, ExtraPush and Normalized ExtraPush performed
similarly well. They are significantly faster than subgradient-push, even when
we hand-optimize the step sizes for the latter.
</p>
{{{{ARTICLE_PARSER}}}}<p>K-means is one of the most widely used algorithms for clustering in Data
Mining applications, which attempts to minimize the sum of the square of the
Euclidean distance of the points in the clusters from the respective means of
the clusters. However, K-means suffers from local minima problem and is not
guaranteed to converge to the optimal cost. K-means++ tries to address the
problem by seeding the means using a distance-based sampling scheme. However,
seeding the means in K-means++ needs $O\left(K\right)$ sequential passes
through the entire dataset, and this can be very costly for large datasets.
Here we propose a method of seeding the initial means based on factorizations
of higher order moments for bounded data. Our method takes $O\left(1\right)$
passes through the entire dataset to extract the initial set of means, and its
final cost can be proven to be within $O(\sqrt{K})$ of the optimal cost. We
demonstrate the performance of our algorithm in comparison with the existing
algorithms on various benchmark datasets.
</p>
{{{{ARTICLE_PARSER}}}}<p>We provide a finite forbidden induced subgraph characterization for the graph
class $\varUpsilon_k$, for all $k \in \mathbb{N}_0$, which is defined as
follows. A graph is in $\varUpsilon_k$ if for any induced subgraph, $\Delta
\leq \chi -1 + k$ holds, where $\Delta$ is the maximum degree and $\chi$ is the
chromatic number of the subgraph.
</p>
<p>We compare these results with those given in [O. Schaudt, V. Weil, On
bounding the difference between the maximum degree and the clique number,
Graphs and Combinatorics 31(5), 1689-1702 (2015). DOI:
10.1007/s00373-014-1468-3], where we studied the graph class $\varOmega_k$, for
$k \in \mathbb{N}_0$, whose graphs are such that for any induced subgraph,
$\Delta \leq \omega -1 + k$ holds, where $\omega$ denotes the clique number of
a graph. In particular, we give a characterization in terms of $\varOmega_k$
and $\varUpsilon_k$ of those graphs where the neighborhood of every vertex is
perfect.
</p>
{{{{ARTICLE_PARSER}}}}<p>It is well-known that selling different goods in a single bundle can
significantly increase revenue, even when the valuations for the goods are
independent. However, bundling is no longer profitable if the goods have high
production costs. To overcome this challenge, we introduce a new mechanism,
Pure Bundling with Disposal for Cost (PBDC), where after buying the bundle, the
customer is allowed to return any subset of goods for their production cost. We
derive both distribution-dependent and distribution-free guarantees on its
profitability, which improve previous techniques. Our distribution-dependent
bound suggests that the firm should never price the bundle such that the profit
margin is less than 1/3 of the expected welfare, while also showing that PBDC
is optimal for a large number of independent goods. Our distribution-free bound
suggests that on the distributions where PBDC performs worst, individual sales
perform well. Finally, we conduct extensive simulations which confirm that PBDC
outperforms other simple pricing schemes overall.
</p>
{{{{ARTICLE_PARSER}}}}<p>Visual attributes are great means of describing images or scenes, in a way
both humans and computers understand. In order to establish a correspondence
between images and to be able to compare the strength of each property between
images, relative attributes were introduced. However, since their introduction,
hand-crafted and engineered features were used to learn increasingly complex
models for the problem of relative attributes. This limits the applicability of
those methods for more realistic cases. We introduce a deep neural network
architecture for the task of relative attribute prediction. A convolutional
neural network (ConvNet) is adopted to learn the features by including an
additional layer (ranking layer) that learns to rank the images based on these
features. We adopt an appropriate ranking loss to train the whole network in an
end-to-end fashion. Our proposed method outperforms the baseline and
state-of-the-art methods in relative attribute prediction on various coarse and
fine-grained datasets. Our qualitative results along with the visualization of
the saliency maps show that the network is able to learn effective features for
each specific attribute. Source code of the proposed method is available at
https://github.com/yassersouri/ghiaseddin.
</p>
{{{{ARTICLE_PARSER}}}}<p>A new submodule clustering method via sparse and low-rank representation for
multi-way data is proposed in this paper. Instead of reshaping multi-way data
into vectors, this method maintains their natural orders to preserve data
intrinsic structures, e.g., image data kept as matrices. To implement
clustering, the multi-way data, viewed as tensors, are represented by the
proposed tensor sparse and low-rank model to obtain its submodule
representation, called a free module, which is finally used for spectral
clustering. The proposed method extends the conventional subspace clustering
method based on sparse and low-rank representation to multi-way data submodule
clustering by combining t-product operator. The new method is tested on several
public datasets, including synthetical data, video sequences and toy images.
The experiments show that the new method outperforms the state-of-the-art
methods, such as Sparse Subspace Clustering (SSC), Low-Rank Representation
(LRR), Ordered Subspace Clustering (OSC), Robust Latent Low Rank Representation
(RobustLatLRR) and Sparse Submodule Clustering method (SSmC).
</p>
{{{{ARTICLE_PARSER}}}}<p>A new algorithm is proposed which accelerates the mini-batch k-means
algorithm of Sculley (2010) by using the distance bounding approach of Elkan
(2003). We argue that, when incorporating distance bounds into a mini-batch
algorithm, already used data should preferentially be reused. To this end we
propose using nested mini-batches, whereby data in a mini-batch at iteration t
is automatically reused at iteration t+1.
</p>
<p>Using nested mini-batches presents two difficulties. The first is that
unbalanced use of data can bias estimates, which we resolve by ensuring that
each data sample contributes exactly once to centroids. The second is in
choosing mini-batch sizes, which we address by balancing premature fine-tuning
of centroids with redundancy induced slow-down. Experiments show that the
resulting nmbatch algorithm is very effective, often arriving within 1% of the
empirical minimum 100 times earlier than the standard mini-batch algorithm.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study the extension complexity of polytopes with few vertices or facets.
On the one hand, we provide a complete classification of $d$-polytopes with at
most $d+4$ vertices according to their extension complexity: Out of the
super-exponentially many $d$-polytopes with $d+4$ vertices, all have extension
complexity $d+4$ except for some families of size $\theta(d^2)$. On the other
hand, we show that generic realizations of simplicial/simple $d$-polytopes with
$d+1+\alpha$ vertices/facets have extension complexity at least $2
\sqrt{d(d+\alpha)} -d + 1$, which shows that for all $d&gt;(\frac{\alpha-1}{2})^2$
there are $d$-polytopes with $d+1+\alpha$ vertices or facets and extension
complexity $d+1+\alpha$.
</p>
{{{{ARTICLE_PARSER}}}}<p>We adapt the greedy Stack-LSTM dependency parser of Dyer et al. (2015) to
support a training-with-exploration procedure using dynamic oracles(Goldberg
and Nivre, 2013) instead of cross-entropy minimization. This form of training,
which accounts for model predictions at training time rather than assuming an
error-free action history, improves parsing accuracies for both English and
Chinese, obtaining very strong results for both languages. We discuss some
modifications needed in order to get training with exploration to work well for
a probabilistic neural-network.
</p>
{{{{ARTICLE_PARSER}}}}<p>Traditional statistical analysis requires that the analysis process and data
are independent. By contrast, the new field of adaptive data analysis hopes to
understand and provide algorithms and accuracy guarantees for research as it is
commonly performed in practice, as an iterative process of interacting
repeatedly with the same data set. Previous work has defined a model with a
rather strong lower bound on sample complexity in terms of the number of
queries, $n\sim\sqrt q$, arguing that adaptive data analysis is much harder
than static data analysis, where $n\sim\log q$ is possible. Instead, we argue
that those strong lower bounds point to a bug in the model, an information
asymmetry with no basis in the typical application.
</p>
<p>In its place, we propose a new Bayesian version of the problem without this
unnecessary asymmetry. The previous lower bounds are no longer valid, which
offers the possibility for stronger results. As a first contribution to this
model, though, we show that a large family of methods, including all previously
proposed algorithms, cannot achieve the static dependence of $n\sim\log q$, but
instead require polylogarithmically many samples. These early results suggest
that adaptive data analysis is harder than static data analysis even with
information symmetry, but leave open many possibilities for new developments.
</p>
{{{{ARTICLE_PARSER}}}}<p>We study a class of Linear Feedback Shift Registers (LFSRs) with
characteristic polynomial $f(x)=p(x)q(x)$ where $p(x)$ and $q(x)$ are distinct
irreducible polynomials in $\F_2[x]$. Important properties of the LFSRs, such
as the cycle structure and the adjacency graph, are derived. A method to
determine a state belonging to each cycle and a generic algorithm to find all
conjugate pairs shared by any pair of cycles are given. The process explicitly
determines the edges and their labels in the adjacency graph. The results are
then combined with the cycle joining method to efficiently construct a new
class of de Bruijn sequences. An estimate of the number of resulting sequences
is given. In some cases, using cyclotomic numbers, we can determine the number
exactly.
</p>
{{{{ARTICLE_PARSER}}}}<p>The capacity of classical channels is convex. This is not the case for the
quantum capacity of a channel: the capacity of a mixture of different quantum
channels exceeds the mixture of the individual capacities and thus is
non-convex. Here we show that this effect goes beyond the quantum capacity and
holds for the private and classical environment-assisted capacities of quantum
channels.
</p>
{{{{ARTICLE_PARSER}}}}<p>We address the problem of novel view synthesis: given an input image,
synthesizing new images of the same object or scene observed from arbitrary
viewpoints. We approach this as a learning task but, critically, instead of
learning to synthesize pixels from scratch, we learn to copy them from the
input image. Our approach exploits the observation that the visual appearance
of different views of the same instance is highly correlated, and such
correlation could be explicitly learned by training a convolutional neural
network (CNN) to predict appearance flows -- 2-D coordinate vectors specifying
which pixels in the input view could be used to reconstruct the target view.
Furthermore, the proposed framework easily generalizes to multiple input views
by learning how to optimally combine single-view predictions. We show that for
both objects and scenes, our approach is able to synthesize novel views of
higher perceptual quality than previous CNN-based techniques.
</p>
{{{{ARTICLE_PARSER}}}}<p>As ISPs face IPv4 address scarcity they increasingly turn to network address
translation (NAT) to accommodate the address needs of their customers.
Recently, ISPs have moved beyond employing NATs only directly at individual
customers and instead begun deploying Carrier-Grade NATs (CGNs) to apply
address translation to many independent and disparate endpoints spanning
physical locations, a phenomenon that so far has received little in the way of
empirical assessment. In this work we present a broad and systematic study of
the deployment and behavior of these middleboxes. We develop a methodology to
detect the existence of hosts behind CGNs by extracting non-routable IP
addresses from peer lists we obtain by crawling the BitTorrent DHT. We
complement this approach with improvements to our Netalyzr troubleshooting
service, enabling us to determine a range of indicators of CGN presence as well
as detailed insights into key properties of CGNs. Combining the two data
sources we illustrate the scope of CGN deployment on today's Internet, and
report on characteristics of commonly deployed CGNs and their effect on end
users.
</p>
{{{{ARTICLE_PARSER}}}}<p>The popularity of neural networks (NNs) spans academia, industry, and popular
culture. In particular, convolutional neural networks (CNNs) have been applied
to many image based machine learning tasks and have yielded strong results. The
availability of hardware/software systems for efficient training and deployment
of large and/or deep CNN models has been, and continues to be, an important
consideration for the field. Early systems for NN computation focused on
leveraging existing dense linear algebra techniques and libraries. Current
approaches use low-level machine specific programming and/or closed-source,
purpose-built vendor libraries. In this work, we present an open source system
that, compared to existing approaches, achieves competitive computational speed
while achieving higher portability. We achieve this by targeting the
vendor-neutral OpenCL platform using a code-generation approach. We argue that
our approach allows for both: (1) the rapid development of new computational
kernels for existing hardware targets, and (2) the rapid tuning of existing
computational kernels for new hardware targets. Results are presented for a
case study of targeting the Qualcomm Snapdragon 820 mobile computing platform
for CNN deployment.
</p>
{{{{ARTICLE_PARSER}}}}<p>We propose an image representation scheme combining the local and nonlocal
characterization of patches in an image. Our representation scheme can be shown
to be equivalent to a tight frame constructed from convolving local bases (e.g.
wavelet frames, discrete cosine transforms, etc.) with nonlocal bases (e.g.
spectral basis induced by nonlinear dimension reduction on patches), and we
call the resulting frame elements {\it convolution framelets}. Insight gained
from analyzing the proposed representation leads to a novel interpretation of a
recent high-performance patch-based image inpainting algorithm using Point
Integral Method (PIM) and Low Dimension Manifold Model (LDMM) [Osher, Shi and
Zhu, 2016]. In particular, we show that LDMM is a weighted
$\ell_2$-regularization on the coefficients obtained by decomposing images into
linear combinations of convolution framelets; based on this understanding, we
extend the original LDMM to a reweighted version that yields further improved
inpainting results. In addition, we establish the energy concentration property
of convolution framelet coefficients for the setting where the local basis is
constructed from a given nonlocal basis via a linear reconstruction framework;
a generalization of this framework to unions of local embeddings can provide a
natural setting for interpreting BM3D, one of the state-of-the-art image
denoising algorithms.
</p>
{{{{ARTICLE_PARSER}}}}<p>We propose a semantic attention mechanism for image caption generation,
called text-conditional semantic attention, which allows the caption generator
to automatically learn which parts of the image feature to focus on given
previously generated text. To acquire text-related image features for our
attention model, we also improve the guiding Long Short-Term Memory (gLSTM)
structure by back-propagating the training loss though semantic guidance to
fine-tune the CNN weights. In contrast to existing gLSTM methods, such as
emb-gLSTM, our fine-tuned model enables guidance information to be more
text-related. This also allows jointly learning of the image embedding, text
embedding, semantic attention and language model with one simple network
architecture in an end-to-end manner. We implement our model based on
NeuralTalk2, an open-source image caption generator, and test it on MSCOCO
dataset. We evaluate the proposed method with three metrics: BLEU, METEOR and
CIDEr. The proposed methods outperform state-of-the-art methods.
</p>
{{{{ARTICLE_PARSER}}}}<p>We show how to build a compressed index for the offline Order Preserving
Pattern Matching problem. Our solution is based on the new approach of
decomposing the sequence to be indexed into an order component, containing
ordering information, and a delta component, containing information on the
absolute values. The Order Preserving Matching problem is then transformed into
an exact matching problem on the order component followed by a verification
phase on the delta component. Experiments show that this approach is viable and
it is the first one offering simultaneously small space usage and fast
retrieval.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper proposes implicit CF-NADE, a neural autoregressive model for
collaborative filtering tasks using implicit feedback ( e.g. click, watch,
browse behaviors). We first convert a users implicit feedback into a like
vector and a confidence vector, and then model the probability of the like
vector, weighted by the confidence vector. The training objective of implicit
CF-NADE is to maximize a weighted negative log-likelihood. We test the
performance of implicit CF-NADE on a dataset collected from a popular digital
TV streaming service. More specifically, in the experiments, we describe how to
convert watch counts into implicit relative rating, and feed into implicit
CF-NADE. Then we compare the performance of implicit CF-NADE model with the
popular implicit matrix factorization approach. Experimental results show that
implicit CF-NADE significantly outperforms the baseline.
</p>
{{{{ARTICLE_PARSER}}}}<p>This article investigates the performance of an ultra-dense network (UDN)
from an energy-efficiency (EE) standpoint leveraging the interplay between
stochastic geometry (SG) and mean-field game (MFG) theory. In this setting,
base stations (BSs) (resp. users) are uniformly distributed over a
two-dimensional plane as two independent homogeneous Poisson point processes
(PPPs), where users associate to their nearest BSs. The goal of every BS is to
maximize its own energy efficiency subject to channel uncertainty, random BS
location, and interference levels. Due to the coupling in interference, the
problem is solved in the mean-field (MF) regime where each BS interacts with
the whole BS population via time-varying MF interference. As a main
contribution, the asymptotic convergence of MF interference to zero is
rigorously proved in a UDN with multiple transmit antennas. It allows us to
derive a closed-form EE representation, yielding a tractable EE optimal power
control policy. This proposed power control achieves more than 1.5 times higher
EE compared to a fixed power baseline.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we consider a multiple-access fading channel where $N$ users
transmit to a single base station (BS) within a limited number of time slots.
We assume that each user has a fixed amount of energy available to be consumed
over the transmission window. We derive the optimal energy allocation policy
for each user that maximizes the total system throughput under two different
assumptions on the channel state information. First, we consider the offline
allocation problem where the channel states are known a priori before
transmission. We solve a convex optimization problem to maximize the
sum-throughput under energy and delay constraints. Next, we consider the online
allocation problem, where the channels are causally known to the BS and obtain
the optimal energy allocation via dynamic programming when the number of users
is small. We also develop a suboptimal resource allocation algorithm whose
performance is close to the optimal one. Numerical results are presented
showing the superiority of the proposed algorithms over baseline algorithms in
various scenarios.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper employs case-based reasoning (CBR) to capture the personal styles
of individual artists and generate the human facial portraits from photos
accordingly. For each human artist to be mimicked, a series of cases are
firstly built-up from her/his exemplars of source facial photo and hand-drawn
sketch, and then its stylization for facial photo is transformed as a
style-transferring process of iterative refinement by looking-for and applying
best-fit cases in a sense of style optimization. Two models, fitness evaluation
model and parameter estimation model, are learned for case retrieval and
adaptation respectively from these cases. The fitness evaluation model is to
decide which case is best-fitted to the sketching of current interest, and the
parameter estimation model is to automate case adaptation. The resultant sketch
is synthesized progressively with an iterative loop of retrieval and adaptation
of candidate cases until the desired aesthetic style is achieved. To explore
the effectiveness and advantages of the novel approach, we experimentally
compare the sketch portraits generated by the proposed method with that of a
state-of-the-art example-based facial sketch generation algorithm as well as a
couple commercial software packages. The comparisons reveal that our CBR based
synthesis method for facial portraits is superior both in capturing and
reproducing artists' personal illustration styles to the peer methods.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we analyze the benefits of including downlink pilots in a
cell-free massive MIMO system. We derive an approximate per-user achievable
downlink rate for conjugate beamforming processing, which takes into account
both uplink and downlink channel estimation errors, and power control. A
performance comparison is carried out, in terms of per-user net throughput,
considering cell-free massive MIMO operation with and without downlink
training, for different network densities. We take also into account the
performance improvement provided by max-min fairness power control in the
downlink. Numerical results show that, exploiting downlink pilots, the
performance can be considerably improved in low density networks over the
conventional scheme where the users rely on statistical channel knowledge only.
In high density networks, performance improvements are moderate.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper we study sums of powers of affine functions in (mostly) one
variable. Although quite simple, this model is a generalization of two
well-studied models: Waring decomposition and sparsest shift. For these three
models there are natural extensions to several variables, but this paper is
mostly focused on univariate polynomials. We present structural results which
compare the expressive power of the three models; and we propose algorithms
that find the smallest decomposition of f in the first model (sums of affine
powers) for an input polynomial f given in dense representation. We also begin
a study of the multivariate case. This work could be extended in several
directions. In particular, just as for Sparsest Shift and Waring decomposition,
one could consider extensions to \"supersparse\" polynomials and attempt a fuller
study of the multi-variate case. We also point out that the basic univariate
problem studied in the present paper is far from completely solved: our
algorithms all rely on some assumptions for the exponents in an optimal
decomposition, and some algorithms also rely on a distinctness assumption for
the shifts. It would be very interesting to weaken these assumptions, or even
to remove them entirely. Another related and poorly understood issue is that of
the bit size of the constants appearing in an optimal decomposition: is it
always polynomially related to the bit size of the input polynomial given in
dense representation?
</p>
{{{{ARTICLE_PARSER}}}}<p>Efficient usage of the knowledge provided by the Linked Data community is
often hindered by the need for domain experts to formulate the right SPARQL
queries to answer questions. For new questions they have to decide which
datasets are suitable and in which terminology and modelling style to phrase
the SPARQL query.
</p>
<p>In this work we present an evolutionary algorithm to help with this
challenging task. Given a training list of source-target node-pair examples our
algorithm can learn patterns (SPARQL queries) from a SPARQL endpoint. The
learned patterns can be visualised to form the basis for further investigation,
or they can be used to predict target nodes for new source nodes.
</p>
<p>Amongst others, we apply our algorithm to a dataset of several hundred human
associations (such as \"circle - square\") to find patterns for them in DBpedia.
We show the scalability of the algorithm by running it against a SPARQL
endpoint loaded with &gt; 7.9 billion triples. Further, we use the resulting
SPARQL queries to mimic human associations with a Mean Average Precision (MAP)
of 39.9 % and a Recall@10 of 63.9 %.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we explore the possibility to apply machine learning to make
diagnostic predictions using discomfort drawings. A discomfort drawing is an
intuitive way for patients to express discomfort and pain related symptoms.
These drawings have proven to be an effective method to collect patient data
and make diagnostic decisions in real-life practice. A dataset from real-world
patient cases is collected for which medical experts provide diagnostic labels.
Next, we use a factorized multimodal topic model, Inter-Battery Topic Model
(IBTM), to train a system that can make diagnostic predictions given an unseen
discomfort drawing. The number of output diagnostic labels is determined by
using mean-shift clustering on the discomfort drawing. Experimental results
show reasonable predictions of diagnostic labels given an unseen discomfort
drawing. Additionally, we generate synthetic discomfort drawings with IBTM
given a diagnostic label, which results in typical cases of symptoms. The
positive result indicates a significant potential of machine learning to be
used for parts of the pain diagnostic process and to be a decision support
system for physicians and other health care personnel.
</p>
{{{{ARTICLE_PARSER}}}}<p>The concept of an \"approximation algorithm\" is usually only applied to
optimization problems since in optimization problems the performance of the
algorithm on any given input is a continuous parameter. We introduce a new
concept of approximation applicable to decision problems and functions,
inspired by Bayesian probability. From the perspective of a Bayesian reasoner
with limited computational resources, the answer to a problem that cannot be
solved exactly is uncertain and therefore should be described by a random
variable. It thus should make sense to talk about the expected value of this
random variable, an idea we formalize in the language of average-case
complexity theory by introducing the concept of optimal polynomial-time
estimator. We show that optimal polynomial-time estimators exhibit many
parallels with \"classical\" probability theory, prove some existence theorems
and completeness results.
</p>
{{{{ARTICLE_PARSER}}}}<p>Due to the dramatic growth in mobile data traffic on one hand and the
scarcity of the licensed spectrum on the other hand, mobile operators are
considering the use of unlicensed bands (especially those in 5 GHz) as
complementary spectrum for providing higher system capacity and better user
experience. This approach is currently being standardized by 3GPP under the
name of LTE Licensed-Assisted Access (LTE-LAA). In this paper, we take a
holistic approach for LTE-LAA small cell traffic balancing by jointly
optimizing the use of the licensed and unlicensed bands. We pose this traffic
balancing as an optimization problem that seeks proportional fair coexistence
of WiFi, small cell and macro cell users by adapting the transmission
probability of the LTE-LAA small cell in the licensed and unlicensed bands. The
motivation for this formulation is for the LTE-LAA small cell to switch between
or aggregate licensed and unlicensed bands depending on the
interference/traffic level and the number of active users in each band. We
derive a closed form solution for this optimization problem and additionally
propose a transmission mechanism for the operation of the LTE-LAA small cell on
both bands. Through numerical and simulation results, we show that our proposed
traffic balancing scheme, besides enabling better LTE-WiFi coexistence and
efficient utilization of the radio resources relative to the existing traffic
balancing scheme, also provides a better tradeoff between maximizing the total
network throughput and achieving fairness among all network flows compared to
alternative approaches.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper we consider a time-division duplex cell-free massive
multiple-input multiple-output (MIMO) system where many distributed access
points (APs) simultaneously serve many users. A normalized conjugate
beamforming scheme, which satisfies short-term average power constraints at the
APs, is proposed and analyzed taking into account the effect of imperfect
channel information. We derive an approximate closed-form expression for the
per-user achievable downlink rate of this scheme. We also provide, analytically
and numerically, a performance comparison between the normalized conjugate
beamforming and the conventional conjugate beamforming scheme in [1] (which
satisfies long-term average power constraints). Normalized conjugate
beamforming scheme reduces the beamforming uncertainty gain, which comes from
the users' lack of the channel state information knowledge, and hence, it
improves the achievable downlink rate compared to the conventional conjugate
beamforming scheme.
</p>
{{{{ARTICLE_PARSER}}}}<p>We develop an efficient alternating framework for learning a generalized
version of Factorization Machine (gFM) on steaming data with provable
guarantees. When the instances are sampled from $d$ dimensional random Gaussian
vectors and the target second order coefficient matrix in gFM is of rank $k$,
our algorithm converges linearly, achieves $O(\epsilon)$ recovery error after
retrieving $O(k^{3}d\log(1/\epsilon))$ training instances, consumes $O(kd)$
memory in one-pass of dataset and only requires matrix-vector product
operations in each iteration. The key ingredient of our framework is a
construction of an estimation sequence endowed with a so-called Conditionally
Independent RIP condition (CI-RIP). As special cases of gFM, our framework can
be applied to symmetric or asymmetric rank-one matrix sensing problems, such as
inductive matrix completion and phase retrieval.
</p>
{{{{ARTICLE_PARSER}}}}<p>For a constant $\epsilon$, we prove a poly(N) lower bound on the (randomized)
communication complexity of $\epsilon$-Nash equilibrium in two-player NxN
games. For n-player binary-action games we prove an exp(n) lower bound for the
(randomized) communication complexity of $(\epsilon,\epsilon)$-weak approximate
Nash equilibrium, which is a profile of mixed actions such that at least
$(1-\epsilon)$-fraction of the players are $\epsilon$-best replying.
</p>
{{{{ARTICLE_PARSER}}}}<p>The incentive ratio measures the utility gains from strategic behaviour.
Without any restrictions on the setup, ratios for linear, Leontief and
Cobb-Douglas exchange markets are unbounded, showing that manipulating the
equilibrium is a worthwhile endeavour, even if it is computationally
challenging. Such unbounded improvements can be achieved even if agents only
misreport their utility functions. This provides a sharp contrast with previous
results from Fisher markets. When the Cobb-Douglas setup is more restrictive,
the maximum utility gain is bounded by the number of commodities. By means of
an example, we show that it is possible to exceed a known upper bound for
Fisher markets in exchange economies.
</p>
{{{{ARTICLE_PARSER}}}}<p>Recommendation systems are being explored by Cable TV operators to improve
user satisfaction with services, such as Live TV and Video on Demand (VOD)
services. More recently, Catch-up TV has been introduced, allowing users to
watch recent broadcast content whenever they want to. These services give users
a large set of options from which they can choose from, creating an information
overflow problem. Thus, recommendation systems arise as essential tools to
solve this problem by helping users in their selection, which increases not
only user satisfaction but also user engagement and content consumption. In
this paper we present a learning to rank approach that uses contextual
information and implicit feedback to improve recommendation systems for a Cable
TV operator that provides Live and Catch-up TV services. We compare our
approach with existing state-of-the-art algorithms and show that our approach
is superior in accuracy, while maintaining high scores of diversity and
serendipity.
</p>
{{{{ARTICLE_PARSER}}}}<p>Nowadays, Cable TV operators provide their users multiple ways to watch TV
content, such as Live TV and Video on Demand (VOD) services. In the last years,
Catch-up TV has been introduced, allowing users to watch recent broadcast
content whenever they want to. Understanding how the users interact with such
services is important to develop solutions that may increase user satisfaction
, user engagement and user consumption. In this paper, we characterize, for the
first time, how users interact with a large European Cable TV operator that
provides Live TV, Catch-up TV and VOD services. We analyzed many
characteristics, such as the service usage, user engagement, program type,
program genres and time periods. This characterization will help us to have a
deeper understanding on how users interact with these different services, that
may be used to enhance the recommendation systems of Cable TV providers.
</p>
{{{{ARTICLE_PARSER}}}}<p>Context: One of the black arts of data mining is learning the magic
parameters that control the learners. In software analytics, at least for
defect prediction, several methods, like grid search and differential
evolution(DE), have been proposed to learn those parameters. They've been
proved to be able to improve learner performance.
</p>
<p>Objective: We want to evaluate which method that can find better parameters
in terms of performance score and runtime. Methods: This paper compares grid
search to differential evolution, which is an evolutionary algorithm that makes
extensive use of stochastic jumps around the search space.
</p>
<p>Results: We find that the seemingly complete approach of grid search does no
better, and sometimes worse, than the stochastic search. Yet, when repeated 20
times to check for conclusion validity, DE was over 210 times faster (6.2 hours
vs 54 days for grid search when both tuning Random Forest over 17 test data
sets with F-measure as optimzation objective).
</p>
<p>Conclusions: These results are puzzling: why does a quick partial search be
just as effective as a much slower, and much more,extensive search? To answer
that question, we turned to the theoretical optimization literature. Bergstra
and Bengio conjecture that grid search is not more effective than more
randomized searchers if the underlying search space is inherently low
dimensional.This is significant since recent results show that defect
prediction exhibits very low intrinsic dimensionality-an observation that
explains why a fast method like DE may work as well as a seemingly more
thorough grid search. This suggests, as a future research direction, that it
might be possible to peek at data sets before doing any optimization in order
to match the optimization algorithm to the problem at hand.
</p>
{{{{ARTICLE_PARSER}}}}<p>We consider scenarios from the real-time strategy game StarCraft as new
benchmarks for reinforcement learning algorithms. We propose micromanagement
tasks, which present the problem of the short-term, low-level control of army
members during a battle. From a reinforcement learning point of view, these
scenarios are challenging because the state-action space is very large, and
because there is no obvious feature representation for the state-action
evaluation function. We describe our approach to tackle the micromanagement
scenarios with deep neural network controllers from raw state features given by
the game engine. In addition, we present a heuristic reinforcement learning
algorithm which combines direct exploration in the policy space and
backpropagation. This algorithm allows for the collection of traces for
learning using deterministic policies, which appears much more efficient than,
for example, {\epsilon}-greedy exploration. Experiments show that with this
algorithm, we successfully learn non-trivial strategies for scenarios with
armies of up to 15 agents, where both Q-learning and REINFORCE struggle.
</p>
{{{{ARTICLE_PARSER}}}}<p>Edge-labeled graphs are widely used to describe relationships between
entities in a database. We study a class of queries, referred to as exemplar
queries, on edge-labeled graphs where each query gives an example of what the
user is searching for. Given an exemplar query, we study the problem of
efficiently searching for similar subgraphs in a large data graph, where the
similarity is defined in terms of the well-known graph edit distance. We call
these queries error-tolerant exemplar queries since matches are allowed despite
small variations in the graph structure and the labels. The problem in its
general case is computationally intractable but efficient solutions are
reachable for labeled graphs under well-behaved distribution of the labels,
commonly found in knowledge graphs and RDF databases. In this paper, we propose
two efficient exact algorithms, based on a filtering-and-verification
framework, for finding subgraphs in a large data graph that are isomorphic to a
query graph under some edit operations. Our filtering scheme, which uses the
neighbourhood structure around a node and the presence or absence of paths,
significantly reduces the number of candidates that are passed to the
verification stage. We analyze the costs of our algorithms and the conditions
under which one algorithm is expected to outperform the other. Our cost
analysis identifies some of the variables that affect the cost, including the
number and the selectivity of the edge labels in the query and the degree of
nodes in the data graph, and characterizes the relationships. We empirically
evaluate the effectiveness of our filtering schemes and queries, the efficiency
of our algorithms and the reliability of our cost models on real datasets.
</p>
{{{{ARTICLE_PARSER}}}}<p>Artificial objects often subjectively look eerie when their appearance to
some extent resembles a human, which is known as the uncanny valley phenomenon.
From a cognitive psychology perspective, several explanations of the phenomenon
have been put forth, two of which are object categorization and realism
inconsistency. Recently, MacDorman and Chattopadhyay (2016) reported
experimental data as evidence in support of the latter. In our estimation,
however, their results are still consistent with categorization-based stranger
avoidance. In this Discussions paper, we try to describe why
categorization-based stranger avoidance remains a viable explanation, despite
the evidence of MacDorman and Chattopadhyay, and how it offers a more inclusive
explanation of the impression of eeriness in the uncanny valley phenomenon.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper presents a simple end-to-end model for speech recognition,
combining a convolutional network based acoustic model and a graph decoding. It
is trained to output letters, with transcribed speech, without the need for
force alignment of phonemes. We introduce an automatic segmentation criterion
for training from sequence annotation without alignment that is on par with CTC
while being simpler. We show competitive results in word error rate on the
Librispeech corpus with MFCC features, and promising results from raw waveform.
</p>
{{{{ARTICLE_PARSER}}}}<p>Cross-app collaboration via inter-component communication is a fundamental
mechanism on Android. Although it brings the benefits such as functionality
reuse and data sharing, a threat called component hijacking is also introduced.
By hijacking a vulnerable component in victim apps, an attack app can escalate
its privilege for originally prohibited operations. Many prior studies have
been performed to understand and mitigate this issue, but component hijacking
remains a serious open problem in the Android ecosystem due to no effective
defense deployed in the wild. In this paper, we present our vision on
practically defending against component hijacking in Android apps. First, we
argue that to fundamentally prevent component hijacking, we need to switch from
the previous mindset (i.e., performing system-level control or repackaging
vulnerable apps after they are already released) to a more proactive version
that aims to help security-inexperienced developers make secure components in
the first place. To this end, we propose to embed into apps a secure component
library (SecComp), which performs in-app mandatory access control on behalf of
app components. An important factor for SecComp to be effective is that we find
it is possible to devise a set of practical in-app policies to stop component
hijacking. Furthermore, we allow developers design custom policies, beyond our
by-default generic policies, to support more fine-grained access control. We
have overcome challenges to implement a preliminary SecComp prototype, which
stops component hijacking with very low performance overhead. We hope the
future research that fully implements our vision can eventually help real-world
apps get rid of component hijacking.
</p>
{{{{ARTICLE_PARSER}}}}<p>In this paper, we study the performance of extremum estimators from the
perspective of generalization ability (GA): the ability of a model to predict
outcomes in new samples from the same population. By adapting the classical
concentration inequalities, we derive upper bounds on the empirical
out-of-sample prediction errors as a function of the in-sample errors,
in-sample data size, heaviness in the tails of the error distribution, and
model complexity. We show that the error bounds may be used for tuning key
estimation hyper-parameters, such as the number of folds $K$ in
cross-validation. We also show how $K$ affects the bias-variance trade-off for
cross-validation. We demonstrate that the $\mathcal{L}_2$-norm difference
between penalized and the corresponding un-penalized regression estimates is
directly explained by the GA of the estimates and the GA of empirical moment
conditions. Lastly, we prove that all penalized regression estimates are
$L_2$-consistent for both the $n \geqslant p$ and the $n &lt; p$ cases.
Simulations are used to demonstrate key results.
</p>
<p>Keywords: generalization ability, upper bound of generalization error,
penalized regression, cross-validation, bias-variance trade-off,
$\mathcal{L}_2$ difference between penalized and unpenalized regression, lasso,
high-dimensional data.
</p>
{{{{ARTICLE_PARSER}}}}<p>This work presents a method for estimation of the acoustic intensity, the
energy density and the associated sound field diffuseness around the origin,
when the sound field is weighted with a spatial filter. The method permits
energetic DOA estimation and sound field characterization focused in a specific
angular region determined by the beam pattern of the spatial filter. The
formulation of the estimators is presented and their behavior is analyzed for
the fundamental cases useful in parametric sound field models of a single plane
wave, a uniform diffuse field and a mixture of the two.
</p>
{{{{ARTICLE_PARSER}}}}<p>Causal inference from observational data is a subject of active research and
development in statistics and computer science. Many toolkits have been
developed for this purpose that depends on statistical software. However, these
toolkits do not scale to large datasets. In this paper we describe a suite of
techniques for expressing causal inference tasks from observational data in
SQL. This suite supports the state-of-the-art methods for causal inference and
run at scale within a database engine. In addition, we introduce several
optimization techniques that significantly speedup causal inference, both in
the online and offline setting. We evaluate the quality and performance of our
techniques by experiments of real datasets.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper describes a simple new semantics for logic rules, the founded
semantics, and its straightforward extension to another simple new semantics,
the constraint semantics. The new semantics support unrestricted negation, as
well as unrestricted existential and universal quantifications. They are
uniquely expressive and intuitive by allowing assumptions about the predicates
and rules to be specified explicitly, are completely declarative and easy to
understand, and relate cleanly to prior semantics. In addition, founded
semantics can be computed in linear time in the size of the ground program.
</p>
{{{{ARTICLE_PARSER}}}}<p>Commodity video-gaming hardware (consoles, graphics cards, tablets, etc.)
performance has been advancing at a rapid pace owing to strong consumer demand
and stiff market competition. Gaming hardware devices are currently amongst the
most powerful and cost-effective computational technologies available in
quantity. In this article, we evaluate a sample of current generation
video-gaming hardware devices for scientific computing and compare their
performance with specialized supercomputing general purpose graphics processing
units (GPGPUs). We use the OpenCL SHOC benchmark suite, which is a measure of
the performance of compute hardware on various different scientific application
kernels, and also a popular public distributed computing application,
Einstein@Home in the field of gravitational physics for the purposes of this
evaluation.
</p>
{{{{ARTICLE_PARSER}}}}<p>This paper presents a new method for parallel-jaw grasping of isolated
objects from depth images, under large gripper pose uncertainty. Whilst most
approaches aim to predict the single best grasp pose from an image, our method
first predicts a score for every possible grasp pose, which we denote the grasp
function. With this, it is possible to achieve grasping robust to the gripper's
pose uncertainty, by smoothing the grasp function with the pose uncertainty
function. Therefore, if the single best pose is adjacent to a region of poor
grasp quality, that pose will no longer be chosen, and instead a pose will be
chosen which is surrounded by a region of high grasp quality. To learn this
function, we train a Convolutional Neural Network which takes as input a single
depth image of an object, and outputs a score for each grasp pose across the
image. Training data for this is generated by use of physics simulation and
depth image simulation with 3D object meshes, to enable acquisition of
sufficient data without requiring exhaustive real-world experiments. We
evaluate with both synthetic and real experiments, and show that the learned
grasp score is more robust to gripper pose uncertainty than when this
uncertainty is not accounted for.
</p>
{{{{ARTICLE_PARSER}}}}</div>
<div style="visibility:hidden; display: none;" id="links">http://arxiv.org/pdf/1609.03558{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03559{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03562{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03565{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03566{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03567{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03568{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03569{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03570{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03571{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03572{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03575{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03576{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03577{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03580{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03581{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03583{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03586{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03587{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03591{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03592{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03595{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03611{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03630{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03639{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03656{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03674{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03697{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03711{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03723{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03724{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03748{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03753{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03771{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03775{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03776{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03782{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03791{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03792{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03799{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03807{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03813{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03817{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03826{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03833{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03843{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03858{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03860{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03865{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03875{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03882{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03883{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03896{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03899{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03900{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03903{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03905{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03906{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03908{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03909{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03928{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03932{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03936{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03939{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03941{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03956{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03963{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03968{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03969{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03973{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03982{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03997{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/0804.2680{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1110.0524{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1601.00020{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1601.03536{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1602.03880{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1602.07192{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1603.08919{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.06760{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.02106{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.04304{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.05327{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.00454{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.06280{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.06882{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.07473{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.09174{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.00928{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.02768{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.06868{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.00711{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.03897{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.06116{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.06698{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.07292{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.08627{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.00929{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.02493{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.02796{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.02919{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03233{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03554{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.04226{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.00083{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03480{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03708{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03725{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03783{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03830{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03839{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03878{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03879{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03887{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03898{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03910{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.06169{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03574{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03700{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03715{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03761{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03804{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03830{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03865{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03880{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03910{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03927{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03975{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03985{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1602.03880{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03480{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03589{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03627{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03655{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03669{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03679{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03758{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03764{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03790{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03794{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03801{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03818{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03848{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03849{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03917{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1203.2226{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1507.04853{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1603.03483{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.01582{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.03792{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.04230{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.02131{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.06466{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03560{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03578{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03598{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03603{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03650{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03654{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03687{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03707{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03721{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03736{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03744{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03751{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03758{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03780{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03785{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03788{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03835{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03854{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03866{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03881{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03884{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03901{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03911{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03918{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03937{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03940{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03994{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1503.00651{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1506.05449{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1510.03875{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1511.09042{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1601.04466{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1602.02099{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.04259{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.07974{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.04456{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.09472{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.02297{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.04230{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.06659{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.00985{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.02587{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03590{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03601{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03605{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03614{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03615{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03619{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03628{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03629{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03632{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03640{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03641{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03642{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03643{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03644{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03645{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03647{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03648{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03650{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03657{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03659{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03663{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03664{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03666{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03668{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03675{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03677{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03683{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03690{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03695{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03696{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03703{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03713{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03717{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03727{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03732{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03734{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03737{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03750{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03756{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03759{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03762{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03765{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03768{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03769{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03772{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03773{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03777{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03780{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03784{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03795{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03806{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03815{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03836{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03840{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03847{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03868{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03874{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03876{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03892{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03893{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03894{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03897{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03912{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03930{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03932{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03938{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03946{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03947{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03948{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03958{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03960{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03971{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03976{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03986{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03993{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03996{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1203.2226{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1305.0871{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1306.1149{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1404.1356{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1405.0586{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1405.3322{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1412.0620{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1503.00173{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1504.03977{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1508.05766{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1509.01727{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1510.03919{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1510.08564{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1511.02942{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1511.05933{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1511.08403{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1512.02300{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1512.04103{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1601.00149{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1602.02934{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1602.06894{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1603.03793{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.02492{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.04351{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1604.07974{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.03557{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1605.05606{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.00094{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.01377{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.04621{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.05724{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.07674{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.00500{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.00711{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.02715{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.04753{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.05420{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.07249{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.08206{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.04112{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.05069{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.05121{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.05995{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.06580{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.02423{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.02451{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.02453{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.02613{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.02993{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03095{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03191{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03193{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03322{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03344{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03409{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1609.03540{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1606.06269{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1607.05537{{{{ARTICLE_PARSER}}}}http://arxiv.org/pdf/1608.02239{{{{ARTICLE_PARSER}}}}</div>

</body>
</html>
