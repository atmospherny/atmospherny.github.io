<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Atmospherny feed</title>
<style type="text/css">
@font-face {
    font-family: 'PT Serif';
    src: url(PT_Serif-Web-Regular.ttf);
    font-style: normal;
}

body {
	margin: 0;
}

#container {
	width: 100%;
	max-width: 720px;
	margin: auto;
	background-color: #ccc;
}

button {
	width: 100%;
	margin: 0;
	border-style: none;
	font-size: 48pt;
	padding: 1em;
	font-family: 'PT Serif';
	outline: none;
	-webkit-transition: background-color 2s;
	-moz-transition: background-color 2s;
	-o-transition: background-color 2s;
	transition: background-color 2s;
}

button:hover {
	background-color: #777;
	-webkit-transition: background-color 0.2s;
	-moz-transition: background-color 0.2s;
	-o-transition: background-color 0.2s;
	transition: background-color 0.2s;
}

table {
	width: 100%;
}

#current {
	font-size: 24pt;
	font-family: 'PT Serif';
	padding: 0.3em;
}

summary {
	background-color: #888;
	padding: 0.3em;
	outline: none;
	cursor: pointer;
}

#progress {
	text-align: center;
}

#weekday {
	text-align: center;
}

a {
	color: #333;
}

a:visited {
	color: #888;
}

</style>

<script type="text/javascript">
var currentIndex = 0;
var myChoice = new Object();

var days = ['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday'];

function o(s) {
	return document.getElementById(s);
}

function next() {
	currentIndex++;
	if (currentIndex == o("titles").innerHTML.split("<br><br><br>").length) currentIndex=0;
	update();
	updateIndex();
}

function prev() {
	currentIndex--;
	if (currentIndex < 0) currentIndex = o("titles").innerHTML.split("<br><br><br>").length - 1;
	update();
	updateIndex();
}

function choose() {
	myChoice["a"+currentIndex] = "tr";
	update()
}

function clubs() {
	myChoice["a"+currentIndex] = "bl";
	update()
}

function spades() {
	myChoice["a"+currentIndex] = "br";
	update()	
}

function unchoose() {
	myChoice["a"+currentIndex] = "tl";
	update()
}

function ifchosen() {
	switch(myChoice["a"+currentIndex]) {
		case "tr":
			o("current-title").innerHTML = "&hearts; " + o("current-title").innerHTML
			break;
		case "br":
			o("current-title").innerHTML = "&spades; " + o("current-title").innerHTML
			break;
		case "bl":
			o("current-title").innerHTML = "&clubs; " + o("current-title").innerHTML
			break;
		default:
			o("current-title").innerHTML = "&diams; " + o("current-title").innerHTML		
	}
}

function update() {
	o("current-title").innerHTML = o("titles").innerHTML.split("<br><br><br>")[currentIndex];
	o("current-abstract").innerHTML = o("abstracts").innerHTML.split("<br><br><br>")[currentIndex] + "<br>" + "<a href='" + o("links").innerHTML.split("<br><br><br>")[currentIndex] + "' target='_blank'>download pdf</a>";
	ifchosen();
	updateStorage();
	o("progress").innerHTML = "#" + (currentIndex + 1);
}

function updateStorage() {
	localStorage["myChoice"] = JSON.stringify(myChoice)
}

function updateIndex() {
	localStorage["currentIndex"] = currentIndex;
}

function updateChoice() {
	myChoice = JSON.parse(localStorage["myChoice"]);
	currentIndex = parseInt(localStorage["currentIndex"]);
	if (typeof(currentIndex)!= "number") {
		localStorage["currentIndex"] = 0;
		currentIndex = 0;
	} 
}

function listA() {
	buf = ""
	for(var i in myChoice) {
		if ((myChoice[i] == "tr") || (myChoice[i] == "bl")) buf += "#" + (parseInt(i.split("a")[1]) + 1) + " "
	}
	window.prompt("Share your choice:", buf);
}

function listB() {
	buf = ""
	for(var i in myChoice) {
		if ((myChoice[i] == "tr") || (myChoice[i] == "br")) buf += "#" + (parseInt(i.split("a")[1]) + 1) + " "
	}
	window.prompt("Share your choice:", buf);
}

function setWeekday() {
	var d = new Date();
	var n = d.getUTCHours();
	if (o("weekday").innerHTML != days[now.getDay()]) {
		if((days[now.getDay()] != "Saturday") && (days[now.getDay()] != "Sunday")) {
			if(n>3) {
				localStorage["myChoice"] = JSON.stringify({});
				localStorage["currentIndex"] = 0;
			}
		}
	}
}

</script>
</head>
<body onload="updateChoice();update()">

<div id="container">
<div id="current">

<div id="weekday">Sunday</div>
<div id="progress"></div>

<p id="current-title">

</p>

<details>
<summary>abstract</summary>
<p id="current-abstract">

</p>
</details>

</div>
<table>
<tr>
<td><button onclick="prev()">
<
</button></td><td><button onclick="next()">
>
</button></td>
</tr>
<tr>
<td><button onclick="unchoose()">
&diams;
</button></td><td><button onclick="choose()">
&hearts;
</button></td>
</tr>
<tr>
<td><button onclick="clubs()">
&clubs;
</button></td><td><button onclick="spades()">
&spades;
</button></td>
</tr>

<tr><td><button onclick="listA()">(&clubs;&hearts;)</button></td><td><button onclick="listB()">(&spades;&hearts;)</button></td></tr>

</table>

</div>

<div id="viewer">

</div>

<div style="visibility:hidden; display: none;" id="titles">Beyond Jcrit: a critical curve for suppression of H2-cooling in protogalaxies.<br><br><br>Efficiency of Planetesimal Ablation in Giant Planetary Envelopes.<br><br><br>Taming outliers in pulsar-timing datasets with hierarchical likelihoods and Hamiltonian sampling.<br><br><br>Star Formation in Nearby Early-Type Galaxies: The Radio Continuum Perspective.<br><br><br>Impeded Dark Matter.<br><br><br>An Ultra-Faint Galaxy Candidate Discovered in Early Data from the Magellanic Satellites Survey.<br><br><br>Boosting Lya and HeII 1640A Line Fluxes from Pop III Galaxies: Stochastic IMF Sampling and Departures from Case-B.<br><br><br>Ultra-deep GEMINI near-infrared observations of the bulge globular cluster NGC 6624.<br><br><br>Photochemical-dynamical models of externally FUV irradiated protoplanetary discs.<br><br><br>SDSS-IV MaNGA: A serendipitous observation of a potential gas accretion event.<br><br><br>The binary fraction, separation distribution, and merger rate of white dwarfs from the SPY sample.<br><br><br>Quantum-gravity phenomenology with primordial black holes.<br><br><br>The environment of radio sources in the VLA-COSMOS Survey field.<br><br><br>Experimental radiative lifetimes for highly excited states and calculated oscillator strengths for lines of astrophysical interest in singly ionized cobalt (Co II).<br><br><br>Observational Confirmation of a Link Between Common Envelope Binary Interaction and Planetary Nebula Shaping.<br><br><br>Bayes Factors via Savage-Dickey Supermodels.<br><br><br>Chemical abundances for A-and F-type supergiant stars.<br><br><br>First NuSTAR observations of the BL Lac - type blazar PKS~2155-304: constraints on the jet content and distribution of radiating particles.<br><br><br>A comprehensive search for stellar bowshock nebulae in the Milky Way: a catalog of 709 mid-infrared selected candidates.<br><br><br>Nuclear Physics and Astrophysics of Neutrino Oscillations.<br><br><br>A Trip to the Cataclysmic Binary Zoo: Detailed Follow-Up of 35 Recently-Discovered Systems.<br><br><br>Stellar nuclei and inner polar disks in lenticular galaxies.<br><br><br>A millisecond pulsar candidate in a 21-hr orbit: 3FGL J0212.1+5320.<br><br><br>The unrelaxed dynamical structure of the galaxy cluster Abell 85.<br><br><br>Investigating Magnetic Activity in the Galactic Centre by Global MHD Simulation.<br><br><br>The 26 December 2001 Solar Event Responsible for GLE63. I. Observations of a Major Long-Duration Flare with the Siberian Solar Radio Telescope.<br><br><br>Inverse Compton emission from a cosmic-ray precursor in RX J1713.7-3946.<br><br><br>Contact Binaries as Viable Distance Indicators: New, Competitive (V)JHKs Period-Luminosity Relations.<br><br><br>Infrared-Faint Radio Sources in the SERVS deep fields: Pinpointing AGNs at high redshift.<br><br><br>CERES: A Set of Automated Reduction Routines for Echelle Spectra.<br><br><br>The Influence of Environment on the Chemical Evolution in Low-mass Galaxies.<br><br><br>Residual Non-Abelian Dark Matter and Dark Radiation.<br><br><br>NUV signatures of environment driven galaxy quenching in SDSS groups.<br><br><br>Organizing the Parameter Space of the Global 21-cm Signal.<br><br><br>Dust grains from the heart of supernovae.<br><br><br>Optical and Ultraviolet Observations of the Very Young Type IIP SN 2014cx in NGC 337.<br><br><br>A new view on the maximum mass of differentially rotating neutron stars.<br><br><br>The Fermi Large Area Telescope as a Galactic Supernovae Axionscope.<br><br><br>The HI content of extremely metal-deficient blue compact dwarf galaxies.<br><br><br>Clumpy and Extended Starbursts in the Brightest Unlensed Submillimeter Galaxies.<br><br><br>Formation of globular clusters induced by external ultraviolet radiation II: Three-dimensional radiation hydrodynamics simulations.<br><br><br>A Single Zone Synchrotron Model for Flares of PKS 1510-089.<br><br><br>Scaling regimes in spherical shell rotating convection.<br><br><br>Inward Diffusion and Acceleration of Particles Driven by Turbulent Fluctuations in Magnetosphere.<br><br><br>Observing the PTPS Sample of Evolved Exoplanet Host Candidates Using the NPOI.<br><br><br>Spectroscopic and Interferometric Measurements of Nine K Giant Stars.<br><br><br>Solar neutrinos: Oscillations or No-oscillations?.<br><br><br>Detection of Possible Quasi-periodic Oscillations in the Long-term Optical Light Curve of the BL Lac Object OJ 287.<br><br><br>Ring Formation around Giant Planets by Tidal Disruption of a Single Passing Large Kuiper Belt Object.<br><br><br>A Massive Dense Gas Cloud close to the Nucleus of the Seyfert galaxy NGC 1068.<br><br><br>Evolution of intermediate mass and massive binary stars: physics, mass loss, and rotation.<br><br><br>Reversal and amplification of zonal flows by boundary enforced thermal wind.<br><br><br>Discovery of a pseudobulge galaxy launching powerful relativistic jets.<br><br><br>Ageing of a space-based CCD: photometric performance development of the low Earth orbiting detectors of the CoRoT mission.<br><br><br>Parallel Calibration for Sensor Array Radio Interferometers.<br><br><br>Non-axisymmetric instabilities in discs with imposed zonal flows.<br><br><br>Monte-Carlo modelling of multi-object adaptive optics performance on the European Extremely Large Telescope.<br><br><br>A detector interferometric calibration experiment for high precision astrometry.<br><br><br>Age Estimates of Universe: from Globular Clusters to Cosmological Models and Probes.<br><br><br>MOCCA-SURVEY database I. Accreting white dwarf binary systems in globular clusters -- II. Cataclysmic variables -- progenitors and population at birth.<br><br><br>The ortho-to-para ratio of interstellar NH$_2$: Quasi-classical trajectory calculations and new simulations.<br><br><br>A cavity and further radial substructures in the disk around HD~97048.<br><br><br>An Ordered Magnetic Field in the Protoplanetary Disk of AB Aur Revealed by Mid-Infrared Polarimetry.<br><br><br>The Making of FR Is I. Numerical Hydrodynamic 3D Simulations of Low Power Jets.<br><br><br>Measurement of the Muon Production Depths at the Pierre Auger Observatory.<br><br><br>A Model of White Dwarf Pulsar AR Scorpii.<br><br><br>The nearby interstellar medium towards alpha Leo -- UV observations and modeling of a warm cloud within hot gas.<br><br><br>A critical reassessment of the fundamental properties for GJ 504: chemical composition and age.<br><br><br>Modelling Void Abundance in Modified Gravity.<br><br><br>The Chandra COSMOS Legacy Survey: Clustering of X-ray selected AGN at 2.9<z<5.5 using photometric redshift Probability Distribution Functions.<br><br><br>Circularization of Tidally Disrupted Stars around Spinning Supermassive Black Holes.<br><br><br>Helium Reionization Simulations. I. Modeling Quasars as Radiation Sources.<br><br><br>Stability of Earth-mass Planets in the Kepler-68 System.<br><br><br>f(T) teleparallel gravity and cosmology.<br><br><br>On the classification of GRBs and their occurrence rates.<br><br><br>Three-dimensional, global, radiative GRMHD simulations of a thermally unstable disc.<br><br><br>Application of Bayesian graphs to SN Ia data analysis and compression.<br><br><br>Photophoresis on particles hotter/colder than the ambient gas in the free molecular flow.<br><br><br>Toward $ab\,initio$ extremely metal poor stars.<br><br><br>The Origin and Evolution of Saturn, with Exoplanet Perspective.<br><br><br>A DECam Search for an Optical Counterpart to the LIGO Gravitational Wave Event GW151226.<br><br><br>Quantum Walks as simulators of neutrino oscillations in vacuum and matter.<br><br><br>Feebly Interacting Dark Matter Particle as the Inflaton.<br><br><br>A new astrobiological model of the atmosphere of Titan.<br><br><br>ALMA spectroscopic survey in the Hubble Ultra Deep Field: Continuum number counts, resolved 1.2-mm extragalactic background, and properties of the faintest dusty star forming galaxies.<br><br><br>Orbital alignment and starspot properties in the WASP-52 planetary system.<br><br><br>Ocular Shock Front in the Colliding Galaxy IC 2163.<br><br><br>Collapse of Axion Stars.<br><br><br>Dark matter search in the inner galactic center halo with H.E.S.S.<br><br><br>Evolution of Protoplanetary Discs with Magnetically Driven Disc Winds.<br><br><br>Zodiacal Exoplanets in Time (ZEIT) IV: seven transiting planets in the Praesepe cluster.<br><br><br>Axion star collisions with Neutron stars and Fast Radio Bursts.<br><br><br>Unveiling the nucleon tensor charge at Jefferson Lab: A study of the SoLID case.<br><br><br>Decomposing transverse momentum balance contributions for quenched jets in PbPb collisions at sqrt(s[NN]) = 2.76 TeV.<br><br><br>Muon Flux Measurements at the Davis Campus of the Sanford Underground Research Facility with the Majorana Demonstrator Veto System.<br><br><br>An experimental review on elliptic flow of strange and multi-strange hadrons in relativistic heavy ion collisions.<br><br><br>Nuclear Physics and Astrophysics of Neutrino Oscillations.<br><br><br>B(E2) anomaly in $6^+$ isomers of $^{134-138}$Sn isotopes and neutron single-particle energies beyond N=82.<br><br><br>Quasi-particle random phase approximation with quasi-particle-vibration coupling: application to the Gamow-Teller response of the superfluid nucleus $^{120}$Sn.<br><br><br>Heavy-Meson Decay Constants: QCD Sum-Rule Glance at Isospin Breaking.<br><br><br>Unveiling the nucleon tensor charge at Jefferson Lab: A study of the SoLID case.<br><br><br>A symmetry conserving description of odd-nuclei with the Gogny force.<br><br><br>Existence of the critical endpoint in the vector meson extended linear sigma model.<br><br><br>X-ray bounds on the r-mode amplitude in millisecond pulsars.<br><br><br>The First-Order Euler-Lagrange equations and some of their uses.<br><br><br>Asymptotic stability of strong contact discontinuity for full compressible Navier-Stokes equations with initial boundary value problem.<br><br><br>Conformal covariance and the split property.<br><br><br>Heat Kernel Renormalization on Manifolds with Boundary.<br><br><br>Entanglement Entropy in the $\sigma$-Model with the de Sitter Target Space.<br><br><br>Spectral and scattering theory for Gauss-Bonnet operators on perturbed topological crystals.<br><br><br>Batalin-Vilkovisky formalism as a theory of integration for polyvectors.<br><br><br>Exact-dimensional property of density of states measure of Sturm Hamiltonian.<br><br><br>Multistability of Phase-Locking in Equal-Frequency Kuramoto Models on Planar Graphs.<br><br><br>Macroscopic evolution of mechanical and thermal energy in a harmonic chain with random flip of velocities.<br><br><br>The limit distribution in the $q$-CLT for $q \ge 1$ is unique and can not have a compact support.<br><br><br>Quantum Coherence as a Resource.<br><br><br>Random coalescing geodesics in first-passage percolation.<br><br><br>A nonlinear generalization of the Camassa-Holm equation with peakon solutions.<br><br><br>A Mathematical Theory of Optimal Milestoning (with a Detour via Exact Milestoning).<br><br><br>Series solutions of the non-stationary Heun equation.<br><br><br>Heat kernel upper bounds for interacting particle systems.<br><br><br>Quantum Metrology: Towards an alternative definition for the meter.<br><br><br>Existence and uniqueness for stochastic 2D Euler flows with bounded vorticity.<br><br><br>Quantum Yang-Mills Theory in Two Dimensions: Exact versus Perturbative.<br><br><br>Decimation of the Dyson-Ising Ferromagnet.<br><br><br>Generalized negative flows in hierarchies of integrable evolution equations.<br><br><br>Superconformal SU(1,1|n) mechanics.<br><br><br>Towards Noncommutative Topological Quantum Field Theory: Tangential Hodge-Witten cohomology.<br><br><br>Ultimate precision of adaptive quantum metrology.<br><br><br>Secret key capacity of the thermal-loss channel: Improving the lower bound.<br><br><br>Metrological measures of non-classical correlations.<br><br><br>Phase transitions in definite total spin states of two-component Fermi gases.<br><br><br>Fundamental Limitation on Cooling under Classical Noise.<br><br><br>Quantum lattice gas algorithmic representation of gauge field theory.<br><br><br>Decomposition of radiation energy into work and heat.<br><br><br>Generation and evaluation of entanglement between two multi-photon systems using single photon sources and linear optics.<br><br><br>Nodal Variational Principle for Excited States.<br><br><br>A Unified Hamiltonian Solution to Maxwell-Schrodinger Equations for Modeling Electromagnetic Field-Particle Interaction.<br><br><br>$\cal PT$-symmetric theory in two dimensional spaces.<br><br><br>Defect production in nonequilibrium phase transitions: Experimental investigation of the Kibble-Zurek mechanism in a two-qubit quantum simulator.<br><br><br>Bell inequality of frequency-bin entangled photon pairs with time-resolved detection.<br><br><br>Converting heat into directed transport on a tilted lattice.<br><br><br>Quatnum coherence behaviors of fermionic system in non-inertial frame.<br><br><br>Classical and Quantum Mechanics with Lie Brackets and Pseudocanonical Transformations.<br><br><br>Casimir effect on the lattice: U(1) gauge theory in two spatial dimensions.<br><br><br>Routing protocol for wireless quantum multi-hop Mesh backbone network based on partially entangled GHZ state.<br><br><br>$T^3$-interferometer for atoms.<br><br><br>Influence of classic noise on entangled state formation in parametric systems.<br><br><br>Highly bright photon-pair generation in Doppler-broadened ladder-type atomic system.<br><br><br>Symmetry between quasielectrons and quasiholes for fractional quantum Hall models defined on lattices.<br><br><br>Realization and application of parity-time-symmetric oscillators in quantum regime.<br><br><br>Quantum-enhanced metrology without entanglement based on optical cavities with feedback.<br><br><br>A quantum-inspired algorithm for estimating the permanent of positive semidefinite matrices.<br><br><br>Quantum dimensions from local operator excitations in the Ising model.<br><br><br>Coarse graining flow of spin foam intertwiners.<br><br><br>Quench dynamics of the three-dimensional U(1) complex field theory: geometric and scaling characterisation of the vortex tangle.<br><br><br>Topological order of mixed states in quantum many-body systems.<br><br><br>Lattice effects on Laughlin wave functions and parent Hamiltonians.<br><br><br>Quantum Coherence as a Resource.<br><br><br>Dynamic phase transitions of a driven Ising chain in a dissipative cavity.<br><br><br>Quantum optical circulator controlled by a single chirally coupled atom.<br><br><br>Classical geometric phase of gyro-motion is a coherent quantum Berry phase.<br><br><br>Quantum-assisted learning of graphical models with arbitrary pairwise connectivity.<br><br><br>Constructive Interference Between Disordered Couplings Enhances Multiparty Entanglement in Quantum Heisenberg Spin Glass Models.<br><br><br>An atomic symmetry-controlled thermal switch.<br><br><br>How to Verify a Quantum Computation.<br><br><br>A one parameter fit for glassy dynamics as a quantum corollary of the liquid to solid transition.<br><br><br>Planar three-body bound states induced by $p$-wave interatomic resonance.<br><br><br>Geometric entangler via spin-electric coupling in molecular magnets.<br><br><br>(Almost) C*-algebras as sheaves with self-action.<br><br><br>Stefan-Boltzmann law for massive photons.<br><br><br>Tight bound on the classical value of generalized Clauser-Horne-Shimony-Holt games.<br><br><br>Modular transformations through sequences of topological charge projections.<br><br><br>Temporal evolution of resonant transmission under telegraph noise.<br><br><br>Observation of pair tunneling and coherent destruction of tunneling in arrays of optical waveguides.<br><br><br>Persistent Spin Textures in Semiconductor Nanostructures.<br><br><br>On Classical and Quantum Logical Entropy: The analysis of measurement.<br><br><br>Single-shot simulations of dynamics of quantum dark solitons.<br><br><br>Optomechanics with two-phonon driving.<br><br><br>Exact Electromagnetic Casimir Energy of a Disk Opposite a Plane.<br><br><br>Adiabatic Perturbation Theory and Geometry of Periodically-Driven Systems.<br><br><br>Effect of quantum noise on deterministic joint remote state preparation of a qubit state via a GHZ channel.<br><br><br>Quantum Walks as simulators of neutrino oscillations in vacuum and matter.<br><br><br>Random Wave Function Collapse.<br><br><br>Stationary States in Quantum Walk Search.<br><br><br>Lyapunov Exponent and Four-Point Correlator's Growth Rate in a Chaotic System.<br><br><br>Optimum mixed-state discrimination for noisy entanglement-enhanced sensing.<br><br><br>Spin-electric Berry phase shift in triangular molecular magnets.<br><br><br>Unruh acceleration effect on the precision of parameter estimation.<br><br><br>Tracking Algorithm for Microscopic Flow Data Collection.<br><br><br>Random Shuffling and Resets for the Non-stationary Stochastic Bandit Problem.<br><br><br>The Effect of Class Imbalance and Order on Crowdsourced Relevance Judgments.<br><br><br>Distributed sampled-data control of nonholonomic multi-robot systems with proximity networks.<br><br><br>Feedback and Timing in a Crowdsourcing Game.<br><br><br>Online Learning for Sparse PCA in High Dimensions: Exact Dynamics and Phase Transitions.<br><br><br>Energy Transparency for Deeply Embedded Programs.<br><br><br>Pilot Contamination Attack Detection by Key-Confirmation in Secure MIMO Systems.<br><br><br>Sequential Linear Quadratic Optimal Control for Nonlinear Switched Systems.<br><br><br>Discrete Variational Autoencoders.<br><br><br>Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation.<br><br><br>Automated Segmentation of Retinal Layers from Optical Coherent Tomography Images Using Geodesic Distance.<br><br><br>Fitted Learning: Models with Awareness of their Limits.<br><br><br>Optimal ALOHA-like Random Access with Heterogeneous QoS Guarantees for Multi-Packet Reception Aided Visible Light Communications.<br><br><br>Backpropagation of Hebbian plasticity for lifelong learning.<br><br><br>Exploiting Insurance Telematics for Fun and Profit.<br><br><br>Latent Dependency Forest Models.<br><br><br>Determination of Pedestrian Flow Performance Based on Video Tracking and Microscopic Simulations.<br><br><br>Characterizations and Effective Computation of Supremal Relatively Observable Sublanguages.<br><br><br>On Performance Modeling for MANETs under General Limited Buffer Constraint.<br><br><br>Tighter bound of Sketched Generalized Matrix Approximation.<br><br><br>Ashwin: Plug-and-Play System for Machine-Human Image Annotation.<br><br><br>Deformable Map Matching for Uncertain Loop-Less Maps.<br><br><br>Learning Action Concept Trees and Semantic Alignment Networks from Image-Description Data.<br><br><br>Optimal Resource Allocation for CoMP based Cellular Systems with Base Station Switching.<br><br><br>Physical Layer Security-Aware Routing and Performance Tradeoffs in WANETs.<br><br><br>Performance Analysis of CSMA with Multi-Packet Reception: The Inhomogeneous Case.<br><br><br>Soft Recovery Through $\ell_{1,2}$ Minimization with Applications in Recovery of Simultaneously Sparse and Low-Rank Matrice.<br><br><br>Survey of Consistent Network Updates.<br><br><br>The Probability of Reachability for a Parallel Connected Linear System over a Finite Field, Calculated by a Formula for the Number of Mutually Left Coprime Polynomial Matrices.<br><br><br>Ms. Pac-Man Versus Ghost Team CIG 2016 Competition.<br><br><br>A Lower Bound on the Capacity of the Noncentral Chi Channel with Applications to Soliton Amplitude Modulation.<br><br><br>Distributed agent-based automated theorem proving in order-sorted first-order logic.<br><br><br>Routing-Verification-as-a-Service (RVaaS): Trustworthy Routing Despite Insecure Providers.<br><br><br>A short review and primer on online processing of multiple signal sources in human computer interaction applications.<br><br><br>IT formulae for gamma target: mutual information and relative entropy.<br><br><br>9-1-1 DDoS: Threat, Analysis and Mitigation.<br><br><br>Adaptive Parameter Balancing for Composite Optimization Problems in Imaging.<br><br><br>Ear-to-ear Capture of Facial Intrinsics.<br><br><br>Extraction of Skin Lesions from Non-Dermoscopic Images Using Deep Learning.<br><br><br>Feasibility Study on Disaster Management with Hybrid Network of LTE and Satellite Links.<br><br><br>Improved Optimistic Mirror Descent for Sparsity and Curvature.<br><br><br>Request Patterns and Caching for VoD Services with Recommendation Systems.<br><br><br>Reduced-Rank Channel Estimation for Large-Scale MIMO Systems.<br><br><br>ITect: Scalable Information Theoretic Similarity for Malware Detection.<br><br><br>Comparison of several short-term traffic speed forecasting models.<br><br><br>Practical Demonstration of a Memristive Fuse.<br><br><br>Velocity-Aware Handover Management in Two-Tier Cellular Networks.<br><br><br>What can logic contribute to information theory?.<br><br><br>The Incentive Ratio in Exchange Economies.<br><br><br>Waveform Candidates for 5G Networks: Analysis and Comparison.<br><br><br>Incremental Consistency Guarantees for Replicated Objects.<br><br><br>Large-Scale Multi-Antenna Multi-Sine Wireless Power Transfer.<br><br><br>Proceedings of the 24th International Symposium on Graph Drawing and Network Visualization (GD 2016).<br><br><br>On the Performance Analysis of Underlay Cognitive Radio Systems: A Deployment Perspective.<br><br><br>HashTag Erasure Codes: From Theory to Practice.<br><br><br>A Flexible Recommendation System for Cable TV.<br><br><br>End-to-End Eye Movement Detection Using Convolutional Neural Networks.<br><br><br>A Large-Scale Characterization of User Behaviour in Cable TV.<br><br><br>Voltage stabilization in DC microgrids: an approach based on line-independent plug-and-play controllers.<br><br><br>Families of Optimal Binary Non-MDS Erasure Codes.<br><br><br>An Eigenshapes Approach to Compressed Signed Distance Fields and Their Utility in Robot Mapping.<br><br><br>Quantifying Radiographic Knee Osteoarthritis Severity using Deep Convolutional Neural Networks.<br><br><br>Fashion DNA: Merging Content and Sales Data for Recommendation and Article Mapping.<br><br><br>Information and dimensionality of anisotropic random geometric graphs.<br><br><br>Reduced Memory Region Based Deep Convolutional Neural Network Detection.<br><br><br>Functorial Hierarchical Clustering with Overlaps.<br><br><br>Interference-Free OFDM: Rethinking OFDM for Interference Networks with Inter-Symbol Interference.<br><br><br>Not All Fluctuations are Created Equal: Spontaneous Variations in Thermodynamic Function.<br><br><br>DiSMEC - Distributed Sparse Machines for Extreme Multi-label Classification.<br><br><br>The Number of Fixed Points of AND-OR Networks with Chain Topology.<br><br><br>Latest Datasets and Technologies Presented in the Workshop on Grasping and Manipulation Datasets.<br><br><br>I(FIB)F: Iterated Bloom Filters for Routing in Named Data Networks.<br><br><br>Quantum-assisted learning of graphical models with arbitrary pairwise connectivity.<br><br><br>Congestion-Aware Randomized Routing in Autonomous Mobility-on-Demand Systems.<br><br><br>Learning Phrasal Lexicons for Robotic Commands using Crowdsourcing.<br><br><br>A light-stimulated neuromorphic device based on graphene hybrid phototransistor.<br><br><br>The Discrete and Semi-continuous Fr\'echet Distance with Shortcuts via Approximate Distance Counting and Selection Techniques.<br><br><br>On Ideal Lattices, Gr\\"obner Bases and Generalized Hash Functions.<br><br><br>Constrained $H^1$-regularization schemes for diffeomorphic image registration.<br><br><br>Universal properties of culture: evidence for mixed rationalities in preference formation.<br><br><br>On the Groupoid Model of Computational Paths.<br><br><br>Sublinear Distance Labeling.<br><br><br>Super-Resolution of Point Sources via Convex Programming.<br><br><br>Exact algorithms for linear matrix inequalities.<br><br><br>Strong Scaling for Numerical Weather Prediction at Petascale with the Atmospheric Model NUMA.<br><br><br>Combining Monte-Carlo and Hyper-heuristic methods for the Multi-mode Resource-constrained Multi-project Scheduling Problem.<br><br><br>True Online Temporal-Difference Learning.<br><br><br>A Notation for Markov Decision Processes.<br><br><br>Pymanopt: A Python Toolbox for Optimization on Manifolds using Automatic Differentiation.<br><br><br>How useful is photo-realistic rendering for visual learning?.<br><br><br>Design and Analysis of Transmit Beamforming for Millimetre Wave Base Station Discovery.<br><br><br>Joint Semantic Segmentation and Depth Estimation with Deep Convolutional Networks.<br><br><br>Constant Envelope Precoding for MIMO Systems.<br><br><br>No Downlink Pilots are Needed in TDD Massive MIMO.<br><br><br>Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads.<br><br><br>Learning Convolutional Neural Networks using Hybrid Orthogonal Projection and Estimation.<br><br><br>Manifolds' Projective Approximation Using The Moving Least-Squares (MMLS).<br><br><br>Predicting Branch Visits and Credit Card Up-selling using Temporal Banking Data.<br><br><br>HRTF-based Robust Least-Squares Frequency-Invariant Polynomial Beamforming.<br><br><br>3D Human Pose Estimation Using Convolutional Neural Networks with 2D Pose Information.<br><br><br>Greedy Maximization Framework for Graph-based Influence Functions.<br><br><br>Stacked Approximated Regression Machine: A Simple Deep Learning Approach.<br><br><br>Learning to Rank Questions for Community Question Answering with Ranking SVM.<br><br><br>Multi-Pair Two-Way Relay Network with Harvest-Then-Transmit Users: Resolving Pairwise Uplink-Downlink Coupling.<br><br><br>Title Generation for User Generated Videos.<br><br><br>WNetKAT: A Weighted SDN Programming and Verification Language.<br><br><br>Approximate Bisimulation and Discretization of Hybrid CSP.<br><br><br>Coexistence in 5G: Analysis of Cross-Interference between OFDM/OQAM and Legacy Users.<br><br><br>Axiomatizing Category Theory in Free Logic.<br><br><br>Making a Case for Learning Motion Representations with Phase.<br><br><br>Calculation of Moments of Number of Comparisons used by the Randomized Quick Sort Algorithm.<br><br><br>Dense Motion Estimation for Smoke.<br><br><br>Random matrices meet machine learning: a large dimensional analysis of LS-SVM.<br><br><br></div>
<div style="visibility:hidden; display: none;" id="abstracts"><p>Suppression of H2-cooling in early protogalaxies has important implications
for the formation of supermassive black holes seeds, the first generation of
stars, and the epoch of reionization. This suppression can occur via
photodissociation of H2 (by ultraviolet Lyman-Werner [LW] photons) or by
photodetachment of H, a precursor in H2 formation (by infrared [IR] photons).
Previous studies have typically adopted idealised spectra, with a blackbody or
a power-law shape, in modeling the chemistry of metal-free protogalaxies, and
utilised a single parameter, the critical UV flux, or Jcrit, to determine
whether H2-cooling is prevented. Here we point out that this can be misleading,
and that independent of the spectral shape, there is a critical curve in the
(kLW,kH) plane, where kLW and kH are the H2-dissociation rates by LW and IR
photons, which determines whether a protogalaxy can cool below ~1000 Kelvin. We
use a one-zone model to follow the chemical and thermal evolution of
gravitationally collapsing protogalactic gas, to compute this critical curve,
and provide an accurate analytical fit for it. We then consider a variety of
more realistic Pop III or Pop II-type spectra from population synthesis models,
and perform fully frequency-dependent calculations of the H2-photodissociation
rates for each spectrum. We compute the ratio kLW/kH for each spectrum, as well
as the minimum stellar mass M*, for various IMFs and metallicities, required to
prevent cooling in a neighboring halo a distance d away. We provide critical
M*/d2 values for suppression of H2-cooling, with analytic fits, which can be
used in future studies.
</p>
<br><br><br><p>Observations of exoplanetary spectra are leading to unprecedented constraints
on their atmospheric elemental abundances, particularly O/H, C/H, and C/O
ratios. Recent studies suggest that elemental ratios could provide important
constraints on formation and migration mechanisms of giant exoplanets. A
fundamental assumption in such studies is that the chemical composition of the
planetary envelope represents the sum-total of compositions of the accreted gas
and solids during the formation history of the planet. We investigate the
efficiency with which accreted planetesimals ablate in a giant planetary
envelope thereby contributing to its composition rather than sinking to the
core. From considerations of aerodynamic drag causing `frictional ablation' and
the envelope temperature structure causing `thermal ablation', we compute mass
ablations for impacting planetesimals of radii 30 m to 1 km for different
compositions (ice to iron) and a wide range of velocities and impact angles,
assuming spherical symmetry. Icy impactors are fully ablated in the outer
envelope for a wide range of parameters. Even for Fe impactors substantial
ablation occurs in the envelope for a wide range of sizes and velocities. For
example, iron impactors of sizes below ~0.5 km and velocities above ~30 km/s
are found to ablate by ~60-80% within the outer envelope at pressures below
10^3 bar due to frictional ablation alone. For deeper pressures (~10^7 bar),
substantial ablation happens over a wider range of parameters. Therefore, our
exploratory study suggests that atmospheric abundances of volatile elements in
giant planets reflect their accretion history during formation.
</p>
<br><br><br><p>Pulsar-timing datasets have been analyzed with great success using
probabilistic treatments based on Gaussian distributions, with applications
ranging from studies of neutron-star structure to tests of general relativity
and searches for nanosecond gravitational waves. As for other applications of
Gaussian distributions, outliers in timing measurements pose a significant
challenge to statistical inference, since they can bias the estimation of
timing and noise parameters, and affect reported parameter uncertainties. We
describe and demonstrate a practical end-to-end approach to perform Bayesian
inference of timing and noise parameters robustly in the presence of outliers,
and to identify these probabilistically. The method is fully consistent (i.e.,
outlier-ness probabilities vary in tune with the posterior distributions of the
timing and noise parameters), and it relies on the efficient sampling of the
hierarchical form of the pulsar-timing likelihood. Such sampling has recently
become possible with a \"no-U-turn\" Hamiltonian sampler coupled to a highly
customized reparametrization of the likelihood; this code is described
elsewhere, but it is already available online. We recommend our method as a
standard step in the preparation of pulsar-timing-array datasets: even if
statistical inference is not affected, follow-up studies of outlier candidates
can reveal unseen problems in radio observations and timing measurements;
furthermore, confidence in the results of gravitational-wave searches will only
benefit from stringent statistical evidence that datasets are clean and
outlier-free.
</p>
<br><br><br><p>We present a 1.4 GHz Karl G. Jansky Very Large Array (VLA) study of a sample
of early-type galaxies (ETGs) from the volume- and magnitude-limited ATLAS-3D
survey. The radio morphologies of these ETGs at a resolution of 5\" are diverse
and include sources that are compact on sub-kpc scales, resolved structures
similar to those seen in star-forming spiral galaxies, and kpc-scale radio
jets/lobes associated with active nuclei. We compare the 1.4 GHz, molecular
gas, and infrared (IR) properties of these ETGs. The most CO-rich ATLAS-3D ETGs
have radio luminosities consistent with extrapolations from H_2-mass-derived
star formation rates from studies of late-type galaxies. These ETGs also follow
the radio-IR correlation. However, ETGs with lower molecular gas masses tend to
have less radio emission relative to their CO and IR emission compared to
spirals. The fraction of galaxies in our sample with high IR-radio ratios is
much higher than in previous studies, and cannot be explained by a systematic
underestimation of the radio luminosity due to the presence extended,
low-surface-brightness emission that was resolved-out in our VLA observations.
In addition, we find that the high IR-radio ratios tend to occur at low IR
luminosities, but are not associated with low dynamical mass or metallicity.
Thus, we have identified a population of ETGs that have a genuine shortfall of
radio emission relative to both their IR and molecular gas emission. A number
of mechanisms may conspire to cause this radio deficiency, including a
bottom-heavy stellar initial mass function, weak magnetic fields, a higher
prevalence of environmental effects compared to spirals and enhanced cosmic ray
losses.
</p>
<br><br><br><p>We consider a new class of thermal dark matter models, dubbed \"Impeded Dark
Matter\", in which the mass splitting between the dark matter particles and
their annihilation products is tiny. Compared to the previously proposed
Forbidden Dark Matter scenario, the mass splittings we consider are much
smaller, and are allowed to be either positive or negative. We demonstrate that
either case can be easily realized without requiring tuning of model
parameters. For negative mass splitting, we demonstrate that the annihilation
cross-section for Impeded Dark Matter depends linearly on the dark matter
velocity or may even be kinematically forbidden, making this scenario almost
insensitive to constraints from the cosmic microwave background and from
observations of dwarf galaxies. Accordingly, it may be possible for Impeded
Dark Matter to yield observable signals in clusters or the Galactic center,
with no corresponding signal in dwarfs. For positive mass splitting, we show
that the annihilation cross-section is suppressed by the small mass splitting,
which helps light dark matter to survive increasingly stringent constraints
from indirect searches. As specific realizations for Impeded Dark Matter, we
introduce a model of vector dark matter from a hidden SU(2) sector, and a
composite dark matter scenario based on a QCD-like dark sector.
</p>
<br><br><br><p>We report a new ultra-faint stellar system found in Dark Energy Camera data
from the first observing run of the Magellanic Satellites Survey (MagLiteS).
MagLiteS J0664-5953 (Pictor II or Pic II) is a low surface brightness ({\mu} =
28.5 mag arcsec$^{-2}$ within its half-light radius) resolved overdensity of
old and metal-poor stars located at a heliocentric distance of 45 kpc. The
physical size (r$_{1/2}$ = 46 pc) and low luminosity (Mv = -3.2 mag) of this
satellite are consistent with the locus of spectroscopically confirmed
ultra-faint galaxies. MagLiteS J0664-5953 (Pic II) is located 11.3 kpc from the
Large Magellanic Cloud (LMC), and comparisons with simulation results in the
literature suggest that this satellite was likely accreted with the LMC. The
close proximity of MagLiteS J0664-5953 (Pic II) to the LMC also makes it the
most likely ultra-faint galaxy candidate to still be gravitationally bound to
the LMC.
</p>
<br><br><br><p>We revisit calculations of nebular hydrogen Lya and HeII 1640A line strengths
for population III galaxies, undergoing continuous and bursts of star
formation. We focus on initial mass functions (IMFs) motivated by recent
theoretical studies, which generally span a lower range of stellar masses than
earlier works. We also account for case-B departures and the stochastic
sampling of the IMF. In agreement with previous works, we find that departures
from case-B can enhance the Lya flux by a factor of a few, but we argue that
this enhancement is driven mainly by collisional excitation and ionization, and
not due to photoionization from the n = 2 state of atomic hydrogen. The
increased sensitivity of the Lya flux to the high-energy end of the galaxy
spectrum makes it more subject to stochastic sampling of the IMF. The latter
introduces a dispersion in the predicted nebular line fluxes around the
deterministic value by as much as a factor of ~4. In contrast, the stochastic
sampling of the IMF has less impact on the emerging Lyman Werner (LW) photon
flux. When case-B departures and stochasticity effects are combined, nebular
line emission from population III galaxies can be up to one order of magnitude
brighter than predicted by 'standard' calculations that do not include these
effects. This enhances the prospects for detection with future facilities such
as JWST and large, groundbased telescopes.
</p>
<br><br><br><p>We used ultra-deep $J$ and $K_s$ images secured with the near-infrared GSAOI
camera assisted by the multi-conjugate adaptive optics system GeMS at the
GEMINI South Telescope in Chile, to obtain a ($K_s$, $J-K_s$) color-magnitude
diagram (CMD) for the bulge globular cluster NGC 6624. We obtained the deepest
and most accurate near-infrared CMD from the ground for this cluster, by
reaching $K_s$ $\sim$ 21.5, approximately 8 magnitudes below the horizontal
branch level. The entire extension of the Main Sequence (MS) is nicely sampled
and at $K_s$ $\sim$ 20 we detected the so-called MS \"knee\" in a purely
near-infrared CMD. By taking advantage of the exquisite quality of the data, we
estimated the absolute age of NGC 6624 ($t_{age}$ = 12.0 $\pm$ 0.5 Gyr), which
turns out to be in good agreement with previous studies in the literature. We
also analyzed the luminosity and mass functions of MS stars down to M $\sim$
0.45 M$_{\odot}$ finding evidence of a significant increase of low-mass stars
at increasing distances from the cluster center. This is a clear signature of
mass segregation, confirming that NGC 6624 is in an advanced stage of dynamical
evolution.
</p>
<br><br><br><p>There is growing theoretical and observational evidence that protoplanetary
disc evolution may be significantly affected by the canonical levels of far
ultraviolet (FUV) radiation found in a star forming environment, leading to
substantial stripping of material from the disc outer edge even in the absence
of nearby massive stars. In this paper we perform the first full radiation
hydrodynamic simulations of the flow from the outer rim of protoplanetary discs
externally irradiated by such intermediate strength FUV fields, including
direct modelling of the photon dominated region (PDR) which is required to
accurately compute the thermal properties. We find excellent agreement between
our models and the semi-analytic models of Facchini et al. (2016) for the
profile of the flow itself, as well as the mass loss rate and location of their
\"critical radius\". This both validates their results (which differed
significantly from prior semi-analytic estimates) and our new numerical method,
the latter of which can now be applied to elements of the problem that the
semi--analytic approaches are incapable of modelling. We also obtain the
composition of the flow, but given the simple geometry of our models we can
only hint at some diagnostics for future observations of externally irradiated
discs at this stage. We also discuss the potential for these models as
benchmarks for future photochemical-dynamical codes.
</p>
<br><br><br><p>The nature of warm, ionized gas outside of galaxies may illuminate several
key galaxy evolutionary processes. A serendipitous observation by the MaNGA
survey has revealed a large, asymmetric H$\alpha$ complex with no optical
counterpart that extends $\approx8\"$ ($\approx6.3$ kpc) beyond the effective
radius of a dusty, starbursting galaxy. This H$\alpha$ extension is
approximately three times the effective radius of the host galaxy and displays
a tail-like morphology. We analyze its gas-phase metallicities, gaseous
kinematics, and emission-line ratios, and discuss whether this H$\alpha$
extension could be diffuse ionized gas, a gas accretion event, or something
else. We find that this warm, ionized gas structure is most consistent with gas
accretion through recycled wind material, which could be an important process
that regulates the low-mass end of the galaxy stellar mass function.
</p>
<br><br><br><p>From a sample of spectra of 439 white dwarfs (WDs) from the ESO-VLT
Supernova-Ia Progenitor surveY (SPY), we measure the maximal changes in
radial-velocity (DRVmax) between epochs (generally two epochs, separated by up
to 470d), and model the observed DRVmax statistics via Monte-Carlo simulations,
to constrain the population characteristics of double WDs (DWDs). The DWD
fraction among WDs is fbin=0.103+/-0.017 (1-sigma, random) +/-0.015
(systematic), in the separation range ~&lt;4AU within which the data are sensitive
to binarity. Assuming the distribution of binary separation, a, is a power-law,
dN/dt ~ a^alpha, at the end of the last common-envelope phase and the start of
solely gravitational-wave-driven binary evolution, the constraint by the data
is alpha=-1.4+/-0.4 (1-sigma). If these parameters extend to small separations,
the implied Galactic WD merger rate per unit stellar mass is R_merge=1.4e-13 to
1.3e-11 /yr/Msun (2-sigma), with a likelihood-weighted mean of
R_merge=(7.3+/-2.7)e-13 /yr/Msun (1-sigma). The Milky Way's specific Type-Ia
supernova (SN Ia) rate is likely R_Ia~1.1e-13 /yr/Msun and therefore, in terms
of rates, a fraction of all merging DWDs (e.g. those with massive-enough
primary WDs) could suffice to produce most or all SNe Ia.
</p>
<br><br><br><p>Quantum gravity may allow black holes to tunnel into white holes. If so, the
lifetime of a black hole could be shorter than the one given by Hawking
evaporation, solving the information paradox. More interestingly, this could
open to a new window for quantum-gravity phenomenology, in connection with the
existence of primordial black holes. We discuss in particular the power of the
associated explosion and the possibility to observe an astrophysical signal in
the radio and in the gamma wavelengths.
</p>
<br><br><br><p>This work studies the correlation among environmental density and radio AGN
presence up to $z = 2$. Using data from the photometric COSMOS survey and its
radio 1.4 GHz follow-up (VLA-COSMOS), a sample of radio AGNs has been defined.
The environment was studied using the richness distributions inside a
parallelepiped with base side of 1 Mpc and height proportional to the
photometric redshift precision. Radio AGNs are found to be always located in
environments significantly richer than those around galaxies with no radio
emission. Moreover, a distinction based on radio AGN power shows that the
significance of the environmental effect is only maintained for low-power radio
sources. The results of this work show that denser environments play a
significant role in enhancing the probability that a galaxy hosts a radio AGN
and, in particular, low-power ones.
</p>
<br><br><br><p>This work reports new experimental radiative lifetimes and calculated
oscillator strengths for transitions of astrophysical interest in singly
ionized cobalt. More pre- cisely, nineteen radiative lifetimes in Co+ have been
measured with the time-resolved laser-induced fluorescence technique using one-
and two-step excitations. Out of these, seven belonging to the high lying
3d$^7$($^4$F)4d configuration in the energy range 90697 - 93738 cm$^{-1}$ are
new, and the other twelve from th3d$^7$($^4$F)F)4p configuration with energies
between 45972 and 49328 cm$^{-1}$1 are compared with previous measurements. In
addition, a relativistic Hartree-Fock model including core-polarization e?ects
has been employed to compute transition rates. Supported by the good agreement
between theory and experiment for the lifetimes, new reliable transition
probabilities and os- cillator strengths have been deduced for 5080 Co II
transitions in the spectral range 114 - 8744 nm.
</p>
<br><br><br><p>A current issue in the study of planetary nebulae with close binary central
stars is the extent to which the binaries affect the shaping of the nebulae.
Recent studies have begun to show a high coincidence rate between nebulae with
large-scale axial or point symmetries and close binary stars. In addition,
combined binary-star and spatio-kinematic modeling of the nebulae have
demonstrated that all of the systems studied to date appear to have their
central binary axis aligned with the primary axis of the nebula. Here we add
two more systems to the list, the central stars and nebulae of NGC 6337 and Sp
1. We show both systems to be low inclination, with their binary axis nearly
aligned with our line-of-sight. Their inclinations match published values for
the inclinations of their surrounding nebulae. Including these two systems with
the existing sample statistically demonstrates a direct link between the
central binary and the nebular morphology. In addition to the systems'
inclinations we give ranges for other orbital parameters from binary modeling,
including updated orbital periods for the binary central stars of NGC 6337 and
Sp 1.
</p>
<br><br><br><p>We outline a new method to compute the Bayes Factor for model selection which
bypasses the Bayesian Evidence. Our method combines multiple models into a
single, nested, Supermodel using one or more hyperparameters. Since the models
are now nested the Bayes Factors between the models can be efficiently computed
using the Savage-Dickey Density Ratio (SDDR). In this way model selection
becomes a problem of parameter estimation. We consider two ways of constructing
the supermodel in detail: one based on combined models, and a second based on
combined likelihoods. We report on these two approaches for a Gaussian linear
model for which the Bayesian evidence can be calculated analytically and a toy
nonlinear problem. Unlike the combined model approach, where a standard Monte
Carlo Markov Chain (MCMC) struggles, the combined-likelihood approach fares
much better in providing a reliable estimate of the log-Bayes Factor. This
scheme potentially opens the way to computationally efficient ways to compute
Bayes Factors in high dimensions that exploit the good scaling properties of
MCMC, as compared to methods such as nested sampling that fail for high
dimensions.
</p>
<br><br><br><p>We present the stellar parameters and elemental abundances of a set of
A--F-type supergiant stars HD\,45674, HD\,180028, HD\,194951 and HD\,224893
using high resolution ($R$\,$\sim$\,42,000) spectra taken from ELODIE library.
We present the first results of the abundance analysis for HD\,45674 and
HD\,224893. We reaffirm the abundances for HD\,180028 and HD\,194951 studied
previously by Luck (2014) respectively. Alpha-elements indicates that objects
belong to the thin disc population. From their abundances and its location on
the Hertzsprung-Russell diagram seems point out that HD\,45675, HD\,194951 and
HD\,224893 are in the post-first dredge-up (post-1DUP) phase and they are
moving in the red-blue loop region. HD~180028, on the contary, shows typical
abundances of the population I but its evolutionary status could not be
satisfactorily defined.
</p>
<br><br><br><p>We report the first hard X-ray observations with NuSTAR of the BL Lac type
blazar PKS 2155-304, augmented with soft X-ray data from XMM-Newton and
gamma-ray data from the Fermi Large Area Telescope, obtained in April 2013 when
the source was in a very low flux state. A joint NuSTAR and XMM spectrum,
covering the energy range 0.5 - 60 keV, is best described by a model consisting
of a log-parabola component with curvature beta = 0.3(+0.2,-0.1) and a (local)
photon index 3.04 +/- 0.15 at photon energy of 2 keV, and a hard power-law tail
with photon index 2.2 +/- 0.4. The hard X-ray tail can be smoothly joined to
the quasi-simultaneous gamma-ray spectrum by a synchrotron self-Compton
component produced by an electron distribution with index p = 2.2. Assuming
that the power-law electron distribution extends down to the minimum electron
Lorentz factor gamma_min = 1 and that there is one proton per electron, an
unrealistically high total jet power L_p of roughly 10^47 erg/s is inferred.
This can be reduced by two orders of magnitude either by considering a
significant presence of electron-positron pairs with lepton-to-proton ratio of
at least 30, or by introducing an additional, low-energy break in the electron
energy distribution at the electron Lorentz factor gamma_br1 of roughly 100. In
either case, the jet composition is expected to be strongly matter-dominated.
</p>
<br><br><br><p>We identify 709 arc-shaped mid-infrared nebula in 24 micron Spitzer Space
Telescope or 22 micron Wide Field Infrared Explorer surveys of the Galactic
Plane as probable dusty interstellar bowshocks powered by early-type stars.
About 20% are visible at 8 microns or shorter mid-infrared wavelengths as well.
The vast majority (660) have no previous identification in the literature.
These extended infrared sources are strongly concentrated near Galactic
mid-Plane with an angular scale height of ~0.6 degrees. All host a
symmetrically placed star implicated as the source of a stellar wind sweeping
up interstellar material. These are candidate \"runaway\" stars potentially
having high velocities in the reference frame of the local medium. Among the
286 objects with measured proper motions, we find an unambiguous excess having
velocity vectors aligned with the infrared morphology --- kinematic evidence
that many of these are \"runaway\" stars with large peculiar motions responsible
for the bowshock signature. We discuss a population of \"in-situ\" bowshocks (103
objects) that face giant HII regions where the relative motions between the
star and ISM may be caused by bulk outflows from an overpressured bubble. We
also identify 58 objects that face 8 micron bright-rimmed clouds and apparently
constitute a sub-class of in-situ bowshocks where the stellar wind interacts
with a photo-evaporative flow from an eroding molecular cloud interface (i.e.,
\"PEF bowshocks\"). Orientations of the arcuate nebulae exhibit a correlation
over small angular scales, indicating that external influences such as HII
regions are responsible for producing some bowshock nebulae. However, the vast
majority of this sample appear to be isolated (499 objects) from obvious
external influences.
</p>
<br><br><br><p>For a long time very little experimental information was available about
neutrino properties, even though a minute neutrino mass has intriguing
cosmological and astrophysical implications. This situation has changed in
recent decades: intense experimental activity to measure many neutrino
properties took place. Some of these developments and their implications for
astrophysics and cosmology are briefly reviewed with a particular emphasis on
neutrino magnetic moments and collective neutrino oscillations
</p>
<br><br><br><p>We report follow-up studies of 35 recently-discovered cataclysmic variables
(CVs), 32 of which were found in large, automated synoptic sky surveys. The
objects were selected for observational tractability. For 34 of the objects we
present mean spectra and spectroscopic orbital periods, and for one more we
give an eclipse-based period. Thirty-two of the period determinations are new,
and three of these refine published estimates based on superhump periods. The
remaining three of our determinations confirm previously published periods.
Twenty of the stars are confirmed or suspected dwarf novae with periods shorter
than 3 hours, but we also find three apparent polars (AM Her stars), and six
systems with P &gt; 5 h, five of which have secondary stars visible in their
spectra, from which we estimate distances when possible. The orbital period
distribution of this sample is very similar to that of previously discovered
CVs.
</p>
<br><br><br><p>I analyze statistics of the stellar population properties for stellar nuclei
and bulges of nearby lenticular galaxies in different environments by using
panoramic spectral data of the integral-field spectrograph SAURON retrieved
from the open archive of Isaac Newton Group. I estimate also the fraction of
nearby lenticular galaxies having inner polar gaseous disks by exploring the
volume-limited sample of early-type galaxies of the ATLAS-3D survey. By
inspecting the two-dimensional velocity fields of the stellar and gaseous
components with running tilted-ring technique, I have found 7 new cases of the
inner polar disks. Together with those, the frequency of inner polar disks in
nearby S0 galaxies reaches 10% that is much higher than the frequency of
large-scale polar rings. Interestingly, the properties of the nuclear stellar
populations in the inner polar ring hosts are statistically the same as those
in the whole S0 sample implying similar histories of multiple gas accretion
events from various directions.
</p>
<br><br><br><p>We present the discovery of a variable optical counterpart to the
unidentified gamma-ray source 3FGL J0212.1+5320, and argue this is a new
compact binary millisecond pulsar (MSP) candidate. We show 3FGL J0212.1+5320
hosts a semi-detached binary with a 0.86955$\pm$0.00015 d orbital period and a
F6-type companion star at an estimated distance of D=1.1$\pm$0.2 kpc, with a
radial velocity curve semi-amplitude K$_2$=214.1$\pm$5.0 km s$^{-1}$ and a
projected rotational velocity of Vsin(i)=73.2$\pm$1.6 km s$^{-1}$. We find a
hard X-ray source at the same location with a 0.5$-$10 keV luminosity
L$_\mathrm{X}$=2.6$\times$10$^{32}$ (D/1.1 kpc)$^2$ erg s$^{-1}$, which
strengthens the MSP identification. Our results imply a mass ratio
q=M$_2$/M$_1$=0.26$^{+0.02}_{-0.03}$ if the companion star fills its Roche
lobe, and q$\gtrsim$0.23 in any case. This classifies 3FGL J0212.1+5320 as a
\"redback\" binary MSP; if its MSP nature is confirmed, this will be the
brightest compact binary MSP in the optical band (r'$\simeq$14.3 mag) and will
have the longest orbital period among Galactic field systems (nearly 21 hr).
Based on the light curve peak-to-peak amplitude ($\Delta$r=0.19 mag), we
further suggest that the orbital inclination is high and the putative pulsar
mass is close to canonical (M$_1$$\simeq$1.3$-$1.6 M$_\odot$). Finally, we
discuss the lack of heating signatures and asymmetric optical light curves in
the context of other redback MSPs.
</p>
<br><br><br><p>For the first time, we explore the dynamics of the central region of a galaxy
cluster within $r_{500}\sim 600h^{-1}$~kpc from its center by combining optical
and X-ray spectroscopy. We use (1) the caustic technique that identifies the
cluster substructures and their galaxy members with optical spectroscopic data,
and (2) the X-ray redshift fitting procedure that estimates the redshift
distribution of the intracluster medium (ICM). We use the spatial and redshift
distributions of the galaxies and of the X-ray emitting gas to associate the
optical substructures to the X-ray regions. When we apply this approach to
Abell 85 (A85), a complex dynamical structure of A85 emerges from our analysis:
a galaxy group, with redshift $z=0.0509 \pm 0.0021$ is passing through the
cluster center along the line of sight dragging part of the ICM present in the
cluster core; two additional groups, at redshift $z=0.0547 \pm 0.0022$ and
$z=0.0570 \pm 0.0020$, are going through the cluster in opposite directions,
almost perpendicularly to the line of sight, and have substantially perturbed
the dynamics of the ICM. An additional group in the outskirts of A85, at
redshift $z=0.0561 \pm 0.0023$, is associated to a secondary peak of the X-ray
emission, at redshift $z=0.0583^{+0.0039}_{-0.0047}$. Although our analysis and
results on A85 need to be confirmed by high-resolution spectroscopy, they
demonstrate how our new approach can be a powerful tool to constrain the
formation history of galaxy clusters by unveiling their central and surrounding
structures.
</p>
<br><br><br><p>By performing a global magnetohydrodynamical (MHD) simulation for the Milky
Way with an axisymmetric gravitational potential, we propose that spatially
dependent amplification of magnetic fields possibly explains the observed
noncircular motion of the gas in the Galactic centre (GC) region. The radial
distribution of the rotation frequency in the bulge region is not monotonic in
general. The amplification of the magnetic field is enhanced in regions with
stronger differential rotation, because magnetorotational instability and
field-line stretching are more effective. The strength of the amplified
magnetic field reaches &gt;~ 0.5 mG, and radial flows of the gas are excited by
the inhomogeneous transport of angular momentum through turbulent magnetic
field that is amplified in a spatially dependent manner. As a result, the
simulated position-velocity diagram exhibits a time-dependent asymmetric
parallelogram-shape owing to the intermittency of the magnetic turbulence; the
present model provides a viable alternative to the bar-potential-driven model
for the parallelogram shape of the central molecular zone. In addition, Parker
instability (magnetic buoyancy) creates vertical magnetic structure, which
would correspond to observed molecular loops, and frequently excited vertical
flows. Furthermore, the time-averaged net gas flow is directed outward, whereas
the flows are highly time dependent, which would contribute to the outflow from
the bulge.
</p>
<br><br><br><p>Ground Level Enhancements (GLEs) of cosmic-ray intensity occur, on average,
once a year. Due to their rareness, studying the solar sources of GLEs is
especially important to approach understanding their origin. The SOL2001-12-26
eruptive-flare event responsible for GLE63 seems to be challenging in some
aspects. Deficient observations limited its understanding. Analysis of extra
observations found for this event provided new results shading light on the
flare. This article addresses the observations of this flare with the Siberian
Solar Radio Telescope (SSRT). Taking advantage of its instrumental
characteristics, we analyze the detailed SSRT observations of a major
long-duration flare at 5.7 GHz without cleaning the images. The analysis
confirms that the source of GLE63 was associated with an event in active region
9742 that comprised two flares. The first flare (04:30-05:03 UT) reached a GOES
importance of about M1.6. Two microwave sources were observed, whose brightness
temperatures at 5.7 GHz exceeded 10 MK. The main flare, up to the M7.1
importance, started at 05:04 UT, and occurred in strong magnetic fields. The
observed microwave sources reached about 250 MK. They were not static. Having
appeared on the weaker-field periphery of the active region, the microwave
sources moved toward each other nearly along the magnetic neutral line,
approaching a stronger-field core of the active region, and then moved away
from the neutral line like expanding ribbons. These motions rule out an
association of the non-thermal microwave sources with a single flaring loop.
</p>
<br><br><br><p>Recently, the High Energy Stereoscopic System (H.E.S.S.) reported two new
interesting results for a gamma-ray emitting supernova remnant, RX J1713.7-3946
(G347.3-0.5). One is a broken power-law spectrum of GeV-TeV gamma-rays. The
other is a more extended gamma-ray spatial profile than X-rays. In this paper,
we show both of these results can be explained by inverse Compton emission from
accelerated electrons. If the maximum energy of electrons being accelerated
decreases with time, the broken power-law spectrum can be generated by
accumulation. Furthermore, the extended component of gamma-ray profile can be
interpreted as a CR precursor of currently accelerated electrons.
</p>
<br><br><br><p>Based on the largest catalogs currently available, comprising 6090 contact
binaries (CBs) and 2167 open clusters, we determine the near-infrared $JHK_{\rm
s}$ CB period--luminosity (PL) relations, for the first time achieving the low
levels of intrinsic scatter that make these relations viable as competitive
distance calibrators. To firmly establish our distance calibration on the basis
of open cluster CBs, we require that (i) the CB of interest must be located
inside the core radius of its host cluster; (ii) the CB's proper motion must be
located within the $2\sigma$ distribution of that of its host open cluster; and
(iii) the CB's age, $t$, must be comparable to that of its host cluster, i.e.,
$\Delta \log (t\mbox{ yr}^{-1}) &lt;0.3$. We thus select a calibration sample of
66 CBs with either open cluster distances or accurate space-based parallaxes.
The resulting near-infrared PL relations, for both late-type (i.e., W Ursae
Majoris-type) and---for the first time---early-type CBs, are as accurate as the
well-established $JHK_{\rm s}$ Cepheid PL relations, (characterized by
single-band statistical uncertainties of $\sigma &lt; 0.10$ mag). We show that CBs
can be used as viable distance tracers, yielding distances with uncertainties
of better than 5\% for 90\% of the 6090 CBs in our full sample. By combining
the full $JHK_{\rm s}$ photometric data set, CBs can trace distances with an
accuracy, $\sigma=0.05 \mbox{ (statistical)} \pm0.03 \mbox{ (systematic)}$ mag.
The 102 CBs in the Large Magellanic Cloud are used to determine a distance
modulus to the galaxy of $(m-M_V)_0^{\rm LMC}=18.41\pm0.20$ mag.
</p>
<br><br><br><p>Infrared-Faint Radio Sources (IFRS) represent an unexpected class of objects
relatively bright at radio wavelength, but unusually faint at infrared (IR) and
optical wavelengths. A recent and extensive campaign on the radio-brightest
IFRSs (S_{1.4GHz} &gt;= 10 mJy) has provided evidence that most of them (if not
all) contain an AGN. Still uncertain is the nature of the radio-faintest ones
(S_{1.4GHz} &lt;= 1 mJy). The scope of this paper is to assess the nature of the
radio-faintest IFRSs, testing their classification and improving the knowledge
of their IR properties making use of the most sensitive IR survey available so
far: the Spitzer Extragalactic Representative Volume Survey (SERVS). We also
explore how the criteria of IFRSs can be fine-tuned to pinpoint radio-loud AGNs
at very high redshift (z &gt; 4). We analysed a number of IFRS samples identified
in SERVS fields, including a new sample (21 sources) extracted from the Lockman
Hole. 3.6 and 4.5 mum IR counterparts of the 64 sources located in the SERVS
fields were searched for, and, when detected, their IR properties were studied.
We compared the radio/IR properties of the IR-detected IFRSs with those
expected for a number of known classes of objects. We found that they are
mostly consistent with a mixture of high-redshift (z &gt;= 3) radio-loud AGNs. The
faintest ones (S_{1.4GHz} ~ 100 muJy), however, could be also associated with
nearer (z ~ 2) dust-enshrouded star-burst galaxies. We also argue that, while
IFRSs with radio-to-IR ratios &gt; 500 can very efficiently pinpoint radio-loud
AGNs at redshift 2 &lt; z &lt; 4, lower radio-to-IR ratios (~ 100--200) are expected
for higher redshift radio-loud AGNs.
</p>
<br><br><br><p>We present the Collection of Extraction Routines for Echelle Spectra (CERES).
These routines were developed for the construction of automated pipelines for
the reduction, extraction and analysis of spectra acquired with different
instruments, allowing the obtention of homogeneous and standardised results.
This modular code includes tools for handling the different steps of the
processing: CCD reductions, tracing of the echelle orders, optimal and simple
extraction, computation of the wavelength solution, estimation of radial
velocities, and rough and fast estimation of the atmospheric parameters.
Currently, CERES has been used to develop automated pipelines for eleven
different spectrographs, namely CORALIE, FEROS, HARPS, PUCHEROS, FIDEOS, CAFE,
DuPont/Echelle, Magellan/Mike, Keck/HIRES, Magellan/PFS and APO/ARCES, but the
routines can be easily used in order to deal with data coming from other
spectrographs. We show the high precision in radial velocity that CERES
achieves for some of these instruments and we briefly summarize some results
that have already been obtained using the CERES pipelines.
</p>
<br><br><br><p>The mean alpha-to-iron abundance ratio ([$\alpha$/Fe]) of galaxies is
sensitive to the chemical evolution processes at early time, and it is an
indicator of star formation timescale ($\tau_{{\rm SF}}$). Although the
physical reason remains ambiguous, there is a tight relation between
[$\alpha$/Fe] and stellar velocity dispersion ($\sigma$) among massive
early-type galaxies (ETGs). However, no work has shown convincing results as to
how this relation behaves at low masses. We assemble 15 data sets from the
literature and build a large sample that includes 192 nearby low-mass
($18&lt;\sigma&lt;80$~\kms) ETGs. We find that the [$\alpha$/Fe]-$\sigma$ relation
generally holds for low-mass ETGs, except in extreme environments.
Specifically, in normal galaxy cluster environments, the [$\alpha$/Fe]-$\sigma$
relation and its intrinsic scatter are, within uncertainties, similar for
low-mass and high-mass ETGs. However, in the most massive relaxed galaxy
cluster in our sample, the zero point of the relation is higher and the
intrinsic scatter is significantly larger. By contrast, in galaxy groups the
zero point of the relation offsets in the opposite direction, again with
substantial intrinsic scatter. The elevated [$\alpha$/Fe] of low-mass ETGs in
the densest environments suggests that their star formation was quenched
earlier than in high-mass ETGs. For the low-mass ETGs in the lowest density
environments, we suggest that their more extended star formation histories
suppressed their average [$\alpha$/Fe]. The large scatter in [$\alpha$/Fe] may
reflect stochasticity in the chemical evolution of low-mass galaxies.
</p>
<br><br><br><p>We propose a novel particle physics model in which vector dark matter (VDM)
and dark radiation (DR) originate from the same non-Abelian dark sector. We
show an illustrating example where dark $SU(3)$ is spontaneously broken into
$SU(2)$ subgroup by the nonzero vacuum expectation value of a complex scalar in
fundamental representation of $SU(3)$. The massless gauge bosons associated
with the residual unbroken $SU(2)$ constitute DR and help to relieve the
tension in Hubble constant measurements between $\textit{Planck}$ and Hubble
Space Telescope. In the meantime, massive dark gauge bosons associated with the
broken generators are VDM candidates. Intrinsically, this non-Abelian VDM can
interact with non-Abelian DR in the cosmic background, which results in a
suppressed matter power spectrum and leads to a smaller $\sigma_8$ for
structure formation.
</p>
<br><br><br><p>We have investigated the effect of group environment on residual star
formation in galaxies, using Galex NUV galaxy photometry with the SDSS group
catalogue of Yang et al. (2007). We compared the (NUV $- r$) colours of grouped
and non-grouped galaxies, and find a significant increase in the fraction of
red sequence galaxies with blue (NUV $- r$) colours outside of groups. When
comparing galaxies in mass matched samples of satellite (non-central), and
non-grouped galaxies, we found a &gt; 4{\sigma} difference in the distribution of
(NUV $- r$) colours, and an (NUV $- r$) blue fraction $&gt; 3{\sigma}$ higher
outside groups. A comparison of satellite and non-grouped samples has found the
NUV fraction is a factor of $\sim2$ lower for satellite galaxies between
$10^{10.5}M_{\bigodot}$ and $10^{10.7}M_{\bigodot}$, showing that higher mass
galaxies are more able to form stars when not influenced by a group potential.
There was a higher (NUV $- r$) blue fraction of galaxies with lower Sersic
indices (n &lt; 3) outside of groups, not seen in the satellite sample. We have
used stellar population models of Bruzual &amp; Charlot (2003) with multiple burst,
or exponentially declining star formation histories to find that many of the
(NUV $- r$) blue non-grouped galaxies can be explained by a slow ($\sim 2$ Gyr)
decay of star formation, compared to the satellite galaxies. We suggest that
taken together, the difference in (NUV $- r$) colours between samples can be
explained by a population of secularly evolving, non-grouped galaxies, where
star formation declines slowly. This slow channel is less prevalent in group
environments where more rapid quenching can occur.
</p>
<br><br><br><p>The early star-forming Universe is still poorly constrained, with the
properties of high-redshift stars, the first heating sources, and reionization
highly uncertain. This leaves observers planning 21-cm experiments with little
theoretical guidance. In this work we explore the possible range of
high-redshift parameters including the star formation efficiency and the
minimal mass of star-forming halos; the efficiency, spectral energy
distribution, and redshift evolution of the first X-ray sources; and the
history of reionization. These parameters are only weakly constrained by
available observations, mainly the optical depth to the cosmic microwave
background. We use realistic semi-numerical simulations to produce the global
21-cm signal over the redshift range $z = 6-40$ for each of 181 different
combinations of the astrophysical parameters spanning the allowed range. We
show that the expected signal fills a large parameter space, but with a fixed
general shape for the global 21-cm curve. Even with our wide selection of
models we still find clear correlations between the key features of the global
21-cm signal and underlying astrophysical properties of the high redshift
Universe, namely the Ly$\alpha$ intensity, the X-ray heating rate, and the
production rate of ionizing photons. These correlations can be used to directly
link future measurements of the global 21-cm signal to astrophysical quantities
in a mostly model-independent way. We identify additional correlations that can
be used as consistency checks.
</p>
<br><br><br><p>Dust grains are classically thought to form in the winds of asymptotic giant
branch (AGB) stars. However, there is increasing evidence today for dust
formation in supernovae (SNe). To establish the relative importance of these
two classes of stellar sources of dust, it is important to know the fraction of
freshly formed dust in SN ejecta that is able to survive the passage of the
reverse shock and be injected in the interstellar medium. We have developed a
new code (GRASH\_Rev) which follows the newly-formed dust evolution throughout
the supernova explosion until the merging of the forward shock with the
circumstellar ISM. We have considered four well studied SNe in the Milky Way
and Large Magellanic Cloud: SN1987A, CasA, the Crab Nebula, and N49. For all
the simulated models, we find good agreement with observations and estimate
that between 1 and 8$\%$ of the observed mass will survive, leading to a SN
dust production rate of $(3.9 \pm 3.7) \times 10^{-4}$ M$_{\odot}$yr$^{-1}$ in
the Milky Way. This value is one order of magnitude larger than the dust
production rate by AGB stars but insufficient to counterbalance the dust
destruction by SNe, therefore requiring dust accretion in the gas phase.
</p>
<br><br><br><p>Extensive photometric and spectroscopic observations are presented for SN
2014cx, a type IIP supernova (SN) exploding in the nearby galaxy NGC 337. The
observations are performed in optical and ultraviolet bands, covering from -20
to +400 days from the peak light. The stringent detection limit from
prediscovery images suggests that this supernova was actually detected within
about 1 day after explosion. Evolution of the very early-time light curve of SN
2014cx is similar to that predicted from a shock breakout and post-shock
cooling decline before reaching the optical peak. Our photometric observations
show that SN 2014cx has a plateau duration of ~ 100 days, an absolute V-band
magnitude of ~ -16.5 mag at t~50 days, and a nickel mass of 0.056+-0.008 Msun.
The spectral evolution of SN 2014cx resembles that of normal SNe IIP like SN
1999em and SN 2004et, except that it has a slightly higher expansion velocity
(~ 4200 km/s at 50 days). From the cooling curve of photospheric temperature,
we derive that the progenitor has a pre-explosion radius of ~ 640 Rsun,
consistent with those obtained from SNEC modeling (~ 620 Rsun) and
hydrodynamical modeling of the observables (~ 570 Rsun). Moreover, the
hydrodynamical simulations yield a total explosion energy of ~ 0.4*10e51 erg,
and an ejected mass of ~ 8 Msun. These results indicate that the immediate
progenitor of SN 2014cx is likely a red supergiant star with a mass of ~ 10
Msun.
</p>
<br><br><br><p>We study the main astrophysical properties of differentially rotating neutron
stars described as stationary and axisymmetric configurations of a moderately
stiff $\Gamma=2$ polytropic fluid. The high level of accuracy and of stability
of our relativistic multidomain pseudo-spectral code enables us to explore the
whole solution space for broad ranges of the degree of differential rotation,
but also of the stellar density and oblateness. Staying within an
astrophysicaly motivated range of rotation profiles, we investigate the
characteristics of neutron stars with maximal mass for all types of families of
differentially rotating relativistic objects identified in a previous article
Ansorg, Gondek-Rosinsla, Villain (2009). We find that the maximum mass depends
on both the degree of differential rotation and on the type of solution. It
turns out that the maximum allowed mass can be up to 4 times higher than what
it is for non-rotating stars with the same equation of state. Such values are
obtained for a modest degree of differential rotation but for one of the newly
discovered type of solutions. Since such configurations of stars are not that
extreme, this result may have important consequences for the gravitational wave
signal to expect from coalescing neutron star binaries or from some supernovae
events.
</p>
<br><br><br><p>In a Galactic core-collapse supernova (SN), axionlike particles (ALPs) could
be emitted via the Primakoff process and eventually convert into $\gamma$ rays
in the magnetic field of the Milky Way. From a data-driven sensitivity
estimate, we find that, for a SN exploding in our Galaxy, the Fermi Large Area
Telescope (LAT) would be able to explore the photon-ALP coupling down to
$g_{a\gamma} \simeq 2 \times 10^{-13}\,$GeV$^{-1}$ for an ALP mass $m_a
\lesssim 10^{-9}\,$eV. These values are out of reach of next generation
laboratory experiments. In this event, the Fermi LAT would probe large regions
of the ALP parameter space invoked to explain the anomalous transparency of the
Universe to $\gamma$ rays, stellar cooling anomalies, and cold dark matter. If
no $\gamma$-ray emission were to be detected, Fermi-LAT observations would
improve current bounds derived from SN1987A by more than one order of
magnitude.
</p>
<br><br><br><p>We have obtained new HI observations with the 100m Green Bank Telescope (GBT)
for a sample of 29 extremely metal-deficient star-forming Blue Compact Dwarf
(BCD) galaxies, selected from the Sloan Digital Sky Survey spectral data base
to be extremely metal-deficient (12+logO/H&lt;7.6). Neutral hydrogen was detected
in 28 galaxies, a 97% detection rate. Combining the HI data with SDSS optical
spectra for the BCD sample and adding complementary galaxy samples from the
literature to extend the metallicity and mass ranges, we have studied how the
HI content of a galaxy varies with various global galaxian properties. There is
a clear trend of increasing gas mass fraction with decreasing metallicity, mass
and luminosity. We obtain the relation M(HI)/L(g)~L(g)^{-0.3}, in agreement
with previous studies based on samples with a smaller luminosity range. The
median gas mass fraction f(gas) for the GBT sample is equal to 0.94 while the
mean gas mass fraction is 0.90+/-0.15, with a lower limit of ~0.65. The HI
depletion time is independent of metallicity, with a large scatter around the
median value of 3.4 Gyr. The ratio of the baryonic mass to the dynamical mass
of the metal-deficient BCDs varies from 0.05 to 0.80, with a median value of
~0.2. About 65% of the BCDs in our sample have an effective yield larger than
the true yield, implying that the neutral gas envelope in BCDs is more
metal-deficient by a factor of 1.5-20, as compared to the ionized gas.
</p>
<br><br><br><p>The central structure in three of the brightest unlensed z=3-4 submillimeter
galaxies are investigated through 0.015\" - 0.05\" (120 -- 360~pc) 860 micron
continuum images obtained using the Atacama Large Millimeter/submillimeter
Array (ALMA). The distribution in the central kpc in AzTEC1 and AzTEC8 are
extremely complex, and they are composed of multiple ~200 pc clumps. AzTEC4
consists of two sources that are separated by ~1.5 kpc, indicating a mid-stage
merger. The peak star formation rate densities in the central clumps are ~300 -
3000 Msun/yr/kpc^2, suggesting regions with extreme star formation near the
Eddington Limit. By comparing the flux obtained by ALMA and Submillimeter Array
(SMA), we find that 68-90% of the emission is extended (&gt; 1 kpc) in AzTEC 4 and
8. For AzTEC1, we identify at least 11 additional compact (~200 pc) clumps in
the extended 3 - 4 kpc region. Overall, the data presented here suggest that
the luminosity surface densities observed at &lt; 150 pc scales are roughly
similar to that observed in local ULIRGs, as in the eastern nucleus of Arp 220.
Between 10 to 30% of the 860 micron continuum is concentrated in clumpy
structures in the central kpc while the remaining flux is distributed over &gt; 1
kpc regions, some of which could also be clumpy. These sources can be explained
by a rapid inflow of gas such as a merger of gas-rich galaxies, surrounded by
extended and clumpy starbursts. However, the cold mode accretion model is not
ruled out.
</p>
<br><br><br><p>We explore the possibility of the formation of globular clusters under
ultraviolet (UV) background radiation. One-dimensional spherical symmetric
radiation hydrodynamics (RHD) simulations by Hasegawa et al. have demonstrated
that the collapse of low-mass (10^6-10^7 solar masses) gas clouds exposed to
intense UV radiation can lead to the formation of compact star clusters like
globular clusters (GCs) if gas clouds contract with supersonic infall
velocities. However, three-dimensional effects, such as the anisotropy of
background radiation and the inhomogeneity in gas clouds, have not been studied
so far. In this paper, we perform three-dimensional RHD simulations in a
semi-cosmological context, and reconsider the formation of compact star
clusters in strong UV radiation fields. As a result, we find that although
anisotropic radiation fields bring an elongated shadow of neutral gas, almost
spherical compact star clusters can be procreated from a \"supersonic infall\"
cloud, since photo-dissociating radiation suppresses the formation of hydrogen
molecules in the shadowed regions and the regions are compressed by UV heated
ambient gas. The properties of resultant star clusters match those of GCs. On
the other hand, in weak UV radiation fields, dark matter-dominated star
clusters with low stellar density form due to the self-shielding effect as well
as the positive feedback by ionizing photons. Thus, we conclude that the
\"supersonic infall\" under a strong UV background is a potential mechanism to
form GCs.
</p>
<br><br><br><p>PKS 1510-089 is one of the most variable blazars. Very high energy gamma ray
emission from this source was observed by H.E.S.S. during March-April 2009 and
by MAGIC from February 3 to April 3, 2012 quasi-simultaneously with
multi-wavelength flares. The spectral energy distributions of these flares have
been modeled earlier with the external Compton mechanism which depends on our
knowledge of the densities of the seed photons in the broad line region, the
dusty infrared torus or a hypothetical slow sheath surrounding the jet around
the radio core. Here we show that to explain the multi-wavelength data with
synchrotron emission of electrons and protons the minimum jet luminosity
required is $\sim 2\times10^{48}$ ergs/sec.
</p>
<br><br><br><p>Rayleigh-B\'enard convection in rotating spherical shells can be considered
as a simplified analogue of many astrophysical and geophysical fluid flows.
Here, we use three-dimensional direct numerical simulations to study this
physical process. We construct a dataset of more than 200 numerical models that
cover a broad parameter range with Ekman numbers spanning $3\times 10^{-7} \leq
E \leq 10^{-1}$, Rayleigh numbers within the range $10^3 &lt; Ra &lt; 2\times
10^{10}$ and a Prandtl number unity. We investigate the scaling behaviours of
both local (length scales, boundary layers) and global (Nusselt and Reynolds
numbers) properties across various physical regimes from onset of rotating
convection to weakly-rotating convection. Close to critical, the convective
flow is dominated by a triple force balance between viscosity, Coriolis force
and buoyancy. For larger supercriticalities, a subset of our numerical data
approaches the asymptotic diffusivity-free scaling of rotating convection
$Nu\sim Ra^{3/2}E^{2}$ in a narrow fraction of the parameter space delimited by
$6\,Ra_c \leq Ra \leq 0.4\,E^{-8/5}$. Using a decomposition of the viscous
dissipation rate into bulk and boundary layer contributions, we establish a
theoretical scaling of the flow velocity that accurately describes the
numerical data. In rapidly-rotating turbulent convection, the fluid bulk is
controlled by a triple force balance between Coriolis, inertia and buoyancy,
while the remaining fraction of the dissipation can be attributed to the
viscous friction in the Ekman layers. Beyond $Ra \simeq E^{-8/5}$, the
rotational constraint on the convective flow is gradually lost and the flow
properties vary to match the regime changes between rotation-dominated and
non-rotating convection. The quantity $Ra E^{12/7}$ provides an accurate
transition parameter to separate rotating and non-rotating convection.
</p>
<br><br><br><p>Charged particles in a magnetosphere are spontaneously attracted to a planet
while increasing their kinetic energy via inward diffusion process. A
constraint on particles' micro-scale adiabatic invariants restricts the class
of motions available to the system, giving rise to a proper frame on which
particle diffusion occurs. We investigate the inward diffusion process by
numerical simulation of particles on constrained phase space. The results
reveal the emergence of inhomogeneous density gradient and anisotropic heating,
which is consistent with spacecraft observations, experimental observations,
and the recently formulated diffusion model on the constrained phase space.
</p>
<br><br><br><p>We plan to measure the angular diameters of a sample of Penn State-Torun
Planet Search (PTPS) giant exoplanet host star candidates using the Navy
Precision Optical Interferometer. The radii of evolved giant stars obtained
using spectroscopy are usually ill-defined because of the method's indirect
nature and evolutionary model dependency. The star's radius is a critical
parameter used to calculate luminosity and mass, which are often not well known
for giant stars. Therefore, this problem also affects the orbital period, mass,
and surface temperature of the planet. Our interferometric observations will
significantly decrease the errors for these parameters. We present preliminary
results from NPOI observations of six stars in the PTPS sample.
</p>
<br><br><br><p>We present spectroscopic and interferometric measurements for a sample of
nine K giant stars. These targets are of particular interest because they are
slated for stellar oscillation observations. Our improved parameters will
directly translate into reduced errors in the final masses for these stars when
interferometric radii and asteroseismic densities are combined. Here we
determine each star's limb-darkened angular diameter, physical radius,
luminosity, bolometric flux, effective temperature, surface gravity,
metallicity, and mass. When we compare our interferometric and spectroscopic
results, we find no systematic offsets in the diameters and the values
generally agree within the errors. Our interferometric temperatures for seven
of the nine stars are hotter than those determined from spectroscopy with an
average difference of about 380 K.
</p>
<br><br><br><p>The Nobel prize in physics 2015 has been awarded \"... for the discovery of
neutrino oscillations which show that neutrinos have mass\". While
SuperKamiokande (SK), indeed, has discovered oscillations, SNO observed effect
of the adiabatic (almost non-oscillatory) flavor conversion of neutrinos in the
matter of the Sun. Oscillations are irrelevant for solar neutrinos apart from
small $\nu_e$ regeneration inside the Earth. Both oscillations and adiabatic
conversion do not imply masses uniquely and further studies were required to
show that non-zero neutrino masses are behind the SNO results. Phenomena of
oscillations (phase effect) and adiabatic conversion (the MSW effect driven by
the change of mixing in matter) are described in pedagogical way.
</p>
<br><br><br><p>Detection of periodicity in the broad-band non-thermal emission of blazars
has so far been proven to be elusive. However, there are a number of scenarios
which could lead to quasi-periodic variations in blazar light curves. For
example, orbital or thermal/viscous period of accreting matter around central
supermassive black holes could, in principle, be imprinted in the
multi-wavelength emission of small-scale blazar jets, carrying as such crucial
information about plasma conditions within the jet launching regions. In this
paper, we present the results of our time series analysis of $\sim 9.2$
year-long, and exceptionally well-sampled optical light curve of the BL Lac OJ
287. The study primarily uses the data from our own observations performed at
the Mt. Suhora and Krak\'ow Observatories in Poland, and at the Athens
Observatory in Greece. Additionally, SMARTS observations were used to fill in
some of the gaps in the data. The Lomb-Scargle Periodogram and the Weighted
Wavelet Z-transform methods were employed to search for the possible QPOs in
the resulting optical light curve of the source. Both the methods consistently
yielded possible quasi-periodic signal around the periods of $\sim 400$ and
$\sim 800$ days, the former one with a significance (over the underlying
colored noise) of $\geq 99\%$. A number of likely explanations for such are
discussed, with a preference given to a modulation of the jet production
efficiency by highly magnetized accretion disks. This supports the previous
findings and the interpretation reported recently in the literature for OJ 287
and other blazar sources.
</p>
<br><br><br><p>The origin of rings around giant planets remains elusive. Saturn's rings are
massive and made of 90-95% of water ice. In contrast, the much less massive
rings of Uranus and Neptune are dark and likely to have higher rock fraction.
Here we investigate, for the first time, the tidal disruption of a passing
object, including the subsequent formation of planetary rings. First, we
perform SPH simulations of the tidal destruction of big differentiated objects
($M_{\rm body}=10^{21-23}$) that experience close encounters with Saturn or
Uranus. We find that about $0.1-10$% of the mass of the passing body is
gravitationally captured around the planet. However, these fragments are
initially big chunks and have highly eccentric orbits around the planet. Then,
we perform N-body simulations including the planet's oblateness, starting with
data obtained from the SPH simulations. Our N-body simulations show that the
chunks are tidally destroyed during their next several orbits. Their individual
orbits then start to precess incoherently around the planet's equator, which
enhances their encounter velocities on longer-term evolution, resulting in more
destructive impacts. These collisions would damp their eccentricities resulting
in a progressive collapse of the debris cloud into a thin equatorial and
low-eccentricity ring. These high energy impacts are expected to be
catastrophic enough to produce small particles. Our numerical results also show
that the mass of formed rings is large enough to explain current rings
including inner regular satellites around Saturn and Uranus. In the case of
Uranus, a body can go deeper inside the planet's Roche limit resulting in a
more efficient capture of rocky material compared to Saturn's case in which
mostly ice is captured. Thus, our results can naturally explain the
compositional difference between the rings of Saturn, Uranus and Neptune.
</p>
<br><br><br><p>Using the ALMA archival data of both CO(6--5) line and 689 GHz continuum
emission towards the archetypical Seyfert galaxy, NGC 1068, we identified a
distinct continuum peak separated by 14 pc from the nuclear radio component S1
in projection. The continuum flux gives a gas mass of ~2x10^5 Msun and
bolometric luminosity of ~10^8 Lsun, leading to a star formation rate of ~0.1
Msun/yr. Subsequent analysis on the line data suggest that the gas has a size
of ~10 pc, yielding to mean H2 number density of ~10^5 cm^{-3}. We therefore
refer to the gas as \"massive dense gas cloud\": the gas density is high enough
to form a \"proto starcluster\" whose stellar mass of ~10^4 Msun. We found that
the gas stands a unique position between galactic and extraglactic clouds in
the diagrams of start formation rate (SFR) vs. gas mass proposed by Lada et al.
and surface density of gas vs. SFR density by Krumholz and McKee. All the
gaseous and star-formation properties may be understood in terms of the
turbulence-regulated star formation scenario. Since there are two stellar
populations with the ages of 300 Myr and 30 Myr in the 100 pc-scale
circumnulear region, we discuss that NGC1068 has experienced at least three
episodic star formation events with a tendency that the inner star-forming
region is the younger. Together with several lines of evidence that the
dynamics of the nuclear region is decoupled from that of the entire galactic
disk, we discuss that the gas inflow towards the nuclear region of NGC 1068 may
be driven by a past minor merger.
</p>
<br><br><br><p>In the present review we discuss the past and present status of the
interacting OB-type binary frequency. We critically examine the popular idea
that Be-stars and supergiant sgB[e] stars are binary evolutionary products. The
effects of rotation on stellar evolution in general, stellar population studies
in particular, and the link with binaries will be evaluated. Finally a
discussion is presented of massive double compact star binary mergers as
possible major sites of chemical enrichment of r-process elements and as the
origin of recent aLIGO GW events.
</p>
<br><br><br><p>Zonal flows in rapidly-rotating celestial objects such as the Sun, gas or ice
giants form in a variety of surface patterns and amplitudes. Whereas the
differential rotation on the Sun, Jupiter and Saturn features a super-rotating
equatorial region, the ice giants, Neptune and Uranus harbour an equatorial jet
slower than the planetary rotation. Global numerical models covering the
optically thick, deep-reaching and rapidly rotating convective envelopes of gas
giants reproduce successfully the prograde jet at the equator. In such models,
convective columns shaped by the dominant Coriolis force typically exhibit a
consistent prograde tilt. Hence angular momentum is pumped away from the
rotation axis via Reynolds stresses. Those models are found to be strongly
geostrophic, hence a modulation of the zonal flow structure along the axis of
rotation, e.g. introduced by persistent latitudinal temperature gradients,
seems of minor importance. Within our study we stimulate these thermal
gradients and the resulting ageostrophic flows by applying an axisymmetric and
equatorially symmetric outer boundary heat flux anomaly ($Y_{20}$) with
variable amplitude and sign. Such a forcing pattern mimics the thermal effect
of intense solar or stellar irradiation. Our results suggest that the
ageostrophic flows are linearly amplified with the forcing amplitude $q^\star$
leading to a more pronounced dimple of the equatorial jet (alike Jupiter). The
geostrophic flow contributions, however, are suppressed for weak $q^\star$, but
inverted and re-amplified once $q^\star$ exceeds a critical value. The inverse
geostrophic differential rotation is consistently maintained by now also
inversely tilted columns and reminiscent of zonal flow profiles observed for
the ice giants. Analysis of the main force balance and parameter studies
further foster these results.
</p>
<br><br><br><p>Supermassive black holes launching plasma jets at close to speed of light,
producing gamma-rays, have ubiquitously been found to be hosted by massive
elliptical galaxies. Since elliptical galaxies are generally believed to be
built through galaxy mergers, active galactic nuclei (AGN) launching
relativistic jets are associated to the latest stages of galaxy evolution. We
have discovered a pseudo-bulge morphology in the host galaxy of the gamma-ray
AGN PKS 2004-447. This is the first gamma-ray emitter radio loud AGN found to
be launched from a system where both black hole and host galaxy have been
actively growing via secular processes. This is evidence for an alternative
black hole-galaxy co-evolutionary path to develop powerful relativistic jets
that is not merger-driven.
</p>
<br><br><br><p>In this thesis we have analysed the time evolution of the photometric
precision achieved by the space-based exoplanet-hunting mission CoRoT during
its flight phase (2007-2012). This study of the noise level of CoRoT light
curves has been based on a previous paper by Aigrain et al. 2009, where they
found a gradual degradation of the photometric performance over time for the
first 14 months of data. Here we have analysed the anti-center runs IRa01
(2007), LRa01 (2008), LRa03 (2010) and LRa06 (2012). The two first runs were
studied by Aigrain as well, so we are able to compare our results. The two last
runs allowed us to evaluate the trend of photometric degradation over more than
5 years. We obtain low observational point-to-point noise, although a factor 3
bigger than the source photon noise. We find effects showing the ageing of the
CoRoT CCDs. On 2h time scales we notice a receding photometric performance,
with a noise increase of about 2.1 times across the four analysed runs,
corresponding to a 15% increase per year. Correlated noise becomes more
important than white uncorrelated noise for the two last studied runs, LRa03
and LRa06. The strongest degradation, however, occurs during the first year of
operations, with a 30% noise increase, opening up a two-ageing-timescales
scenario.
</p>
<br><br><br><p>In order to meet the theoretically achievable imaging performance,
calibration of modern radio interferometers is a mandatory challenge,
especially at low frequencies. In this perspective, we propose a novel parallel
iterative multi-wavelength calibration algorithm. The proposed algorithm
estimates the apparent directions of the calibration sources, the directional
and undirectional complex gains of the array elements and their noise powers,
with a reasonable computational complexity. Furthermore, the algorithm takes
into account the specific variation of the aforementioned parameter values
across wavelength. Realistic numerical simulations reveal that the proposed
scheme outperforms the mono-wavelength calibration scheme and approaches the
derived constrained Cram\'er-Rao bound even with the presence of
non-calibration sources at unknown directions, in a computationally efficient
manner.
</p>
<br><br><br><p>We conduct a linear stability calculation of an ideal Keplerian flow on which
a sinusoidal zonal flow is imposed. The analysis uses the shearing sheet model
and is carried out both in isothermal and adiabatic conditions, with and
without self-gravity (SG). In the non-SG regime a structure in the potential
vorticity (PV) leads to a non-axisymmetric Kelvin-Helmholtz (KH) instability;
in the short-wavelength limit its growth rate agrees with the incompressible
calculation by Lithwick (2007), which only considers perturbations elongated in
the streamwise direction. The instability's strength is analysed as a function
of the structure's properties, and zonal flows are found to be stable if their
wavelength is $\gtrsim 8H$, where $H$ is the disc's scale height, regardless of
the value of the adiabatic index $\gamma$. The non-axisymmetric KH instability
can operate in Rayleigh-stable conditions, and it therefore represents the
limiting factor to the structure's properties. Introducing SG triggers a second
non-axisymmetric instability, which is found to be located around a PV maximum,
while the KH instability is linked to a PV minimum, as expected. In the
adiabatic regime, the same gravitational instability is detected even when the
structure is present only in the entropy (not in the PV) and the instability
spreads to weaker SG conditions as the entropy structure's amplitude is
increased. This eventually yields a non-axisymmetric instability in the non-SG
regime, albeit of weak strength, localised around an entropy maximum.
</p>
<br><br><br><p>The performance of a wide-field adaptive optics system depends on input
design parameters. Here we investigate the performance of a multi-object
adaptive optics system design for the European Extremely Large Telescope, using
an end-to-end Monte-Carlo adaptive optics simulation tool, DASP, with relevance
for proposed instruments such as MOSAIC. We consider parameters such as the
number of laser guide stars, sodium layer depth, wavefront sensor pixel scale,
actuator pitch and natural guide star availability. We provide potential areas
where costs savings can be made, and investigate trade-offs between performance
and cost, and provide solutions that would enable such an instrument to be
built with currently available technology. Our key recommendations include a
trade-off for laser guide star wavefront sensor pixel scale of about 0.7
arcseconds per pixel, and a field of view of at least 7 arcseconds, that EMCCD
technology should be used for natural guide star wavefront sensors even if
reduced frame rate is necessary, and that sky coverage can be improved by a
slight reduction in natural guide star sub-aperture count without significantly
affecting tomographic performance. We find that adaptive optics correction can
be maintained across a wide field of view, up to 7 arcminutes in diameter. We
also recommend the use of at least 4 laser guide stars, and include
ground-layer and multi-object adaptive optics performance estimates.
</p>
<br><br><br><p>Context: Exoplanet science has made staggering progress in the last two
decades, due to the relentless exploration of new detection methods and
refinement of existing ones. Yet astrometry offers a unique and untapped
potential of discovery of habitable-zone low-mass planets around all the
solar-like stars of the solar neighborhood. To fulfill this goal, astrometry
must be paired with high precision calibration of the detector.
</p>
<p>Aims: We present a way to calibrate a detector for high accuracy astrometry.
An experimental testbed combining an astrometric simulator and an
interferometric calibration system is used to validate both the hardware needed
for the calibration and the signal processing methods. The objective is an
accuracy of 5e-6 pixel on the location of a Nyquist sampled polychromatic point
spread function.
</p>
<p>Methods: The interferometric calibration system produced modulated Young
fringes on the detector. The Young fringes were parametrized as products of
time and space dependent functions, based on various pixel parameters. The
minimization of func- tion parameters was done iteratively, until convergence
was obtained, revealing the pixel information needed for the calibration of
astrometric measurements.
</p>
<p>Results: The calibration system yielded the pixel positions to an accuracy
estimated at 4e-4 pixel. After including the pixel position information, an
astrometric accuracy of 6e-5 pixel was obtained, for a PSF motion over more
than five pixels. In the static mode (small jitter motion of less than 1e-3
pixel), a photon noise limited precision of 3e-5 pixel was reached.
</p>
<br><br><br><p>We performed the photometric analysis of M2 and M92 globular clusters in g
and r bands of SLOAN photometric system. We transformed these g and r bands
into BV bands of Johnson-Cousins photometric system and built the color
magnitude diagram (CMD). We estimated the age, and metallicity of both the
clusters, by fitting Padova isochrones of different age and metallicities onto
the CMD. We studied Einstein and de Sitter model, bench mark model, the
cosmological parameters by WMAP and Planck surveys. Finally, we compared
estimated age of globular clusters to the ages from the cosmological models and
cosmological parameters values of WMAP and Planck surveys.
</p>
<br><br><br><p>This is the second in a series of papers associated with cataclysmic
variables (CVs) and related objects, formed in a suite of simulations for
globular cluster evolution performed with the MOCCA Monte Carlo code. We study
the properties of our simulated CV populations throughout the entire cluster
evolution. We find that dynamics extends the range of binary CV progenitor
properties, causing CV formation from binary progenitors that would otherwise
not become CVs. The CV formation rate in our simulations can be separated into
two regimes: an initial burst ($\lesssim$ 1 Gyr) connected with the formation
of the most massive WDs, followed by a nearly constant formation rate. This
result holds for all models regardless of the adopted initial conditions, even
when most CVs form dynamically. Given the cluster age-dependence of CV
properties, we argue that direct comparisons to observed Galactic field CVs
could be misleading, since cluster CVs can be up to 4 times older than their
field counterparts. Our results also illustrate that, due mainly to unstable
mass transfer, some CVs that form in our simulations are destroyed before the
present-day. Finally, some field CVs might have originated from GCs, as found
in our simulations, although the fraction of such escapers should be small
relative to the entire Galactic field CV population.
</p>
<br><br><br><p>Based on recent $Herschel$ results, the ortho-to-para ratio (OPR) of NH$_2$
has been measured towards the following high-mass star-forming regions: W31C
(G10.6-0.4), W49N (G43.2-0.1), W51 (G49.5-0.4), and G34.3+0.1. The OPR at
thermal equilibrium ranges from the statistical limit of three at high
temperatures to infinity as the temperature tends toward zero, unlike the case
of H$_{2}$. Depending on the position observed along the lines-of-sight, the
OPR was found to lie either slightly below the high temperature limit of three
(in the range $2.2-2.9$) or above this limit ($\sim3.5$, $\gtrsim 4.2$, and
$\gtrsim 5.0$). In low temperature interstellar gas, where the H$_{2}$ is
para-enriched, our nearly pure gas-phase astrochemical models with nuclear-spin
chemistry can account for anomalously low observed NH$_2$-OPR values. We have
tentatively explained OPR values larger than three by assuming that spin
thermalization of NH$_2$ can proceed at least partially by H-atom exchange
collisions with atomic hydrogen, thus increasing the OPR with decreasing
temperature. In this paper, we present quasi-classical trajectory calculations
of the H-exchange reaction NH$_2$ + H, which show the reaction to proceed
without a barrier, confirming that the H-exchange will be efficient in the
temperature range of interest. With the inclusion of this process, our models
suggest both that OPR values below three arise in regions with temperatures
$\gtrsim20-25$~K, depending on time, and values above three but lower than the
thermal limit arise at still lower temperatures.
</p>
<br><br><br><p>Context: Gaps, cavities and rings in circumstellar disks are signposts of
disk evolution and planet-disk interactions. We follow the recent suggestion
that Herbig Ae/Be disks with a flared disk harbour a cavity, and investigate
the disk around HD~97048.
</p>
<p>Aims: We aim to resolve the 34$\pm$ 4 au central cavity predicted by Maaskant
et al. (2013) and to investigate the structure of the disk.
</p>
<p>Methods: We image the disk around HD~97048 using ALMA at 0.85~mm and 2.94~mm,
and ATCA (multiple frequencies) observations. Our observations also include the
12CO J=1-0, 12CO J=3-2 and HCO+ J=4-3 emission lines.
</p>
<p>Results: A central cavity in the disk around HD~97048 is resolved with a
40-46 au radius. Additional radial structure present in the surface brightness
profile can be accounted for either by an opacity gap at ~90 au or by an extra
emitting ring at ~150 au. The continuum emission tracing the dust in the disk
is detected out to 355 au. The 12CO J=3-2 disk is detected 2.4 times farther
out. The 12CO emission can be traced down to $\approx$ 10 au scales.
Non-Keplerian kinematics are detected inside the cavity via the HCO+ J=4-3
velocity map. The mm spectral index measured from ATCA observations suggests
that grain growth has occurred in the HD~97048 disk. Finally, we resolve a
highly inclined disk out to 150 au around the nearby 0.5~$M_{\odot}$ binary
ISO-ChaI 126.
</p>
<p>Conclusions: The data presented here reveal a cavity in the disk of HD 97048,
and prominent radial structure in the surface brightness. The cavity size
varies for different continuum frequencies and gas tracers. The gas inside the
cavity follows non-Keplerian kinematics seen in HCO+ emission. The variable
cavity size along with the kinematical signature suggests the presence of a
substellar companion or massive planet inside the cavity.
</p>
<br><br><br><p>Magnetic fields (B-fields) play a key role in the formation and evolution of
protoplanetary disks, but their properties are poorly understood due to the
lack of observational constraints. Using CanariCam at the 10.4-m Gran
Telescopio Canarias, we have mapped out the mid-infrared polarization of the
protoplanetary disk around the Herbig Ae star AB Aur. We detect ~0.44%
polarization at 10.3 micron from AB Aur's inner disk (r &lt; 80 AU), rising to
~1.4% at larger radii. Our simulations imply that the mid-infrared polarization
of the inner disk arises from dichroic emission of elongated particles aligned
in a disk B-field. The field is well ordered on a spatial scale commensurate
with our resolution (~50 AU), and we infer a poloidal shape tilted from the
rotational axis of the disk. The disk of AB Aur is optically thick at 10.3
micron, so polarimetry at this wavelength is probing the B-field near the disk
surface. Our observations therefore confirm that this layer, favored by some
theoretical studies for developing magneto-rotational instability and its
resultant viscosity, is indeed very likely to be magnetized. At radii beyond
~80 AU, the mid-infrared polarization results primarily from scattering by dust
grains with sizes up to ~1 micron, a size indicating both grain growth and,
probably, turbulent lofting of the particles from the disk mid-plane.
</p>
<br><br><br><p>Extragalactic radiosources have been classified in two classes,
Fanaroff-Riley I and II, which differ in morphology and radio power. Strongly
emitting sources belong to the edge brightened FR II class while the weak ones
to the edge darkened FR I class. The origin of this dichotomy is not yet fully
understood. Numerical simulations are successful in generating FR~II
morphologies but they fail to reproduce the diffuse structure of FR Is.
</p>
<p>By means of hydro-dynamical 3D simulations of supersonic jets, we investigate
how the displayed morphologies depend on the jet parameters. Bow shocks and
Mach disks at the jet's head, likely responsible for the presence of hot spots
in the FR II sources, disappear for a jet kinetic power less than 10^43 erg/s.
This threshold compares favorably with the luminosity at which the FR~I/FR~II
transition is observed.
</p>
<p>The problem is addressed by numerical means carrying out three-dimensional HD
simulations of supersonic jets that propagate in a non homogeneous medium with
the ambient temperature that increases with distance from the jet origin,
maintaining pressure constant.
</p>
<p>In the lower power sources, the jet energy, instead of being deposited at the
terminal shock, is gradually dissipated by the turbulence. The jets spreads out
while propagating, it smoothly decelerates while mixing with the ambient medium
and produces the plumes characteristic of FR I objects.
</p>
<p>Three-dimensionality is an essential ingredient to explore the FR I
evolution, because the properties of turbulence in two and three dimensions are
very different, since in two dimensions there is no energy cascade to small
scales and two-dimensional simulations with the same parameters, lead to FRII
like behavior.
</p>
<br><br><br><p>The muon content of extensive air showers is an observable sensitive to the
primary composition and to the hadronic interaction properties. The Pierre
Auger Observatory uses water-Cherenkov detectors to measure particle densities
at the ground and therefore is sensitive to the muon content of air showers. We
present here a method which allows us to estimate the muon production depths by
exploiting the measurement of the muon arrival times at the ground recorded
with the Surface Detector of the Pierre Auger Observatory. The analysis is
performed in a large range of zenith angles, thanks to the capability of
estimating and subtracting the electromagnetic component, and for energies
between $10^{19.2}$ and $10^{20}$ eV.
</p>
<br><br><br><p>A 3.56-hour white dwarf (WD) - M dwarf (MD) close binary system, AR Scorpii,
was recently reported to show pulsating emission in radio, IR, optical, and UV,
with a 1.97-minute period, which suggests the existence of a WD with a rotation
period of 1.95 minutes. We propose a model to explain the temporal and spectral
characteristics of the system. The WD is a nearly perpendicular rotator, with
both open field line beams sweeping the MD stellar wind periodically. A bow
shock propagating into the stellar wind accelerates electrons in the wind.
Synchrotron radiation of these shocked electrons can naturally account for the
broad-band (from radio to X-rays) spectral energy distribution of the system.
</p>
<br><br><br><p>We analyze interstellar absorption features in the full UV spectrum of the
nearby (d = 24 pc) B8 IVn star alpha Leo (Regulus) obtained at high resolution
and high S/N by the HST ASTRAL Treasury program. We derive column densities for
many key atomic species and interpret their partial ionizations. The gas in
front of alpha Leo exhibits two absorption components, one of which coincides
in velocity with the local interstellar cloud (LIC) that surrounds the Sun. The
second, smaller, component is shifted by +5.6 km/s relative to the main
component, in agreement with results for other lines of sight in this region of
the sky. The excitation of the C II fine-structure levels and the ratio of Mg I
to Mg II reveal a temperature T = 6500 (+750,-600)K and electron density n(e) =
0.11 (+0.025,-0.03) cm^-3. Our investigation of the ionization balance of all
the available species indicates that about 1/3 of the hydrogen atoms are
ionized and that metals are significantly depleted onto grains. We infer that
N(H I) = 1.9 (+0.9,-0.6) X 10^{18} cm^-2, which indicates that this partly
neutral gas occupies only 2 to 8 pc (about 13%) of the space toward the star,
with the remaining volume presumably being filled with a hot gas that emits
soft X-rays. We do not detect any absorption features from the highly ionized
species that could be produced in an interface between the warm medium and the
surrounding hot gas. Finally, the radial velocity of the LIC agrees with that
of the Local Leo Cold Cloud, indicating that they may be physically related.
</p>
<br><br><br><p>The recent development of brand-new observational techniques and theoretical
models greatly advanced the exoplanet research field. Despite significant
achievements, which have allowed to detect thousands extrasolar systems, a
comprehensive understanding of planetary formation and evolution mechanisms is
still desired. One relevant limitation is given by the accuracy in the
measurements of planet-host star ages. The star GJ 504 has been found to host a
sub-stellar companion whose nature is strongly debated. There has been a recent
difference of opinion in the literature due to the uncertainty on the age of
the system: a young age of $\sim$ 160 Myr would imply a giant planet as a
companion, but a recent revision pointing to a solar age ($\sim$ 4 Gyr)
suggests instead a brown dwarf. With the aim of shedding light on this debated
topic, we have carried out a high-resolution spectroscopic study of GJ 504 in
order to derive stellar parameters, metallicity and abundances of both light
and heavy elements, providing a full chemical characterisation. The main
objective is to infer clues on the evolutionary stage (hence the age) of this
system. We performed a strictly differential (line-by-line) analysis of GJ 504
with respect to two reference stars, that is the planet-host dwarf $\iota$ Hor
and the sub-giant HIP 84827. The former is crucial in this context because its
stellar parameters (hence the evolutionary stage) is well constrained from
asteroseismic observations. Regardless of the zero point offsets, our
differential approach allows us to put tight constraints on the age of GJ 504
with respect to $\iota$ Hor, minimising the internal uncertainties. We found
that the surface gravity of GJ 504 is 0.2 $\pm$ 0.07 dex lower than that of the
main-sequence star $\iota$ Hor, suggesting a past turn-off evolution for our
target [...]
</p>
<br><br><br><p>We use a spherical model and an extended excursion set formalism with
drifting diffusive barriers to predict the abundance of cosmic voids in the
context of general relativity as well as f(R) and symmetron models of modified
gravity. We detect spherical voids from a suite of N-body simulations of these
gravity theories and compare the measured void abundance to theory predictions.
We find that our model correctly describes the abundance of both dark matter
and galaxy voids, providing a better fit than previous proposals in the
literature based on static barriers. We use the simulation abundance results to
fit for the abundance model free parameters as a function of modified gravity
parameters, and show that counts of dark matter voids can provide interesting
constraints on modified gravity. For galaxy voids, more closely related to
optical observations, we find that constraining modified gravity from void
abundance alone may be significantly more challenging. In the context of
current and upcoming galaxy surveys, the combination of void and halo
statistics including their abundances, profiles and correlations should be
effective in distinguishing modified gravity models that display different
screening mechanisms.
</p>
<br><br><br><p>We present the measurement of the projected and redshift space 2-point
correlation function (2pcf) of the new catalog of Chandra COSMOS-Legacy AGN at
2.9$\leq$z$\leq$5.5 ($\langle L_{bol} \rangle \sim$10$^{46}$ erg/s) using the
generalized clustering estimator based on phot-z probability distribution
functions (Pdfs) in addition to any available spec-z. We model the projected
2pcf estimated using $\pi_{max}$ = 200 h$^{-1}$ Mpc with the 2-halo term and we
derive a bias at z$\sim$3.4 equal to b = 6.6$^{+0.60}_{-0.55}$, which
corresponds to a typical mass of the hosting halos of log M$_h$ =
12.83$^{+0.12}_{-0.11}$ h$^{-1}$ M$_{\odot}$. A similar bias is derived using
the redshift-space 2pcf, modelled including the typical phot-z error $\sigma_z$
= 0.052 of our sample at z$\geq$2.9. Once we integrate the projected 2pcf up to
$\pi_{max}$ = 200 h$^{-1}$ Mpc, the bias of XMM and \textit{Chandra} COSMOS at
z=2.8 used in Allevato et al. (2014) is consistent with our results at higher
redshift. The results suggest only a slight increase of the bias factor of
COSMOS AGN at z$\gtrsim$3 with the typical hosting halo mass of moderate
luminosity AGN almost constant with redshift and equal to logM$_h$ =
12.92$^{+0.13}_{-0.18}$ at z=2.8 and log M$_h$ = 12.83$^{+0.12}_{-0.11}$ at
z$\sim$3.4, respectively. The observed redshift evolution of the bias of COSMOS
AGN implies that moderate luminosity AGN still inhabit group-sized halos at
z$\gtrsim$3, but slightly less massive than observed in different independent
studies using X-ray AGN at z$\leq2$.
</p>
<br><br><br><p>We study the circularization of tidally disrupted stars on bound orbits
around spinning supermassive black holes by performing three-dimensional
smoothed particle hydrodynamic simulations with Post-Newtonian corrections. Our
simulations reveal that debris circularization depends sensitively on the
efficiency of radiative cooling. There are two stages in debris circularization
if radiative cooling is inefficient: first, the stellar debris streams
self-intersect due to relativistic apsidal precession; shocks at the
intersection points thermalize orbital energy and the debris forms a
geometrically thick, ring-like structure around the black hole. The ring
rapidly spreads via viscous diffusion, leading to the formation of a
geometrically thick accretion disk. In contrast, if radiative cooling is
efficient, the stellar debris circularizes due to self-intersection shocks and
forms a geometrically thin ring-like structure. In this case, the dissipated
energy can be emitted during debris circularization as a precursor to the
subsequent tidal disruption flare. The possible radiated energy is up to
~2*10^{52} erg for a 1 Msun star orbiting a 10^6 Msun black hole. We also find
that a retrograde (prograde) black hole spin causes the shock-induced
circularization timescale to be shorter (longer) than that of a non-spinning
black hole in both cooling cases. The circularization timescale is remarkably
long in the radiatively efficient cooling case, and is also sensitive to black
hole spin. Specifically, Lense-Thirring torques cause dynamically important
nodal precession, which significantly delays debris circularization. On the
other hand, nodal precession is too slow to produce observable signatures in
the radiatively inefficient case. We also discuss the relationship between our
simulations and the parabolic TDEs that are characteristic of most stellar
tidal disruptions.
</p>
<br><br><br><p>We introduce a new project to understand helium reionization using fully
coupled $N$-body, hydrodynamics, and radiative transfer simulations. This
project aims to capture correctly the thermal history of the intergalactic
medium (IGM) as a result of reionization and make predictions about the
Lyman-$\alpha$ forest and baryon temperature-density relation. The dominant
sources of radiation for this transition are quasars, so modeling the source
population accurately is very important for making reliable predictions. In
this first paper, we present a new method for populating dark matter halos with
quasars. Our set of quasar models include two different light curves, a
lightbulb (simple on/off) and symmetric exponential model, and
luminosity-dependent quasar lifetimes. Our method self-consistently reproduces
an input quasar luminosity function (QLF) given a halo catalog from an $N$-body
simulation, and propagates quasars through the merger history of halo hosts.
After calibrating quasar clustering using measurements from BOSS, we find that
the characteristic mass of quasar hosts is $M_h \sim 2.5 \times 10^{12}
M_\odot$ $h^{-1}$ for the lightbulb model, and $M_h \sim 2.3 \times 10^{12}
M_\odot$ $h^{-1}$ for the exponential model. In the exponential model, the peak
quasar luminosity for a given halo mass is larger than that in the lightbulb
model, typically by a factor of 1.5-2. The effective lifetime for quasars in
the lightbulb model is 59 Myr, and in the exponential case, the effective time
constant is about 15 Myr. We include semi-analytic calculations of helium
reionization, and discuss how to include these quasars as sources of ionizing
radiation for full hydrodynamics with radiative transfer simulations in order
to study helium reionization.
</p>
<br><br><br><p>A key component of characterizing multi-planet exosystems is testing the
orbital stability based on the observed properties. Such characterization not
only tests the validity of how observations are interpreted but can also place
additional constraints upon the properties of the detected planets. The Kepler
mission has identified hundreds of multi-planet systems but there are a few
that have additional non-transiting planets and also have well characterized
host stars. Kepler-68 is one such system for which we are able to provide a
detailed study of the orbital dynamics. We use the stellar parameters to
calculate the extent of the Habitable Zone for this system, showing that the
outer planet lies within that region. We use N-body integrations to study the
orbital stability of the system, in particular placing an orbital inclination
constraint on the outer planet of i &gt; 5 degrees. Finally, we present the
results of an exhaustive stability simulation that investigates possible
locations of stable orbits for an Earth-mass planet. We show that there are
several islands of stability within the Habitable Zone that could harbor such a
planet, most particularly at the 2:3 mean motion resonance with the outer
planet.
</p>
<br><br><br><p>Over the past decades, the role of torsion in gravity has been extensively
investigated along the main direction of bringing gravity closer to its gauge
formulation and incorporating spin in a geometric description. Here we review
various torsional constructions, from teleparallel, to Einstein-Cartan, and
metric-affine gauge theories, resulting in extending torsional gravity in the
paradigm of f(T) gravity, where f(T) is an arbitrary function of the torsion
scalar. Based on this theory, we further review the corresponding cosmological
and astrophysical applications. In particular, we study cosmological solutions
arising from f(T) gravity, both at the background and perturbation levels, in
different eras along the cosmic expansion. The f(T) gravity construction can
provide a theoretical interpretation of the late-time universe acceleration,
and it can easily accommodate with the regular thermal expanding history
including the radiation and cold dark matter dominated phases. Furthermore, if
one traces back to very early times, a sufficiently long period of inflation
can be achieved and hence can be investigated by cosmic microwave background
observations, or alternatively, the Big Bang singularity can be avoided due to
the appearance of non-singular bounces. Various observational constraints,
especially the bounds coming from the large-scale structure data in the case of
f(T) cosmology, as well as the behavior of gravitational waves, are described
in detail. Moreover, the spherically symmetric and black hole solutions of the
theory are reviewed. Additionally, we discuss various extensions of the f(T)
paradigm. Finally, we consider the relation with other modified gravitational
theories, such as those based on curvature, like f(R) gravity, trying to
enlighten the subject of which formulation might be more suitable for
quantization ventures and cosmological applications.
</p>
<br><br><br><p>There is mounting evidence for the binary nature of the progenitors of
gamma-ray bursts (GRBs). For a long GRB, the induced gravitational collapse
(IGC) paradigm proposes as progenitor, or \"in-state\", a tight binary system
composed of a carbon-oxygen core (CO$_{core}$) undergoing a supernova (SN)
explosion which triggers hypercritical accretion onto a neutron star (NS)
companion. For a short GRB, a NS-NS merger is traditionally adopted as the
progenitor. We divide long and short GRBs into two sub-classes, depending on
whether or not a black hole (BH) is formed in the merger or in the
hypercritical accretion process exceeding the NS critical mass. For long
bursts, when no BH is formed we have the sub-class of X-ray flashes (XRFs),
with isotropic energy $E_{iso}\lesssim10^{52}$ erg and rest-frame spectral peak
energy $E_{p,i}\lesssim200$ keV. When a BH is formed we have the sub-class of
binary-driven hypernovae (BdHNe), with $E_{iso}\gtrsim10^{52}$ erg and
$E_{p,i}\gtrsim200$ keV. In analogy, short bursts are similarly divided into
two sub-classes. When no BH is formed, short gamma-ray flashes (S-GRFs) occur,
with $E_{iso}\lesssim10^{52}$ erg and $E_{p,i}\lesssim2$ MeV. When a BH is
formed, the authentic short GRBs (S-GRBs) occur, with $E_{iso}\gtrsim10^{52}$
erg and $E_{p,i}\gtrsim2$ MeV. We give examples and observational signatures of
these four sub-classes and their rate of occurrence. From their respective
rates it is possible that \"in-states\" of S-GRFs and S-GRBs originate from the
\"out-states\" of XRFs. We indicate two additional progenitor systems: white
dwarf-NS and BH-NS. These systems have hybrid features between long and short
bursts. In the case of S-GRBs and BdHNe evidence is given of the coincidence of
the onset of the high energy GeV emission with the birth of a Kerr BH.
</p>
<br><br><br><p>We present results of a set of three-dimensional, general relativistic
radiation magnetohydrodynamics simulations of thin accretion discs around a
non-rotating black hole to test their thermal stability. We consider two cases,
one that is initially radiation pressure dominated and expected to be thermally
unstable and another that is initially gas-pressure dominated and expected to
remain stable. Indeed, we find that cooling dominates over heating in the
radiation pressure dominated model, causing the disc to collapse vertically on
roughly the local cooling timescale. We also find that heating and cooling
within the disc have a different dependence on the mid-plane pressure, a
prerequisite of thermal instability. Comparison of our data with the relevant
thin-disc thermal equilibrium curve suggests that our disc may be headed for
the thermally stable, gas-pressure-dominated branch. However, because the disc
collapses to the point that we are no longer able to resolve it, we had to
terminate the simulation. On the other hand, the gas pressure dominated model,
which was run for twice as long as the radiation pressure dominated one,
remains stable, with heating and cooling roughly in balance. Finally, the
radiation pressure dominated simulation shows some evidence of viscous
instability. The strongest evidence is in plots of surface density, which show
the disc breaking up into rings.
</p>
<br><br><br><p>Bayesian graphical models are an efficient tool for modelling complex data
and derive self-consistent expressions of the posterior distribution of model
parameters. We apply Bayesian graphs to perform statistical analyses of Type Ia
supernova (SN Ia) luminosity distance measurements from the joint light-curve
analysis (JLA) data set. In contrast to the $\chi^2$ approach used in previous
studies, the Bayesian inference allows us to fully account for the
standard-candle parameter dependence of the data covariance matrix. Comparing
with $\chi^2$ analysis results, we find a systematic offset of the marginal
model parameter bounds. We demonstrate that the bias is statistically
significant in the case of the SN Ia standardization parameters with a maximal
6 $\sigma$ shift of the SN light-curve colour correction. In addition, we find
that the evidence for a host galaxy correction is now only 2.4 $\sigma$.
Systematic offsets on the cosmological parameters remain small, but may
increase by combining constraints from complementary cosmological probes. The
bias of the $\chi^2$ analysis is due to neglecting the parameter-dependent
log-determinant of the data covariance, which gives more statistical weight to
larger values of the standardization parameters. We find a similar effect on
compressed distance modulus data. To this end, we implement a fully consistent
compression method of the JLA data set that uses a Gaussian approximation of
the posterior distribution for fast generation of compressed data. Overall, the
results of our analysis emphasize the need for a fully consistent Bayesian
statistical approach in the analysis of future large SN Ia data sets.
</p>
<br><br><br><p>Aerosol particles experience significant photophoretic forces at low
pressure. Previous work assumed the average particle temperature to be very
close to the gas temperature. This might not always be the case. If the
particle temperature or the thermal radiation field differs significantly from
the gas temperature (optically thin gases), given approximations overestimate
the photophoretic force by an order of magnitude on average with maximum errors
up to more than three magnitudes. We therefore developed a new general
approximation which on average only differs by 1 % from the true value.
</p>
<br><br><br><p>Extremely metal poor stars have been the focus of much recent attention owing
to the expectation that their chemical abundances can shed light on the metal
and dust yields of the earliest supernovae. We present our most realistic
simulation to date of the astrophysical pathway to the first metal enriched
stars. We simulate the radiative and supernova hydrodynamic feedback of a
$60\,M_\odot$ Population III star starting from cosmological initial conditions
realizing Gaussian density fluctuations. We follow the gravitational
hydrodynamics of the supernova remnant at high spatial resolution through its
freely-expanding, adiabatic, and radiative phases, until gas, now
metal-enriched, has resumed runaway gravitational collapse. Our findings are
surprising: while the Population III progenitor exploded with a low energy of
$10^{51}\,\text{erg}$ and injected an ample metal mass of $6\,M_\odot$, the
first cloud to collapse after the supernova explosion is a dense surviving
primordial cloud on which the supernova blastwave deposited metals only
superficially, in a thin, unresolved layer. The first metal-enriched stars can
form at a very low metallicity, of only $2-5\times10^{-4}\,Z_\odot$, and can
inherit the parent cloud's highly elliptical, radially extended orbit in the
dark matter gravitational potential.
</p>
<br><br><br><p>Saturn formed beyond the snow line in the primordial solar nebula that made
it possible for it to accrete a large mass. Disk instability and core accretion
models have been proposed for Saturn?s formation, but core accretion is favored
on the basis of its volatile abundances, internal structure, hydrodynamic
models, chemical characteristics of protoplanetary disk, etc. The observed
frequency, properties and models of exoplanets provide additional supporting
evidence for core accretion. The heavy elements with mass greater than 4He make
up the core of Saturn, but are presently poorly constrained, except for carbon.
The C/H ratio is super-solar, and twice that in Jupiter. The enrichment of
carbon and other heavy elements in Saturn and Jupiter requires special delivery
mechanisms for volatiles to these planets. In this chapter we will review our
current understanding of the origin and evolution of Saturn and its atmosphere,
using a multi-faceted approach that combines diverse sets of observations on
volatile composition and abundances, relevant properties of the moons and
rings, comparison with the other gas giant planet, Jupiter, analogies to the
extrasolar giant planets, as well as pertinent theoretical models.
</p>
<br><br><br><p>We report the results of a Dark Energy Camera (DECam) optical follow-up of
the gravitational wave (GW) event GW151226, discovered by the Advanced LIGO
detectors. Our observations cover 28.8 deg$^2$ of the localization region in
the $i$ and $z$ bands (containing 3% of the BAYESTAR localization probability),
starting 10 hours after the event was announced and spanning four epochs at
$2-24$ days after the GW detection. We achieve $5\sigma$ point-source limiting
magnitudes of $i\approx21.7$ and $z\approx21.5$, with a scatter of $0.4$ mag,
in our difference images. Given the two day delay, we search this area for a
rapidly declining optical counterpart with $\gtrsim 3\sigma$ significance
steady decline between the first and final observations. We recover four
sources that pass our selection criteria, of which three are cataloged AGN. The
fourth source is offset by $5.8$ arcsec from the center of a galaxy at a
distance of 187 Mpc, exhibits a rapid decline by $0.5$ mag over $4$ days, and
has a red color of $i-z\approx 0.3$ mag. These properties roughly match the
expectations for a kilonova. However, this source was detected several times,
starting $94$ days prior to GW151226, in the Pan-STARRS Survey for Transients
(dubbed as PS15cdi) and is therefore unrelated to the GW event. Given its
long-term behavior, PS15cdi is likely a Type IIP supernova that transitioned
out of its plateau phase during our observations, mimicking a kilonova-like
behavior. We comment on the implications of this detection for contamination in
future optical follow-up observations.
</p>
<br><br><br><p>We analyze the simulation of Dirac neutrino oscillations using quantum walks,
both in vacuum and in matter. We show that this simulation, in the continuum
limit, reproduces a set of coupled Dirac equations that describe neutrino
flavor oscillations, and we make use of this to establish a connection with
neutrino phenomenology, thus allowing to fix the parameters of the simulation
for a given neutrino experiment. We also analyze how matter effects for
neutrino propagation can be simulated in the quantum walk. In this way,
important features, such as the MSW effect, can be incorporated. Thus, the
simulation of neutrino oscillations with the help of quantum walks might be
useful to explore these effects in extreme conditions, such as the solar
interior or supernovae, in a complementary way to existing experiments.
</p>
<br><br><br><p>We present a scenario where a $Z_2$-symmetric scalar field $\phi$ first
drives cosmic inflation, then reheats the Universe but remains
out-of-equilibrium itself, and finally comprises the observed dark matter
abundance, produced by particle decays \`{a} la freeze-in mechanism. We work
model-independently without specifying the interactions of the scalar field
besides its self-interaction coupling, $\lambda\phi^4$, non-minimal coupling to
gravity, $\xi\phi^2R$, and coupling to another scalar field, $g\phi^2\sigma^2$.
We find the scalar field $\phi$ serves both as the inflaton and a dark matter
candidate if $10^{-9}\lesssim \lambda\lesssim g\lesssim 10^{-7}$ and $3
\rm{keV} \lesssim m_{\rm \phi}\lesssim 85 \rm{MeV}$ for $\xi=\mathcal{O}(1)$.
Such a small value of the non-minimal coupling is also found to be of the right
magnitude to produce the observed curvature perturbation amplitude within the
scenario. We also discuss how the model may be distinguished from other
inflationary models of the same type by the next generation CMB satellites.
</p>
<br><br><br><p>We present results of an investigation into the formation of nitrogen-bearing
molecules in the atmosphere of Titan. We extend a previous model (Li et al.
2015, 2016) to cover the region below the tropopause, so the new model treats
the atmosphere from Titan's surface to an altitude of 1500 km. We consider the
effects of condensation and sublimation using a continuous, numerically stable
method. This is coupled with parameterized treatments of the sedimentation of
the aerosols and their condensates, and the formation of haze particles. These
processes affect the abundances of heavier species such as the nitrogen-bearing
molecules, but have less effect on the abundances of lighter molecules. Removal
of molecules to form aerosols also plays a role in determining the mixing
ratios, in particular of HNC, HC3N and HCN. We find good agreement with the
recently detected mixing ratios of C2H5CN, with condensation playing an
important role in determining the abundance of this molecule below 500 km. Of
particular interest is the chemistry of acrylonitrile (C2H3CN) which has been
suggested by Stevenson et al. (2015) as a molecule that could form biological
membranes in an oxygen-deficient environment. With the inclusion of haze
formation we find good agreement of our model predictions of acrylonitrile with
the available observations.
</p>
<br><br><br><p>We present an analysis of a deep (1$\sigma$=13 $\mu$Jy) cosmological 1.2-mm
continuum map based on ASPECS, the ALMA Spectroscopic Survey in the Hubble
Ultra Deep Field. In the 1 arcmin$^2$ covered by ASPECS we detect nine sources
at $&gt;3.5\sigma$ significance at 1.2-mm. Our ALMA--selected sample has a median
redshift of $z=1.6\pm0.4$, with only one galaxy detected at z$&gt;$2 within the
survey area. This value is significantly lower than that found in millimeter
samples selected at a higher flux density cut-off and similar frequencies. Most
galaxies have specific star formation rates similar to that of main sequence
galaxies at the same epoch, and we find median values of stellar mass and star
formation rates of $4.0\times10^{10}\ M_\odot$ and $\sim40~M_\odot$ yr$^{-1}$,
respectively. Using the dust emission as a tracer for the ISM mass, we derive
depletion times that are typically longer than 300 Myr, and we find molecular
gas fractions ranging from $\sim$0.1 to 1.0. As noted by previous studies,
these values are lower than using CO--based ISM estimates by a factor $\sim$2.
The 1\,mm number counts (corrected for fidelity and completeness) are in
agreement with previous studies that were typically restricted to brighter
sources. With our individual detections only, we recover $55\pm4\%$ of the
extragalactic background light (EBL) at 1.2 mm measured by the Planck
satellite, and we recover $80\pm7\%$ of this EBL if we include the bright end
of the number counts and additional detections from stacking. The stacked
contribution is dominated by galaxies at $z\sim1-2$, with stellar masses of
(1-3)$\times$10$^{10}$ M$_\odot$. For the first time, we are able to
characterize the population of galaxies that dominate the EBL at 1.2 mm.
</p>
<br><br><br><p>We report 13 high-precision light curves of eight transits of the exoplanet
WASP-52b, obtained by using four medium-class telescopes, through different
filters, and adopting the defocussing technique. One transit was recorded
simultaneously from two different observatories and another one from the same
site but with two different instruments, including a multi-band camera.
Anomalies were clearly detected in five light curves and modelled as starspots
occulted by the planet during the transit events. We fitted the clean light
curves with the jktebop code, and those with the anomalies with the prism+gemc
codes in order to simultaneously model the photometric parameters of the
transits and the position, size and contrast of each starspot. We used these
new light curves and some from the literature to revise the physical properties
of the WASP-52 system. Starspots with similar characteristics were detected in
four transits over a period of 43 days. In the hypothesis that we are dealing
with the same starspot, periodically occulted by the transiting planet, we
estimated the projected orbital obliquity of WASP-52b to be lambda = 3.8 \pm
8.4 degree. We also determined the true orbital obliquity, psi = 20 \pm 50
degree, which is, although very uncertain, the first measurement of psi purely
from starspot crossings. We finally assembled an optical transmission spectrum
of the planet and searched for variations of its radius as a function of
wavelength. Our analysis suggests a flat transmission spectrum within the
experimental uncertainties.
</p>
<br><br><br><p>ALMA observations in the CO 1 - 0 line of the interacting galaxies IC 2163
and NGC 2207 at 2\" x 1.5\" resolution reveal how the encounter drives gas to
pile up in narrow, ~ 1 kpc wide, \"eyelids\" in IC 2163. IC 2163 and NGC 2207 are
involved in a grazing encounter, which has led to development in IC 2163 of an
eye-shaped (ocular) structure at mid-radius and two tidal arms. The CO data
show that there are large velocity gradients across the width of each eyelid,
with a mixture of radial and azimuthal streaming of gas at the outer edge of
the eyelid relative to its inner edge. The sense of the radial streaming in the
eyelids is consistent with the idea that gas from the outer part of IC 2163
flows inward until its radial streaming slows down abruptly and the gas piles
up in the eyelids. The radial compression at the eyelids causes an increase in
the gas column density by direct radial impact and also leads to a high rate of
shear. We find a strong correlation between the molecular column densities and
the magnitude of dv/dR across the width of the eyelid at fixed values of
azimuth. Substantial portions of the eyelids have high velocity dispersion in
CO, indicative of elevated turbulence there.
</p>
<br><br><br><p>Axion stars, gravitationally bound states of low-energy axion particles, have
a maximum mass allowed by gravitational stability. Weakly bound states
obtaining this maximum mass have sufficiently large radii such that they are
dilute, and as a result, they are well described by a leading-order expansion
of the axion potential. Heavier states are susceptible to gravitational
collapse. Inclusion of higher-order interactions, present in the full
potential, can give qualitatively different results in the analysis of
collapsing heavy states, as compared to the leading-order expansion. In this
work, we find that collapsing axion stars are stabilized by repulsive
interactions present in the full potential, providing evidence that such
objects do not form black holes. These dense configurations, which are the
endpoints of collapse, have high binding energy, and as a result, decay through
number changing $3\,a\rightarrow a$ interactions with an extremely short
lifetime.
</p>
<br><br><br><p>The presence of dark matter in the universe is nowadays supported by a
substantial set of astronomical and cosmological observations. A large amount
of dark matter is expected in the Galactic Center (GC) region. Thanks also to
its proximity, it is one of the best targets to look for dark matter particle
self-annihilation into very high energy gamma-rays. We perform a search for
annihilating dark matter in the central 300 parsecs around the GC with the
H.E.S.S. array of ground-based Cherenkov telescopes. Using the full H.E.S.S.- I
dataset (2004-2014) of GC observations, new constraints are derived on the
velocity-weighted annihilation cross section $\langle \sigma v \rangle$ with a
2D likelihood method using spectral and spatial morphologies of the DM signal
compared to background. These constraints are the strongest obtained so far in
the TeV mass range and improve the previous constraints by a factor of 5.
Considering an Einasto profile, the constraints reach $\langle \sigma v
\rangle$ values of $6\times 10^{-26}$cm$^{3}$s$^{-1}$ for a DM particle mass of
1.5 TeV annihilation into W$^+$W$^-$ pairs. In the $\tau^+\tau^-$ channel, the
constraints probe the natural scale for thermal relic cross section for DM
particles of masses between 400 GeV and 2 TeV.
</p>
<br><br><br><p>Aims: We investigate the evolution of protoplanetary discs (PPDs hereafter)
with magnetically driven disc winds and viscous heating. Methods: We consider
an initially massive disc with ~0.1 Msun to track the evolution from the early
stage of PPDs. We solve the time evolution of surface density and temperature
by taking into account viscous heating and the loss of the mass and the angular
momentum by the disc winds within the framework of a standard alpha model for
accretion discs. Our model parameters, turbulent viscosity, disc wind mass
loss, and disc wind torque, which are adopted from local magnetohydrodynamical
simulations and constrained by the global energetics of the gravitational
accretion, largely depends on the physical condition of PPDs, particularly on
the evolution of the vertical magnetic flux in weakly ionized PPDs. Results:
Although there are still uncertainties concerning the evolution of the vertical
magnetic flux remaining, surface densities show a large variety, depending on
the combination of these three parameters, some of which are very different
from the surface density expected from the standard accretion. When a PPD is in
a \"wind-driven accretion\" state with the preserved vertical magnetic field, the
radial dependence of the surface density can be positive in the inner region
&lt;1-10 au. The mass accretion rates are consistent with observations, even in
the very low level of magnetohydrodynamical turbulence. Such a positive radial
slope of the surface density gives a great impact on planet formation because
(i)it inhibits the inward drift or even results in the outward drift of
pebble/boulder-sized solid bodies, and (ii) it also makes the inward type-I
migration of proto-planets slower or even reversed. Conclusions: The variety of
our calculated PPDs should yield a wide variety of exoplanet systems.
</p>
<br><br><br><p>Open clusters and young stellar associations are attractive sites to search
for planets and to test theories of planet formation, migration, and evolution.
We present our search for, and characterization of, transiting planets in the
$\simeq$800 Myr old Praesepe (Beehive, M44) Cluster from K2 light curves. We
identify seven planet candidates, six of which we statistically validate to be
real planets. For each host star we obtain high-resolution NIR spectra to
measure its projected rotational broadening and radial velocity, the latter of
which we use to confirm cluster membership. We obtain low-resolution optical
and NIR spectroscopy for each system, which we use in conjunction with the
cluster distance and metallicity to provide precise temperatures, masses,
radii, and luminosities for the host stars. Combining our measurements of
rotational broadening, rotation periods from the K2 light curves, and our
derived stellar radii, we show that all planetary orbits are, within errors,
aligned with their host star's rotation. To constrain the planets' parameters
we fit the K2 light curves including a prior on stellar density to put
constraints on the planetary eccentricities, all of which are consistent with
zero. The difference between the number of planets found in Praesepe and Hyades
(8 planets, $\simeq800$ Myr) and a similar dataset for Pleiades (0 planets,
$\simeq125$ Myr) hints of a trend with age, but may be due to incompleteness of
current search pipelines for younger, faster-rotating stars. We see increasing
evidence that some planets continue to lose atmosphere past 800 Myr, as now two
planets at this age have radii significantly larger than their older
counterparts from Kepler.
</p>
<br><br><br><p>Axions may make a significant contribution to the dark matter of the
universe. It has been suggested that these dark matter axions may condense into
localized clumps, called \"axion stars.\" In this paper we argue that collisions
of dilute axion stars with neutron stars may be the origin of most of the
observed fast radio bursts. This idea is a variation of an idea originally
proposed by Iwazaki. However, instead of the surface effect of Iwazaki, we
propose a perhaps stronger volume effect caused by the induced time dependent
electric dipole moment of neutrons.
</p>
<br><br><br><p>Future experiments at the Jefferson Lab 12 GeV upgrade, in particular, the
Solenoidal Large Intensity Device (SoLID), aim at a very precise data set in
the region where the partonic structure of the nucleon is dominated by the
valence quarks. One of the main goals is to constrain the quark transversity
distributions. We apply recent theoretical advances of the global QCD
extraction of the transversity distributions to study the impact of future
experimental data from the SoLID experiments. Especially, we develop a simple
strategy based on the Hessian matrix analysis that allows one to estimate the
uncertainties of the transversity quark distributions and their tensor charges
extracted from SoLID data simulation. We find that the SoLID measurements with
the proton and the effective neutron targets can improve the precision of the
u- and d-quark transversity distributions up to one order of magnitude in the
range 0.05 &lt; x &lt; 0.6.
</p>
<br><br><br><p>Interactions between jets and the quark-gluon plasma produced in heavy ion
collisions are studied via the angular distributions of summed charged-particle
transverse momenta (pt) with respect to both the leading and subleading jet
axes in high-pt dijet events. The contributions of charged particles in
different momentum ranges to the overall event pt balance are decomposed into
short-range jet peaks and a long-range azimuthal asymmetry in charged-particle
pt. The results for PbPb collisions are compared to those in pp collisions
using data collected in 2011 and 2013, at collision energy sqrt(s[NN]) = 2.76
TeV with integrated luminosities of 166 inverse microbarns and 5.3 inverse
picobarns, respectively, by the CMS experiment at the LHC. Measurements are
presented as functions of PbPb collision centrality, charged-particle pt,
relative azimuth, and radial distance from the jet axis for balanced and
unbalanced dijets.
</p>
<br><br><br><p>We report the first measurement of the total MUON flux underground at the
Davis Campus of the Sanford Underground Research Facility at the 4850 ft level.
Measurements were done with the Majorana Demonstrator veto system arranged in
two different configurations. The measured total flux is (5.31+/-0.17) x 10^-9
muons/s/cm^2.
</p>
<br><br><br><p>Strange hadrons, especially multi-strange hadrons are good probes for the
early partonic stage of heavy ion collisions due to their small hadronic cross
sections. In this paper, I give a brief review on the elliptic flow
measurements of strange and multi-strange hadrons in relativistic heavy ion
collisions at Relativistic Heavy Ion Collider (RHIC) and Large Hadron Collider
(LHC).
</p>
<br><br><br><p>For a long time very little experimental information was available about
neutrino properties, even though a minute neutrino mass has intriguing
cosmological and astrophysical implications. This situation has changed in
recent decades: intense experimental activity to measure many neutrino
properties took place. Some of these developments and their implications for
astrophysics and cosmology are briefly reviewed with a particular emphasis on
neutrino magnetic moments and collective neutrino oscillations
</p>
<br><br><br><p>Isomeric studies in neutron-rich nuclei present a powerful tool to explore
the structure at the nuclear extremes. We recently used the shell model
calculations with Renormalized Charge Depen- dent Bonn (RCDB) effective
interaction to calculate the properties of the $6^+$ seniority isomers in
$^{134-138}$Sn in an attempt to resolve the anomalous B(E2) behavior of the
$6^+$ isomer in $^{136}$Sn [Phys. Rev. C 91, 024321 (2015)]. We further explore
these isomers by using the generalized seniority scheme for multi-j orbitals
recently presented by us [Phys. Lett. B 753, 122 (2016)]; the B(E2) values so
calculated reproduce the experimental data quite well, including the anomaly at
$^{136}$Sn confirming the generalized seniority nature of the $6^+$ isomers. We
then use the generalized seniority guided Large Scale Shell Model (LSSM)
calculations, along with the latest single particle energies from Jones et al :
[Nature (London) 465, 454 (2010)] and Allmond et al : [Phys. Rev. Lett. 112,
172701 (2014)] to estimate more accurate location of i$_{13/2}$ neutron orbital
in the extreme neutron rich N = 82-126 region. This entails a new sub-shell
closure at N = 112 due to the higher location of i$_{13/2}$ neutron orbital,
also consistent with the choice of orbitals in the generalized seniority
scheme. However, a small reduction in the f$_{7/2}$ two-body matrix elements is
still required in the LSSM calculations to reproduce the experimental level
energies as well as the transition probabilities in $^{134-138}$Sn isotopes in
a consistent way.
</p>
<br><br><br><p>We propose a self-consistent quasi-particle random phase approximation (QRPA)
plus quasi-particle-vibration coupling (QPVC) model with Skyrme interactions to
describe the width and the line shape of giant resonances in open-shell nuclei,
in which the effect of superfluidity should be taken into account in both the
ground state and the excited states. We apply the new model to the Gamow-Teller
resonance in the superfluid nucleus $^{120}$Sn, including both the isoscalar
spin-triplet and the isovector spin-singlet pairing interactions. The strength
distribution in $^{120}$Sn is well reproduced and the underlying microscopic
mechanisms, related to QPVC and also to isoscalar pairing, are analyzed in
detail.
</p>
<br><br><br><p>QCD sum rules for decay constants of heavy mesons with $u$ or $d$ quark yield
for $B$ mesons much less isospin breaking than lattice QCD but good agreement
for $D$ mesons.
</p>
<br><br><br><p>Future experiments at the Jefferson Lab 12 GeV upgrade, in particular, the
Solenoidal Large Intensity Device (SoLID), aim at a very precise data set in
the region where the partonic structure of the nucleon is dominated by the
valence quarks. One of the main goals is to constrain the quark transversity
distributions. We apply recent theoretical advances of the global QCD
extraction of the transversity distributions to study the impact of future
experimental data from the SoLID experiments. Especially, we develop a simple
strategy based on the Hessian matrix analysis that allows one to estimate the
uncertainties of the transversity quark distributions and their tensor charges
extracted from SoLID data simulation. We find that the SoLID measurements with
the proton and the effective neutron targets can improve the precision of the
u- and d-quark transversity distributions up to one order of magnitude in the
range 0.05 &lt; x &lt; 0.6.
</p>
<br><br><br><p>We present an approach for the calculation of odd-nuclei with exact
self-consistent blocking and particle number and angular momentum projection
with the finite range density dependent Gogny force. As an application we
calculate the nucleus $^{31}$Mg at the border of the $N=20$ inversion island.
We evaluate the ground state properties, the excited states and the transition
probabilities. In general we obtain a good description of the measured
observables.
</p>
<br><br><br><p>The chiral phase transition of the strongly interacting matter is
investigated at nonzero temperature and baryon chemical potential mu_B within
an extended (2+1) flavor Polyakov constituent quark-meson model which
incorporates the effect of the vector and axial vector mesons. The effect of
the fermionic vacuum and thermal fluctuations computed from the grand potential
of the model is taken into account in the curvature masses of the scalar and
pseudoscalar mesons. The parameters of the model are determined by comparing
masses and tree-level decay widths with experimental values in a
chi^2-minimization procedure which selects between various possible assignments
of scalar nonet states to physical particles. We examine the restoration of the
chiral symmetry by monitoring the temperature evolution of condensates and the
chiral partners' masses and of the mixing angles for the pseudoscalar eta-eta'
and the corresponding scalar complex. We calculate the pressure and various
thermodynamical observables derived from it and compare them to the continuum
extrapolated lattice results of the Wuppertal-Budapest collaboration. We study
the T-mu_B phase diagram of the model and find that a critical end point exists
for parameters of the model, which give acceptable values of chi^2.
</p>
<br><br><br><p>r-mode astroseismology provides a unique way to study the internal
composition of compact stars. Due to their precise timing, recycled millisecond
radio pulsars present a particularly promising class of sources. Although their
thermal properties are still poorly constrained, X-ray data is very useful for
astroseismology since r-modes could strongly heat a star. Using known and new
upper bounds on the temperatures and luminosities of several non-accreting
millisecond radio pulsars we derive bounds on the r-mode amplitude as low as
$\alpha\lesssim10^{-8}$ and discuss the impact on scenarios for their internal
composition.
</p>
<br><br><br><p>In many nonlinear field theories, relevant solutions may be found by reducing
the order of the original Euler-Lagrange equations, e.g., to first order
equations (Bogomolnyi equations, self-duality equations, etc.). Here we
generalise, further develop and apply one particular method for the order
reduction of nonlinear field equations which, despite its systematic and
versatile character, is not widely known.
</p>
<br><br><br><p>This paper is concerned with Dirichlet problem $u(0,t)=0$,
$\theta(0,t)=\theta_-$ for one-dimensional full compressible Navier-Stokes
equations in the half space $\R_+=(0,+\infty)$. Because the boundary decay rate
is hard to control, stability of contact discontinuity result is very
difficult. In this paper, we raise the decay rate and establish that for a
certain class of large perturbation, the asymptotic stability result is contact
discontinuity. Also, we ask the strength of contact discontinuity not small.
The proofs are given by the elementary energy method.
</p>
<br><br><br><p>We show that for a conformal local net of observables on the circle, the
split property is automatic. Both full conformal covariance (i.e.
diffeomorphism covariance) and the circle-setting play essential roles in this
fact, while by previously constructed examples it was already known that even
on the circle, M\\"obius covariance does not imply the split property.
</p>
<p>On the other hand, here we also provide an example of a local conformal net
living on the two-dimensional Minkowski space, which - although being
diffeomorphism covariant - does not have the split property.
</p>
<br><br><br><p>In the monograph Renormalization and Effective Field Theory, Costello gave an
inductive position space renormalization procedure for constructing an
effective field theory that is based on heat kernel regularization of the
propagator. In this paper, we extend Costello's renormalization procedure to a
class of manifolds with boundary. In addition, we reorganize the presentation
of the preexisting material, filling in details and strengthening the results.
</p>
<br><br><br><p>We derive the formula of the entanglement entropy between the left and right
oscillating modes of the $\sigma$-model with the de Sitter target space. To
this end, we study the theory in the cosmological gauge in which the
non-vanishing components of the metric on the two-dimensional base space are
functions of the expansion parameter of the de Sitter space. The model is
embedded in the causal north pole diamond of the Penrose diagram. We argue that
the cosmological gauge is natural to the $\sigma$-model as it is compatible
with the canonical quantization relations. In this gauge, we obtain a new
general solution to the equations of motion in terms of time-independent
oscillating modes. The constraint structure is adequate for quantization in the
Gupta-Bleuler formalism. We construct the space of states as a one-parameter
family of Hilbert spaces and give the Bargmann-Fock and Jordan-Schwinger
representations of it. Also, we give a simple description of the physical
subspace as an infinite product of $\mathcal{D}^{+}_{\frac{1}{2}}$ in the
positive discreet series irreducible representations of the $SU(1,1)$ group. We
construct the map generated by the Hamiltonian between states at two different
values of time and show how it produces the entanglement of left and right
excitations. Next, we derive the formula of the entanglement entropy of the
reduced density matrix for the ground state acted upon by the Hamiltonian map.
Finally, we determine the asymptotic form of the entanglement entropy of a
single mode bi-oscillator in the limit of large values of time.
</p>
<br><br><br><p>In this paper we investigate the spectral and the scattering theory of
Gauss--Bonnet operators acting on perturbed periodic combinatorial graphs. Two
types of perturbation are considered: either a multiplication operator by a
short-range or a long-range potential, or a short-range type modification of
the graph. For short-range perturbations, existence and completeness of local
wave operators are proved. In addition, similar results are provided for the
Laplacian acting on edges.
</p>
<br><br><br><p>The Batalin-Vilkovisky (BV) formalism is a powerful generalization of the
BRST approach of gauge theories and allows to treat more general field
theories. We will see how, starting from the case of a finite dimensional
configuration space, we can see this formalism as a theory of integration for
polyvectors over the shifted cotangent bundle of the configuration space, and
arrive at a formula that admits a generalization to the infinite dimensional
case. The process of gauge fixing and the observables of the theory will be
presented.
</p>
<br><br><br><p>For frequency $\alpha$ of bounded type and coupling $\lambda&gt;20$, we show
that the density of states measure $\NN_{\alpha,\lambda}$ of the related Sturm
Hamiltonian is exact upper and lower dimensional, however, in general it is not
exact-dimensional.
</p>
<br><br><br><p>The number $\mathcal{N}$ of stable fixed points of locally coupled Kuramoto
models depends on the topology of the network on which the model is defined. It
has been shown that cycles in meshed networks play a crucial role in
determining $\mathcal{N}$, because any two different stable fixed points differ
by a collection of loop flows on those cycles. Since the number of different
loop flows increases with the length of the cycle that carries them, one
expects $\mathcal{N}$ to be larger in meshed networks with longer cycles.
Simultaneously, the existence of more cycles in a network means more freedom to
choose the location of loop flows differentiating between two stable fixed
points. Therefore, $\mathcal{N}$ should also be larger in networks with more
cycles. We derive an algebraic upper bound for the number of stable fixed
points of the Kuramoto model with identical frequencies, under the assumption
that angle differences between connected nodes do not exceed $\pi/2$. We obtain
$\mathcal{N}\leq\prod_{k=1}^c\left[2\cdot{\rm Int}(n_k/4)+1\right]$, which
depends both on the number $c$ of cycles and on the spectrum of their lengths
$\{n_k\}$. We further identify network topologies carrying stable fixed points
with angle differences larger than $\pi/2$, which leads us to conjecture an
upper bound for the number of stable fixed points for Kuramoto models on any
planar network. Compared to earlier approaches that give exponential upper
bounds in the total number of vertices, our bounds are much lower and therefore
much closer to the true number of stable fixed points.
</p>
<br><br><br><p>We consider an unpinned chain of harmonic oscillators with periodic boundary
conditions, whose dynamics is perturbed by a random flip of the sign of the
velocities. The dynamics conserves the total volume (or elongation) and the
total energy of the system. We prove that in a diffusive space-time scaling
limit the profiles corresponding to the two conserved quantities converge to
the solution of a diffusive system of differential equations. While the
elongation follows a simple autonomous linear diffusive equation, the evolution
of the energy depends on the gradient of the square of the elongation.
</p>
<br><br><br><p>In a paper by Umarov, Tsallis and Steinberg (2008), a generalization of the
Fourier transform, called the $q$-Fourier transform, was introduced and applied
for the proof of a $q$-generalized central limit theorem ($q$-CLT).
Subsequently, Hilhorst illustrated (2009 and 2010) that the $q$-Fourier
transform for $q&gt;1$ is not invertible in the space of density functions.
Indeed, using an invariance principle, he constructed a family of densities
with the same $q$-Fourier transform and noted that \"as a consequence, the
$q$-central limit theorem falls short of achieving its stated goal\". The
distributions constructed there have compact support. We prove now that the
limit distribution in the $q$-CLT is unique and can not have a compact support.
This result excludes all the possible counterexamples which can be constructed
using the invariance principle and fills the gap mentioned by Hilhorst.
</p>
<br><br><br><p>The coherent superposition of states, in combination with energy
quantization, represents one of the most fundamental features that mark the
departure of quantum mechanics from the classical realm. Quantum coherence in
many-body systems embodies the essence of entanglement and is an essential
ingredient for a plethora of physical phenomena in quantum optics, quantum
information, solid state physics, and nanoscale thermodynamics. In recent
years, research on the presence and functional role of quantum coherence in
biological systems has also attracted a considerable interest. Despite the
fundamental importance of quantum coherence, the development of a rigorous
theory of quantum coherence as a physical resource has only been initiated
recently. In this Colloquium we discuss and review the development of this
rapidly growing research field that encompasses the characterization,
quantification, manipulation, dynamical evolution, and operational application
of quantum coherence.
</p>
<br><br><br><p>We continue the study of infinite geodesics in planar first-passage
percolation, pioneered by Newman in the mid 1990s. Building on more recent work
of Hoffman, and Damron and Hanson, we develop an ergodic theory for infinite
geodesics via the study of what we shall call `random coalescing geodesics'.
Random coalescing geodesics have a range of nice asymptotic properties, such as
asymptotic directions and linear Busemann functions. We show that random
coalescing geodesics are (in some sense) dense in the space of geodesics. This
allows us to extrapolate properties from random coalescing geodesics to obtain
statements on all infinite geodesics. As an application of this theory we solve
the `midpoint problem' of Benjamini, Kalai and Schramm and address a question
of Furstenberg on the existence of bigeodesics.
</p>
<br><br><br><p>A nonlinearly generalized Camassa-Holm equation, depending an arbitrary
nonlinearity power $p \neq 0$, is considered. This equation reduces to the
Camassa-Holm equation when $p=1$ and shares one of the Hamiltonian structures
of the Camassa-Holm equation. Two main results are obtained. A classification
of point symmetries is presented and a peakon solution is derived, for all
powers $p \neq 0$.
</p>
<br><br><br><p>Milestoning is a computational procedure that reduces the dynamics of complex
systems to memoryless jumps between intermediates, or milestones, and only
retains some information about the probability of these jumps and the time lags
between them. Here we analyze a variant of this procedure, termed optimal
milestoning, which relies on a specific choice of milestones to capture exactly
some kinetic features of the original dynamical system. In particular, we prove
that optimal milestoning permits the exact calculation of the mean first
passage times (MFPT) between any two milestones. In so doing, we also analyze
another variant of the method, called exact milestoning, which also permits the
exact calculation of certain MFPTs, but at the price of retaining more
information about the original system's dynamics. Finally, we discuss
importance sampling strategies based on optimal and exact milestoning that can
be used to bypass the simulation of the original system when estimating the
statistical quantities used in these methods.
</p>
<br><br><br><p>We consider the non-stationary Heun equation, also known as quantum
Painlev\'e VI, which has appeared in different works on quantum integrable
models and conformal field theory. We use a generalized kernel function
identity to transform the problem to solve this equation into a
differential-difference equation which, as we show, can be solved by efficient
recursive algorithms. We thus obtain series representations of solutions which
provide elliptic generalizations of the Jacobi polynomials. These series
reproduces, in a limiting case, a perturbative solution of the Heun equation
due to Takemura, but our method is different in that we expand in
non-conventional basis functions that allow us to obtain explicit formulas to
all orders; in particular, for special parameter values, our series reduce to a
single term.
</p>
<br><br><br><p>We show a diffusive upper bound on the transition probability of a tagged
particle in the symmetric simple exclusion process. The proof relies on optimal
spectral gap estimates for the dynamics in finite volume, which are of
independent interest. We also show off-diagonal estimates of Carne-Varopoulos
type.
</p>
<br><br><br><p>The motivation for this article came from an attempt to give an alternative
definition for the meter, the SI unit for measuring length. As a starting point
towards this goal, in this piece of work we present the underlying theory
behind our approach which uses ideas from quantum field theory and
noncommutative geometry, in particular the notion of an odd K-cycle which is
based on the Dirac operator (and its inverse, the Dirac propagator). Using (the
perhaps more familiar) physics terminology, the key point in our strategy is
this: instead of measuring length directly in space-time we measure the
\"algebraic (spectral) length\" in the space of the corresponding quantum states
of some particle (fermion) acted upon by the Dirac propagator. This approach
shares the spirit of the unanimus vote of the 24th General Conference of
Standards and Measures (21st October 2011) in Serves, France for the
redefinition of the fundamental units using Planck's constant.
</p>
<br><br><br><p>Strong existence and pathwise uniqueness of solutions with
$L^{\infty}$-vorticity of 2D stochastic Euler equations is proved. The noise is
multiplicative and involves first derivatives. A Lagrangian approach is
implemented, where a stochastic flow solving a nonlinear flow equation is
constructed. Stability under regularization is also proved.
</p>
<br><br><br><p>The standard Feynman diagrammatic approach to quantum field theories assumes
that perturbation theory approximates the full quantum theory at small coupling
even when a mathematically rigorous construction of the latter is absent. On
the other hand, two-dimensional Yang-Mills theory is a rare (if not the only)
example of a nonabelian (pure) gauge theory whose full quantum theory has a
rigorous construction. Indeed, the theory can be formulated via a lattice
approximation, from which Wilson loop expecation values in the continuum limit
can be described in terms of heat kernels on the gauge group. It is therefore
fundamental to investigate how the exact answer for 2D Yang-Mills compares with
that of the continuum perturbative approach, which a priori are unrelated. In
this paper, we provide a mathematically rigorous formulation of the
perturbative quantization of 2D Yang-Mills, and we consider Wilson loop
expectation values on $\mathbb{R}^2$ and $S^2$ in both holomorphic gauge and in
Coulomb gauge with respect to a general metric. We establish the equivalence of
these two gauges and that both are independent of the choice of gauge-fixing
metric. Additionally, using holomorphic gauge, we discuss partial results
showing agreement between the asymptotics of perturbative and lattice Wilson
loop expectations. Our work therefore presents fundamental progress in
confirming the paradigm that continuum perturbation theory accurately captures
the asymptotics of the continuum limit of the lattice theory.
</p>
<br><br><br><p>We study the decimation to a sublattice of half the sites, of the
one-dimensional Dyson-Ising ferromagnet with slowly decaying long-range pair
interactions of the form $\frac{1}{{|i-j|}^{\alpha}}$, in the phase transition
region (1&lt; $\alpha \leq$ 2, and low temperature). We prove non-Gibbsianness of
the decimated measure at low enough temperatures by exhibiting a point of
essential discontinuity for the finite-volume conditional probabilities of
decimated Gibbs measures. Thus result complements previous work proving
conservation of Gibbsianness for fastly decaying potentials ($\alpha$ &gt; 2) and
provides an example of a \"standard\" non-Gibbsian result in one dimension, in
the vein of similar resuts in higher dimensions for short-range models. We also
discuss how these measures could fit within a generalized (almost vs. weak)
Gibbsian framework. Moreover we comment on the possibility of similar results
for some other transformations.
</p>
<br><br><br><p>A one-parameter generalization of the hierarchy of negative flows is
introduced for integrable hierarchies of evolution equations, which yields a
wider (new) class of non-evolutionary integrable nonlinear wave equations. As
main results, several integrability properties of these generalized negative
flow equation are established, including their symmetry structure, conservation
laws, and bi-Hamiltonian formulation. (The results also apply to the hierarchy
of ordinary negative flows). The first generalized negative flow equation is
worked out explicitly for each of the following integrable equations: Burgers,
Korteweg-de Vries, modified Korteweg-de Vries, Sawada-Kotera, Kaup-Kupershmidt,
Kupershmidt.
</p>
<br><br><br><p>Recent years have seen an upsurge of interest in dynamical realizations of
the superconformal group SU(1,1|2) in mechanics. Remarking that SU(1,1|2) is a
particular member of a chain of supergroups SU(1,1|n) parametrized by an
integer n, here we begin a systematic study of SU(1,1|n) multi-particle
mechanics. A representation of the superconformal algebra su(1,1|n) is
constructed on the phase space spanned by m copies of the (1,2n,2n-1)
supermultiplet. We show that the dynamics is governed by two prepotentials V
and F, and the Witten-Dijkgraaf-Verlinde-Verlinde equation for F shows up as a
consequence of a more general fourth-order equation. All solutions to the
latter in terms of root systems reveal decoupled models only. An extension of
the dynamical content of the (1,2n,2n-1) supermultiplet by angular variables in
a way similar to the SU(1,1|2) case is problematic.
</p>
<br><br><br><p>We present a method to develop a Hodge theory for tangential cohomology of
foliations by mimicing Witten's approach to ordinary Morse theory by
perturbations of the Laplacian
</p>
<br><br><br><p>We consider the problem of estimating a classical parameter encoded in a
quantum channel, assuming the most general strategy allowed by quantum
mechanics. This strategy is based on the exploitation of an unlimited amount of
pre-shared entanglement plus the use of adaptive probings, where the input of
the channel is interactively updated during the protocol. We show that, for the
wide class of teleportation-stretchable channels in finite dimension, including
all Pauli channels and erasure channels, the quantum Fisher information cannot
exceed an ultimate bound given by the Choi matrix of the encoding channel. We
also extend our methods and results to quantum channel discrimination, finding
a corresponding ultimate bound for the minimum error probability. Thus, our
findings establish the ultimate precision limits that are achievable in quantum
metrology and quantum discrimination for the most basic models of
discrete-variable quantum channels.
</p>
<br><br><br><p>We consider the secret key capacity of the thermal loss channel, which is
modeled by a beam splitter mixing an input signal mode with an environmental
thermal mode. This capacity is the maximum value of secret bits that two remote
parties can generate by means of the most general adaptive protocols assisted
by unlimited and two-way classical communication. To date, only upper and lower
bounds are known. The present work improves the lower bound by resorting to
Gaussian protocols based on suitable trusted-noise detectors.
</p>
<br><br><br><p>Non-classical correlations denote the ways in which quantum systems can be
correlated beyond what is possible in classical physics. They include but are
not limited to entanglement: non-entangled states can also exhibit forms of
non-classical correlations. We here review the literature on metrological
measures of non-classical correlations and show how they relate to increased
probe sensitivity in quantum metrology protocols such as interferometric
parameter estimation and state discrimination.
</p>
<br><br><br><p>Symmetry under permutations of indistinguishable particles, contained in each
medium, is one of the fundamental symmetries. Generally, a change in symmetry
affects the medium's thermodynamic properties, leading to phase transitions.
Permutation symmetry can be changed since, in addition to the conventional
symmetric and anti-symmetric states under permutations of bosons and fermions,
mathematical group-representation theory allows for non-Abelian permutation
symmetry. Such symmetry can be hidden in states with defined total spins of
spinor gases, which can be formed in optical cavities. However, the
thermodynamic effects of non-Abelian symmetry are unknown. The present work
shows that the symmetry reveals itself in spin-independent or
coordinate-independent properties of these gases, namely as non-Abelian entropy
in thermodynamic properties. In weakly interacting Fermi gases, saturated and
unsaturated phases appear associated with fermionic and non-Abelian symmetry
under permutations of particle states, respectively. The second-order
transitions between the phases are characterized by discontinuities in specific
heat. Unlike other phase transitions, the present ones are not caused by
interactions and can appear even in ideal gases. In the same way, a change in
permutation symmetry can lead to similar effects in strongly interacting and in
Bose gases. States with non-Abelian symmetry are entangled and can find
applications in quantum metrology, computing and information processing, like
non-Abelian states related to the braid group.
</p>
<br><br><br><p>We prove a general theorem that the action of arbitrary classical noise or
random unitary channels can not increase the maximum population of any
eigenstate of an open quantum system, assuming initial system-environment
factorization. Such factorization is the conventional starting point for
descriptions of open system dynamics. In particular, our theorem implies that a
system can not be ideally cooled down unless it is initially prepared as a pure
state. The resultant inequality rigorously constrains the possibility of
cooling the system solely through temporal manipulation, i.e., dynamical
control over the system Hamiltonian without resorting to measurement based
cooling methods.
</p>
<br><br><br><p>Presented is a quantum lattice gas algorithm to efficiently model a system of
Dirac particles interacting through an intermediary gauge field. The quantum
lattice gas algorithm uses the concept of qubit array that represents both the
spacetime and the particles contained in the spacetime. The quantum algorithm
has the advantage of not breaking Lorentz invariance---i.e., a Dirac
Hamiltonian generates the unitary evolution even at small grid scales. Yet,
this uncommon feature of a lattice-based model implies an unexpected nonlinear
scaling between the smallest observable time and that time measured in the
quantum field theory limit---i.e., this nonlinear scaling is a kind of time
dilation effect that emerges on small scales but has no effect on large scales.
Therefore, since the quantum lattice gas algorithm correctly accounts for the
anticommutative braiding of indistinguishable fermions and since it does not
suffer the Fermi-sign problem, it can be used for dynamical modeling and
simulation. With its high-order numerical convergence, its prime application as
a numerical quantum simulation method for strongly-correlated Fermi gases is
presented for the case when a Dirac particle's Compton wavelength is large
compared to the grid scale of the qubit array.
</p>
<br><br><br><p>We investigate energy transfer by the radiation from a cavity QED system in
the context of quantum thermodynamics. We propose a method of decomposing it
into work and heat within the framework of quantum master equations. We find
that the work and heat respectively correspond to the coherent and incoherent
parts of the radiation. In the derivation of the method, it is crucial to
investigate the dynamics of the system which receives the radiation from the
cavity.
</p>
<br><br><br><p>Single photon sources can be used to generate and control highly
non-classical states of light. Here, we consider the possibility of generating
bi-partite entanglement between two separate multi-photon states by splitting
the input modes of the single photon sources into a mode distributed to $A$ and
a mode distributed to $B$. If multiple photons are combined at $A$ and at $B$,
the large variety of photon patterns observed after linear optics
transformations of the local multi-mode systems can be used to characterize the
non-classical correlation of this entangled state. However, the transformation
between different photon number states in linear optics is a complicated
problem, and further analysis is needed to understand the possible applications
of this scalable quantum system. To obtain maximal complementarity between two
measurements, we consider the unbiased interference of all input modes
represented by a discrete Fourier transform of the multi-mode field. We
identify characteristic patterns that can be used to identify non-classical
correlations between the two multi-photon systems at $A$ and $B$ and derive a
quantitative criterion for entanglement verification. Our results indicate that
the large scale quantum operations can be achieved by a comparatively simple
quantum network interfering the outputs of multiple single photons, where the
size of the system mainly depends on the number of available single photon
sources.
</p>
<br><br><br><p>It is proven that the exact excited-state wavefunction and energy may be
obtained by minimizing the energy expectation value of a trial wave function
that is constrained only to have the correct nodes of the state of interest.
This excited-state nodal minimum principle has the advantage that it requires
neither minimization with the con- straint of wavefunction orthogonality to all
lower eigenstates nor the antisymmetry of the trial wavefunctions. It is also
found that the minimization over the entire space can be partitioned into
several in- terconnected minimizations within the individual nodal regions, and
the exact excited-state energy may be obtained by a minimization in just one or
several of these nodal regions. For the proofs of the the- orem, it is observed
that the many-electron eigenfunction, restricted to a nodal region, is
equivalent to a ground state wavefunction of one electron in a higher
dimensional space; and an explicit excited-state energy variational expression
is obtained by generalizing the Jacobi method of multiplicative variation.
</p>
<br><br><br><p>A novel unified Hamiltonian approach is proposed to solve Maxwell-Schrodinger
equation for modeling the interaction between classical electromagnetic (EM)
fields and particles. Based on the Hamiltonian of electromagnetics and quantum
mechanics, a unified Maxwell-Schrodinger system is derived by the variational
principle. The coupled system is well-posed and symplectic, which ensures
energy conserving property during the time evolution. However, due to the
disparity of wavelengths of EM waves and that of electron waves, a numerical
implementation of the finite-difference time-domain (FDTD) method to the
multiscale coupled system is extremely challenging. To overcome this
difficulty, a reduced eigenmode expansion technique is first applied to
represent the wave function of the particle. Then, a set of ordinary
differential equations (ODEs) governing the time evolution of the
slowly-varying expansion coefficients are derived to replace the original
Schrodinger equation. Finally, Maxwell's equations represented by the vector
potential with a Coulomb gauge, together with the ODEs, are solved
self-consistently. For numerical examples, the interaction between EM fields
and a particle is investigated for both the closed and open electromagnetic
systems. The proposed approach not only captures the Rabi oscillation
phenomenon in the closed cavity but also captures the effects of radiative
decay and shift in the open free space. After comparing with the existing
theoretical approximate models, it is found that the approximate models break
down in certain cases where a rigorous self-consistent approach is needed. This
work is helpful for the EM simulation of emerging nanodevices or
next-generation quantum electrodynamic systems.
</p>
<br><br><br><p>In the 1990's, C. M. Bender established the $\cal PT$-symmetric quantum
theory. In this theory, the parity operator $\cal P$ and time reversal operator
$\cal T$ are of great significance. In this paper, by using the concrete forms
of $\cal P$ and $\cal T$ in two dimensional spaces, we show that if ${\cal
T}^2= I$, then there are many $\cal P$ commuting with $\cal T$. However, if
${\cal T}^2= -I$, then $\cal P$ commuting with $\cal T$ can only be $\pm I$. We
give out the geometrical interpretation of how $\cal P$ commutes with $\cal T$,
and show that if $\cal T$ is given, then $\cal P$ links with the quadric
surfaces; if $\cal P$ is given, then $\cal T$ links with the quadric curves.
Moreover, we present the conditions when an operator is unbroken $\cal
PT$-symmetric, and show that each Hermitian operator is unbroken $\cal
PT$-symmetric.
</p>
<br><br><br><p>Systems passing through quantum critical points at finite rates have a finite
probability of undergoing transitions between different eigenstates of the
instantaneous Hamiltonian. This mechanism was proposed by Kibble as the
underlying mechanism for the formation of topological defects in the early
universe and by Zurek for condensed matter systems. Here, we use a system of
nuclear spins as an experimental quantum simulator undergoing a non-equilibrium
quantum phase transition. The experimental data confirm the validity of the
Kibble-Zurek mechanism of defect formation.
</p>
<br><br><br><p>Entanglement, describing the inseparability of a quantum multiparty system,
is one of the most intriguing features of quantum mechanics. Violation of Bell
inequality, for ruling out the possibility of local hidden variable theories,
is commonly used as a strong witness for quantum entanglement. In previous Bell
test experiments with photonic entanglement based on two-photon coincidence
measurement, the photon temporal wave packets are absorbed completely by the
detectors. That is, the photon coherence time is much shorter than the
detection time window. Here we demonstrate generation of frequency-bin
entangled narrowband biphotons, and for the first time, test the
Clauser-Horne-Shimony-Holt (CHSH) Bell inequality |S|&lt;= 2 for their nonlocal
temporal correlations with time-resolved detection. We obtain a maximum |S|
value of 2.52+/-0.48 that violates the CHSH inequality. Our result will have
applications in quantum information processing involving time-frequency
entanglement.
</p>
<br><br><br><p>We present a self-contained engine, which is made of one or more two-level
systems, each of which is coupled to a single bath, as well as to a common load
composed of a particle on a tilted lattice. We show that the energy and the
entropy absorbed by the spins are transferred to the particle thus setting it
into upward motion at an average constant speed, even when driven by a single
spin connected to a single bath. When considering an ensemble of different
spins, the velocity of the particle is larger when the tilt is on resonance
with any of the spins' energy splitting. Interestingly, we find regimes where
the spins' polarization enters periodic cycles with the oscillation period
being determined by the tilt of the lattice.
</p>
<br><br><br><p>In this paper, we analyse the quantum coherence behaviors of a single qubit
in the relativistic regime beyond the single-mode approximation. Firstly, we
investigate the freezing condition of quantum coherence in fermionic system. We
also study the quantum coherence tradeoff between particle and antiparticle
sector. It is found that there exists quantum coherence transfer between
particle and antiparticle sector, but the coherence lost in particle sector is
not entirely compensated by the coherence generation of antiparticle sector.
Besides, we emphatically discuss the cohering power and decohering power of
Unruh channel with respect to the computational basis. It is shown that
cohering power is vanishing and decohering power is dependent of the choice of
Unruh mode and acceleration. Finally, We compare the behaviors of quantum
coherence with geometric quantum discord and entanglement in relativistic
setup. Our results show that this quantifiers in two region converge at
infinite acceleration limit, which implies that this measures become
independent of Unruh modes beyond the single-mode approximations. It is also
demonstrated that the robustness of quantum coherence and geometric quantum
discord are better than entanglement under the influence of acceleration, since
entanglement undergoes sudden death.
</p>
<br><br><br><p>We emphasize the usefulness of the Lie brackets in the context of classical
and quantum mechanics. By way of examples we show that many dynamical systems,
especially the ones with (gauge) constraints, can equally be treated in their
time development with non-canonical variables and Hamiltonians. After a short
presentation of the Lie bracket algebra and treating some easier standard
problems with the Lie bracket techniques, we concentrate mainly on charged
particles with gauge constraint in a constant external magnetic field. Since
most of our quantum field theories are meanwhile considered effective, we have
purposely treated our final problems with $c$-number instead of field -operator
Lagrangians. The van Vleck determinant, which is exact for our problems, is
employed to calculate the $c$-number Feynman-Schwinger propagation function.
There is no need for operators or renormalization. In particular, the
non-relativistic propagator in $2+1$ dimensions and the more complicated one in
$3+1$ dimensions are presented in all their glorious detail. On the more
editorial side: we have dispensed with numerating the various problems. They
are not so much disjoint that they needed an extra title. Also, the article is
written in a self-consistent way, meaning one should be able to read it without
time-consuming research in textbooks and journals - with a few exceptions, in
particular Schwinger's paper [J. Schwinger, Phys. Rev. 82, 664 (1951)], which
is the most-cited paper in modern quantum-field-theory physics. Most of the
prerequisites for reading the present article can be found in extenso in [W.
Dittrich and M. Reuter, Classical and quantum dynamics (Springer, Berlin,
Germany, 2016)].
</p>
<br><br><br><p>We propose a general numerical method to study the Casimir effect in lattice
gauge theories. We illustrate the method by calculating the energy density of
zero-point fluctuations around two parallel wires of finite static permittivity
in Abelian gauge theory in two spatial dimensions. We discuss various subtle
issues related to the lattice formulation of the problem and show how they can
successfully be resolved. Finally, we calculate the Casimir potential between
the wires of a fixed permittivity, extrapolate our results to the limit of
ideally conducting wires and demonstrate excellent agreement with a known
theoretical result.
</p>
<br><br><br><p>Quantum multi-hop teleportation is important in the field of quantum
communication. In this study, we propose a quantum multi-hop communication
model and a quantum routing protocol with multi-hop teleportation for wireless
mesh backbone networks. Based on an analysis of quantum multi-hop protocols, a
partially entangled Greenberger--Horne--Zeilinger (GHZ) state is selected as
the quantum channel for the proposed protocol. Both quantum and classical
wireless channels exist between two neighboring nodes along the route. With the
proposed routing protocol, quantum information can be transmitted hop by hop
from the source node to the destination node. Based on multi-hop teleportation
based on the partially entangled GHZ state, a quantum route established with
the minimum number of hops. The difference between our routing protocol and the
classical one is that in the former, the processes used to find a quantum route
and establish quantum channel entanglement occur simultaneously. The Bell state
measurement results of each hop are piggybacked to quantum route finding
information. This method reduces the total number of packets and the magnitude
of air interface delay. The deduction of the establishment of a quantum channel
between source and destination is also presented here. The final success
probability of quantum multi-hop teleportation in wireless mesh backbone
networks was simulated and analyzed. Our research shows that quantum multi-hop
teleportation in wireless mesh backbone networks through a partially entangled
GHZ state is feasible.
</p>
<br><br><br><p>The quantum mechanical propagator of a massive particle in a linear
gravitational potential derived already in 1927 by Earle H. Kennard
\cite{Kennard,Kennard2} contains a phase that scales with the third power of
the time $T$ during which the particle experiences the corresponding force.
Since in conventional atom interferometers the internal atomic states are all
exposed to the same acceleration $a$, this $T^3$-phase cancels out and the
interferometer phase scales as $T^2$. In contrast, by applying an external
magnetic field we prepare two different accelerations $a_1$ and $a_2$ for two
internal states of the atom, which translate themselves into two different
cubic phases and the resulting interferometer phase scales as $T^3$. We present
the theoretical background for, and summarize our progress towards
experimentally realizing such a novel atom interferometer.
</p>
<br><br><br><p>A study of \"high temperature\" entangled states in a system of two
parametrically coupled quantum oscillators placed into independent thermal
baths is performed taking into account partially coherent parametric pump.
Processes in an open system are considered based on the Heisenberg-Langevin
formalism. We obtain a closed system of equations for the averaged quadratic
correlation functions in quantum stochastic problem as a result of Markov
processes approximation. On the basis of numerical calculations the dynamics of
the logarithmic negativity, which is the measure of entanglement in the system,
is investigated. It is shown that the partial coherence of the parametric pump
makes the lifetime of the entangled states finite. The threshold
characteristics of the formation and existence of these states are specified.
</p>
<br><br><br><p>We report a bright photon-pair source with a coincidence counting rate per
input power (cps/mW) of tens of thousands, obtained via spontaneous four-wave
mixing from a Doppler-broadened atomic ensemble of the 5S1/2-5P3/2-5D5/2
transition of 87Rb. The photon-pair generation rate is enhanced by the
two-photon coherence contributions from almost all the atomic velocity groups
in the Doppler-broadened ladder-type atomic system. We obtained the violation
of the Cauchy-Schwarz inequality by a factor of 2370(150). We believe that our
scheme for highly bright paired photons is important as a useful quantum light
source for quantum entanglement swapping between completely autonomous sources.
</p>
<br><br><br><p>Quasielectrons in fractional quantum Hall systems are known to be much harder
to describe theoretically than quasiholes. The problem is that one obtains a
singularity in the wavefunction if one tries to naively construct the
quasielectron as the inverse of the quasihole. Here we demonstrate that the
same problem does not arise in lattice fractional quantum Hall models, so that
quasielectrons can indeed be obtained as the inverse of quasiholes. Using this
significantly simplified description, which can be applied quite generally, we
compute braiding properties of quasielectrons and quasiholes and show that the
charge distribution of quasielectrons is minus the charge distribution of
quasiholes for different models in the disc and the torus geometry. We also
derive few-body Hamiltonians, for which various states containing
quasielectrons are ground states.
</p>
<br><br><br><p>Although people have already artificially formed parity--time
($\mathcal{PT}$) symmetry with gain and loss in a balanced manner, it is still
a defect that the gain is restricted to semi--classical but not full quantum.
Here we propose and analyze a theoretical scheme to realize full quantum
oscillator $\mathcal{PT}$--symmetry. The quantum gain is provided by a
dissipation optical cavity with blue detuned laser field. After adiabatically
eliminating the cavity modes, we give an effective master equation, which is a
more complete quantum description compared with non--Hermitian Hamiltonian, to
reveal the quantum behaviors of such a gain oscillator. This kind of
$\mathcal{PT}$--symmetry can eliminate the dissipation effect in quantum
regime. As examples, we finally apply $\mathcal{PT}$--symmetric oscillators to
enhance optomechanically induced transparency and to preserve oscillator
non--classical state.
</p>
<br><br><br><p>There are a number of different strategies to measure the phase shift between
two pathways of light more efficiently than suggested by the standard quantum
limit. One way is to use highly entangled photons. Another way is to expose
photons to a non-linear or interacting Hamiltonian. This paper emphasises that
the conditional dynamics of open quantum systems provides an interesting
additional tool for quantum-enhanced metrology. As a concrete example, we
review a recent scheme which exploits the conditional dynamics of a
laser-driven optical cavity with spontaneous photon emission inside a quantum
feedback loop. Deducing information from second-order photon correlation
measurements requires neither optical non-linearities nor entangled photons and
should therefore be of immediate practical interest.
</p>
<br><br><br><p>We construct a quantum-inspired classical algorithm for computing the
permanent of Hermitian positive semidefinite matrices, by exploiting a
connection between these mathematical structures and the boson sampling model.
Specifically, the permanent of a Hermitian positive semidefinite matrix can be
expressed in terms of the expected value of a random variable, which stands for
a specific photon-counting probability when measuring a linear-optically
evolved random multimode coherent state. Our algorithm then approximates the
matrix permanent from the corresponding sample mean and is shown to run in
polynomial time for various sets of Hermitian positive semidefinite matrices,
achieving a precision that improves over known techniques. This work
illustrates how quantum optics may benefit algorithms development.
</p>
<br><br><br><p>We compare the time evolution of entanglement measures after local operator
excitation in the critical Ising model with predictions from conformal field
theory. For the spin operator and its descendants we find that Renyi entropies
of a block of spins increase by a constant that matches the logarithm of the
quantum dimension of the conformal family. However, for the energy operator we
find a small constant contribution that differs from the conformal field theory
answer equal to zero. We argue that the mismatch is caused by the subtleties in
the identification between the local operators in conformal field theory and
their lattice counterpart. Our results indicate that evolution of entanglement
measures in locally excited states not only constraints this identification,
but also can be used to extract non-trivial data about the conformal field
theory that governs the critical point. We generalize our analysis to the Ising
model away from the critical point, states with multiple local excitations, as
well as the evolution of the relative entropy after local operator excitation
and discuss universal features that emerge from numerics.
</p>
<br><br><br><p>Simplicity constraints play a crucial role in the construction of spin foam
models, yet their effective behaviour on larger scales is scarcely explored. In
this article we introduce intertwiner and spin net models for the quantum group
$\text{SU}(2)_k \times \text{SU}(2)_k$, which implement the simplicity
constraints analogous to 4D Euclidean spin foam models, namely the
Barrett-Crane (BC) and the Engle-Pereira-Rovelli-Livine/Freidel-Krasnov
(EPRL/FK) model. These models are numerically coarse grained via tensor network
renormalization, allowing us to trace the flow of simplicity constraints to
larger scales. In order to perform these simulations we have substantially
adapted tensor network algorithms, which we discuss in detail.
</p>
<p>The BC and the EPRL/FK model behave very differently under coarse graining:
While the unique BC intertwiner model is a fixed point and therefore
constitutes a 2D topological phase, BC spin net models flow away from the
initial simplicity constraints and converge to several different topological
phases. Most of these phases correspond to decoupling spin foam vertices,
however we find also a new phase in which this is not the case, and in which a
non-trivial version of the simplicity constraints holds. The coarse graining
flow of the BC spin net models indicates furthermore that the phase transitions
are not of second order. The EPRL/FK model by contrast reveals a far more
intricate and complex dynamics. We observe an immediate flow away from the
original simplicity constraints, however, with the truncation employed here,
the models generically do not converge to a fixed point.
</p>
<p>The results show that the imposition of simplicity constraints can indeed
lead to interesting and complex dynamics. Thus we will need to further develop
coarse graining tools to efficiently study the large scale behaviour of spin
foam models, in particular for the EPRL/FK model.
</p>
<br><br><br><p>We present a detailed study of the equilibrium properties and stochastic
dynamic evolution of the U(1)-invariant relativistic complex field theory in
three dimensions. This model has been used to describe, in various limits,
properties of relativistic bosons at finite chemical potential, type II su-
perconductors, magnetic materials and aspects of cosmology. We characterise the
thermodynamic second-order phase transition in different ways. We study the
equilibrium vortex configurations and their statistical and geometrical
properties in equilibrium at all temperatures. We show that at very high
temperature the statistics of the filaments is the one of fully-packed loop
models. We identify the temperature, within the ordered phase, at which the
number density of vortex lengths falls-off algebraically and we associate it to
a geometric percolation transition that we characterise in various ways. We
measure the fractal properties of the vortex tangle at this threshold. Next, we
perform infinite rate quenches from equilibrium in the disordered phase, across
the thermo- dynamic critical point, and deep into the ordered phase. We show
that three time regimes can be distinguished: a first approach towards a state
that, within numerical accuracy, shares many features with the one at the
percolation threshold, a later coarsening process that does not alter, at
sufficiently low temperature, the fractal properties of the long vortex loops,
and a final approach to equilibrium. These features are independent of the
reconnection rule used to build the vortex lines. In each of these regimes we
identify the various length-scales of the vortices in the system. We also study
the scaling properties of the ordering process and the progressive annihilation
of topological defects and we prove that the time-dependence of the
time-evolving vortex tangle can be described within the dynamic scaling
framework.
</p>
<br><br><br><p>Topological order has become a new paradigm to distinguish ground states of
interacting many-body systems without conventional long-range order. Here we
discuss possible extensions of this concept to density matrices describing
statistical ensembles. For a large class of quasi-thermal states we generalize
earlier definitions of density matrix topology to generic many-body systems
with correlations and non-local entanglement. We point out that the robustness
of topological order depends crucially on the perturbations under
consideration. While it is intrinsically protected against local perturbations
of arbitrary strength in a closed system, topological order can easily be
destroyed in open systems coupled to local baths. We discuss our classification
scheme using the finite-temperature quantum Hall states and point out that the
classical Hall effect can be understood as a finite temperature topological
phase.
</p>
<br><br><br><p>We investigate lattice effects on wave functions that are lattice analogues
of bosonic and fermionic Laughlin wave functions with number of particles per
flux $\nu=1/q$ in the Landau levels. These wave functions are defined
analytically on lattices with $\mu$ particles per lattice site, where $\mu$ may
be different than $\nu$. We give numerical evidence that these states have the
same topological properties as the corresponding continuum Laughlin states for
different values of $q$ and for different fillings $\mu$. These states define,
in particular, particle-hole symmetric lattice Fractional Quantum Hall states
when the lattice is half-filled. On the square lattice it is observed that for
$q\leq 4$ this particle-hole symmetric state displays the topological
properties of the continuum Laughlin state at filling fraction $\nu=1/q$, while
for larger $q$ there is a transition towards long-range ordered
anti-ferromagnets. This effect does not persist if the lattice is deformed from
a square to a triangular lattice, or on the Kagome lattice, in which case the
topological properties of the state are recovered. We then show that changing
the number of particles while keeping the expression of these wave functions
identical gives rise to edge states that have the same correlations in the bulk
as the reference lattice Laughlin states but a different density at the edge.
We derive an exact parent Hamiltonian for which all these edge states are
ground states with different number of particles. In addition this Hamiltonian
admits the reference lattice Laughlin state as its unique ground state of
filling factor $1/q$. Parent Hamiltonians are also derived for the lattice
Laughlin states at other fillings of the lattice, when $\mu\leq 1/q$ or
$\mu\geq 1-1/q$ and when $q=4$ also at half-filling.
</p>
<br><br><br><p>The coherent superposition of states, in combination with energy
quantization, represents one of the most fundamental features that mark the
departure of quantum mechanics from the classical realm. Quantum coherence in
many-body systems embodies the essence of entanglement and is an essential
ingredient for a plethora of physical phenomena in quantum optics, quantum
information, solid state physics, and nanoscale thermodynamics. In recent
years, research on the presence and functional role of quantum coherence in
biological systems has also attracted a considerable interest. Despite the
fundamental importance of quantum coherence, the development of a rigorous
theory of quantum coherence as a physical resource has only been initiated
recently. In this Colloquium we discuss and review the development of this
rapidly growing research field that encompasses the characterization,
quantification, manipulation, dynamical evolution, and operational application
of quantum coherence.
</p>
<br><br><br><p>We study the nonequilibrium quantum phase transition of an Ising chain in a
dissipative cavity driven by an external transverse light field. When driving
and dissipation are in balance, the system can reach a nonequilibrium steady
state which undergoes a super-radiant phase transition as the driving strength
increases. Interestingly, the super-radiant field changes the effective bias of
the Ising chain in return and drives its own transition between the
ferromagnetic and paramagnetic phase. We study the rich physics in this system
with sophisticated behavior, and investigate important issues in its dynamics
such as the stability of the system and criticality of the phase transition.
</p>
<br><br><br><p>We demonstrate a fiber-integrated quantum optical circulator that is operated
by a single atom and that relies on the chiral interaction between emitters and
transversally confined light. Like its counterparts in classical optics, our
circulator exhibits an inherent asymmetry between light propagation in the
forward and the backward direction. However, rather than a magnetic field or a
temporal modulation, it is the internal quantum state of the atom that controls
the operation direction of the circulator. This working principle is compatible
with preparing the circulator in a coherent superposition of its operational
states. Such a quantum circulator may thus become a key element for routing and
processing quantum information in scalable integrated optical circuits.
Moreover, it features a strongly nonlinear response at the single-photon level,
thereby enabling, e.g., photon number-dependent routing and novel quantum
simulation protocols.
</p>
<br><br><br><p>We show that the geometric phase of the gyro-motion of a classical charged
particle in a uniform time-dependent magnetic field described by Newton's
equation is a coherent quantum Berry phase for the coherent states of the
Schr\\"odinger equation or the Dirac equation. This equivalence is established
by constructing coherent states for a particle using the energy eigenstates on
the Landau levels and proving that the coherent states can maintain their
status of coherent states during the slow varying of the magnetic field. It is
discovered that orbital Berry phases of the eigenstates interfere coherently
such that a coherent Berry phase for the coherent states can be naturally
defined, which is exactly the geometric phase of the classical gyro-motion.
This technique works for particles with and without spin. For particles with
spin, on each of the eigenstates that makes up the coherent states, the Berry
phase consists of two parts that can be identified as those due to the orbital
and the spin motion. It is the orbital Berry phases that interfere coherently
to produce a coherent Berry phase corresponding to the classical geometric
phase of the gyro-motion. The spin Berry phases of the eigenstates, on the
other hand, only result in an incoherent Berry phase for the coherent states,
which remains to be a quantum phase factor for the coherent states and has no
classical counterpart.
</p>
<br><br><br><p>There is increasing interest in the potential advantages of using quantum
computing technologies as sampling engines to speedup machine learning and
probabilistic programming tasks. However, some pressing challenges in
state-of-the-art quantum annealers have to be overcome before we can assess
their actual performance. Most notably, the effective temperature at which
samples are generated is instance-dependent and unknown, the interaction graph
is sparse, the parameters are noisy, and the dynamic range of the parameters is
finite. Of all these limitations, the sparse connectivity resulting from the
local interaction between quantum bits in physical hardware implementations, is
considered the most severe limitation to the quality of constructing powerful
machine learning models. Here we show how to surpass this \"curse of limited
connectivity\" bottleneck and illustrate our findings by training probabilistic
generative models with arbitrary pairwise connectivity. Our model can be
trained in quantum hardware without full knowledge of the effective parameters
specifying the corresponding Boltzmann-like distribution. Therefore, inference
of the effective temperature is avoided and the effect of noise in the
parameters is mitigated. We illustrate our findings by successfully training
hardware-embedded models with all-to-all connectivity on a real dataset of
handwritten digits and two synthetic datasets. In each of these datasets we
show the generative capabilities of the models learned with the assistance of
the quantum annealer in experiments with up to 940 quantum bits. Additionally,
we show a visual Turing test with handwritten digit data, where the machine
generating the digits is a quantum processor. Such digits, with a remarkable
similarity to those generated by humans, are extracted from the experiments
with 940 quantum bits.
</p>
<br><br><br><p>Disordered systems form one of the centrestages of research in many body
sciences and lead to a plethora of interesting phenomena and applications. A
paradigmatic disordered system consists of an one-dimensional array of quantum
spin-1/2 particles, governed by the Heisenberg spin glass Hamiltonian with
natural or engineered quenched disordered couplings in an external magnetic
field. These systems allow disorder-induced enhancement for bipartite and
multipartite observables. Here we show that simultaneous application of
independent quenched disorders results in disorder-induced enhancement, while
the same is absent with individual application of the same disorders. We term
the phenomenon as constructive interference and the corresponding parameter
stretches as the Venus regions. Interestingly, it has only been observed for
multiparty entanglement and is absent for the single- and two-party physical
quantities.
</p>
<br><br><br><p>We propose a simple diatomic system trapped inside an optical cavity to
control the energy flow between two thermal baths. Through the action of the
baths the system is driven to a non- equilibrium steady state. Using the Large
Deviation theory we show that the number of photons flowing between the two
baths is dramatically different depending on the symmetry of the atomic states.
Here we present a deterministic scheme to prepare symmetric and antisymmetric
atomic states with the use of external driving fields, thus implementing an
atomic control switch for the energy flow.
</p>
<br><br><br><p>We give a new theoretical solution to a leading-edge experimental challenge,
namely to the verification of quantum computations in the regime of high
computational complexity. Our results are given in the language of quantum
interactive proof systems. Specifically, we show that any language in BQP has a
quantum interactive proof system with a polynomial-time classical verifier (who
can also prepare random single-qubit pure states), and a quantum
polynomial-time prover. Here, soundness is unconditional---i.e. it holds even
for computationally unbounded provers. Compared to prior work achieving similar
results, our technique does not require the encoding of the input or of the
computation; instead, we rely on encryption of the input (together with a
method to perform computations on encrypted inputs), and show that the random
choice between three types of input (defining a computational run, versus two
types of test runs) suffices. Because the overhead is very low for each run (it
is linear in the size of the circuit), this shows that verification could be
achieved at minimal cost compared to performing the computation. As a proof
technique, we use a reduction to an entanglement-based protocol; this is the
first time this technique has been used in the context of verification of
quantum computations, and it enables a relatively straightforward analysis.
</p>
<br><br><br><p>We apply microcanonical ensemble considerations to suggest that, whenever it
may thermalize, a general disorder-free many-body Hamiltonian of a typical
atomic system has solid-like eigenstates at low energies and fluid-type (and
gaseous, plasma) eigenstates associated with energy densities exceeding those
present in the melting (and, respectively, higher energy) transition(s). In
particular, the lowest energy density at which the eigenstates of such a clean
many body atomic system undergo a non-analytic change is that of the melting
(or freezing) transition. We invoke this observation to analyze the evolution
of a liquid upon supercooling (i.e., cooling rapidly enough to avoid
solidification below the freezing temperature). Expanding the wavefunction of a
supercooled liquid in the complete eigenbasis of the many-body Hamiltonian,
only the higher energy liquid-type eigenstates contribute significantly to
measurable hydrodynamic relaxations (e.g., those probed by viscosity) while
static thermodynamic observables become weighted averages over both solid- and
liquid-type eigenstates. Consequently, when extrapolated to low temperatures,
hydrodynamic relaxation times of deeply supercooled liquids (i.e., glasses) may
seem to diverge at nearly the same temperature at which the extrapolated
entropy of the supercooled liquid becomes that of the solid. In this formal
quantum framework, the increasingly sluggish (and spatially heterogeneous)
dynamics in supercooled liquids as their temperature is lowered stems from the
existence of the single non-analytic change of the eigenstates of the clean
many-body Hamiltonian at the equilibrium melting transition present in low
energy solid-type eigenstates. We derive a single (possibly computable)
dimensionless parameter fit to the viscosity and suggest other testable
predictions of our approach.
</p>
<br><br><br><p>We consider the bound states of a system consisting of a light particle and
two heavy bosonic ones, which are restricted in their quantum mechanical motion
to two space dimensions. A $p$-wave resonance in the heavy-light short-range
potential establishes the interaction between the two heavy particles. Due to
the large ratio of the atomic masses this planar three-body system is perfectly
suited for the Born-Oppenheimer approximation which predicts a Coulomb energy
spectrum with a Gaussian cut-off.
</p>
<br><br><br><p>A fundamental requirement in the circuit model of quantum information
processing is the realization of fault-tolerant multi-qubit quantum gates with
entangling capabilities. A key step towards this end is to achieve control of
qubit states through geometric phases at very small spatial scales in an
effective and feasible way. A spin-electric coupling present in
antiferromagnetic triangular single-molecule magnets (SMMs) allows for
manipulation of the spin (qubit) states with a great flexibility. Here, we
establish an all-electrical two-qubit geometric phase shift gate acting on the
four-fold ground state manifold of a triangular SMM, which represents an
effective two-qubit state space. We show that a two-qubit quantum gate with
arbitrary entangling power can be achieved through the Berry phase effect,
induced by adiabatically varying an external electric field in the plane of the
molecule.
</p>
<br><br><br><p>Via Gelfand duality, a unital C*-algebra $A$ induces a functor from compact
Hausdorff spaces to sets, $\mathsf{CHaus}\to\mathsf{Set}$. We show how this
functor encodes standard functional calculus in $A$ as well as its multivariate
generalization. Certain sheaf conditions satisfied by this functor provide a
further generalization of functional calculus. Considering such sheaves
$\mathsf{CHaus}\to\mathsf{Set}$ abstractly, we prove that the piecewise
C*-algebras of van den Berg and Heunen are equivalent to a full subcategory of
the category of sheaves, where a simple additional constraint characterizes the
objects in the subcategory. It is open whether this additional constraint holds
automatically, in which case piecewise C*-algebras would be the same as sheaves
$\mathsf{CHaus}\to\mathsf{Set}$.
</p>
<p>Intuitively, these structures capture the commutative aspects of C*-algebra
theory. In order to find a complete reaxiomatization of unital C*-algebras
within this language, we introduce almost C*-algebras as piecewise C*-algebras
equipped with a notion of inner automorphisms in terms of a self-action. We
provide some evidence for the conjecture that the forgetful functor from unital
C*-algebras to almost C*-algebras is fully faithful, and ask whether it is an
equivalence of categories. We also develop an analogous notion of almost group,
and prove that the forgetful functor from groups to almost groups is not full.
</p>
<p>In terms of quantum physics, our work can be seen as an attempt at a
reconstruction of quantum theory from physically meaningful axioms, as realized
by Hardy and others in a different framework. Our ideas are inspired by and
also provide new input for the topos-theoretic approach to quantum theory.
</p>
<br><br><br><p>This paper generalizes the Stefan-Boltzmann law to include massive photons. A
crucial ingredient to obtain the correct formula for the radiance is to realize
that a massive photon does not travel at the speed of (massless) light. It
follows that, contrary to what could be expected, the radiance is not
proportional to the energy density times the speed of light.
</p>
<br><br><br><p>Non-local games are an important part of quantum information processing.
Recently there has been an increased interest in generalizing non-local games
beyond the basic setup by considering games with multiple parties and/or with
large alphabet inputs and outputs. In this paper we consider another
interesting generalization -- games with non-uniform inputs. Here we derive a
tight upper bound for the classical winning probability for a specific family
of non-local games with non-uniform input distribution, known as
$\mathrm{CHSH}_q(p)$ which was introduced recently in the context of
relativistic bit-commitment protocols by [Chakraborty et. al., PRL 115, 250501,
2015].
</p>
<br><br><br><p>The ground state subspace of a topological phase of matter forms a
representation of the mapping class group of the space on which the state is
defined. We show that elements of the mapping class group of a surface of genus
$g$ can be obtained through a sequence of topological charge projections along
at least three mutually intersecting non-contractible cycles. We demonstrate
this both through the algebraic theory of anyons and also through an analysis
of the topology of the space-time manifold. We combine this result with two
observations: (i) that surfaces of genus $g$ can be effectively simulated in
planar geometries by using bilayer, or doubled, versions of the topological
phase of interest, and inducing the appropriate types of gapped boundaries; and
(ii) that the required topological charge projections can be implemented as
adiabatic unitary transformations by locally tuning microscopic parameters of
the system, such as the energy gap. These observations suggest a possible path
towards effectively implementing modular transformations in physical systems.
In particular, they also show how the $\text{Ising} \otimes
\overline{\text{Ising}}$ state, in the presence of disconnected gapped
boundaries, can support universal topological quantum computation.
</p>
<br><br><br><p>The environment of a quantum dot, which is connected to two leads, is modeled
by telegraph noise, i.e. random Markovian jumps of the (spinless) electron
energy on the dot between two levels. The temporal evolutions of the charge on
the dot and of the currents in the leads are studied using a recently developed
single-particle basis approach, which is particularly convenient for the
averaging over the histories of the noise. In the steady state limit we recover
the Landauer formula. At a very fast jump rate between the two levels, the
noise does not affect the transport. As the jump rate decreases, the effective
average transmission crosses over from the transmission through a single
(average) level to an incoherent sum of the transmissions through the two
levels. 13 pages, 6 figuresThe transient temporal evolution towards the steady
state is dominated by the displacement current at short times, and by the
Landauer current at long times. It contains oscillating terms, which decay to
zero faster than for the case without noise. When the average chemical
potential on the leads equals the dot's \"original\" energy, without the noise,
the oscillations disappear completely and the transient evolution becomes
independent of the noise.
</p>
<br><br><br><p>We report on the experimental realization of a photonic system that simulates
the one-dimensional two-particle Hubbard model. This analogy is realized by
means of two-dimensional arrays of coupled optical waveguides, fabricated using
femtosecond laser inscription. By tuning the analogous \"interaction strength\",
we reach the strongly-interacting regime of the Hubbard Hamiltonian, and
demonstrate the suppression of standard tunneling for individual \"particles\".
In this regime, the formation of bound states is identified through the direct
observation of pair tunneling. We then demonstrate the coherent destruction of
tunneling (CDT) for the paired particles in the presence of an engineered
oscillating force of high frequency. The precise control over the analogous
\"interaction strength\" and driving force offered by our experimental system
opens an exciting route towards quantum simulation of few-body physics in
photonics.
</p>
<br><br><br><p>Device concepts in semiconductor spintronics make long spin lifetimes
desirable, and the requirements put on spin control by schemes of quantum
information processing are even more demanding. Unfortunately, due to
spin-orbit coupling electron spins in semiconductors are generically subject to
rather fast decoherence. In two-dimensional quantum wells made of zinc-blende
semiconductors, however, the spin-orbit interaction can be engineered in such a
way that persistent spin structures with extraordinarily long spin lifetimes
arise even in the presence of disorder and imperfections. We review
experimental and theoretical developments on this subject both for $n$-doped
and $p$-doped structures, and we discuss possible device applications.
</p>
<br><br><br><p>The notion of a partition on a set is mathematically dual to the notion of a
subset of a set, so there is a logic of partitions dual to Boole's logic of
subsets (Boolean subset logic is usually mis-specified as the special case of
\"propositional\" logic). The notion of an element of a subset has as its dual
the notion of a distinction of a partition (a pair of elements in different
blocks). Boole developed finite logical probability as the normalized counting
measure on elements of subsets so there is a dual concept of logical entropy
which is the normalized counting measure on distinctions of partitions. Thus
the logical notion of information is a measure of distinctions. Classical
logical entropy also extends naturally to the notion of quantum logical entropy
which provides a more natural and informative alternative to the usual Von
Neumann entropy in quantum information theory. The quantum logical entropy of a
post-measurement density matrix has the simple interpretation as the
probability that two independent measurements of the same state using the same
observable will have different results. The main result of the paper is that
this increase in quantum logical entropy due to a projective measurement of a
pure state is the sum of the absolute squares of the off-diagonal entries
(`coherences') of the pure state density matrix that are zeroed (decohered) by
the measurement, i.e., the measure of the distinctions (`decoherences') created
by the measurement. The Von Neumann entropy provides no such analysis of
measurement. That result is also classically modelled using ordinary partitions
and density matrices for such partitions in the pedagogical model of QM/Sets
with point probabilities--which for a fixed basis is just sampling a real r.v.
in classical finite probability theory.
</p>
<br><br><br><p>Eigenstates of Bose particles with repulsive contact interactions in
one-dimensional space with periodic boundary conditions can be found with the
help of the Bethe ansatz. The type~II excitation spectrum identified by E. H.
Lieb, reproduces the dispersion relation of dark solitons in the mean-field
approach. The corresponding eigenstates possess translational symmetry which
can be broken in measurements of positions of particles. We analyze emergence
of single and double solitons in the course of the measurements and investigate
dynamics of the system. In the weak interaction limit, the system follows the
mean-field prediction for a short period of time. Long time evolution reveals
many-body effects that are related to an increasing uncertainty of soliton
positions. In the strong interaction regime particles behave like impenetrable
bosons. Then, the probability densities in the configuration space become
identical to the probabilities of non-interacting fermions but the
wave-functions themselves remember the original Bose statistics. Especially,
the phase flips that are key signatures of the solitons in the weak interaction
limit, can be observed in the time evolution of the strongly interacting
bosons.
</p>
<br><br><br><p>We consider the physics of an optomechanical cavity subject to coherent
two-phonon driving, i.e. degenerate parametric amplification of the mechanical
mode. We show that in such a system, the cavity mode can effectively \"inherit\"
parametric driving from the mechanics, yielding phase-sensitive amplification
and squeezing of optical signals reflected from the cavity. We also demonstrate
how such a system can be used to perform single-quadrature detection of a
near-resonant narrow-band force applied to the mechanics with extremely low
added noise from the optics. The system also exhibits strong differences from a
conventional degenerate parametric amplifier: in particular, the cavity
spectral function can become negative, indicating a negative effective photon
temperature.
</p>
<br><br><br><p>Building on work of Meixner [J. Meixner, Z. Naturforschung 3a, 506 (1948)],
we show how to compute the exact scattering amplitude (or $T$-matrix) for
electromagnetic scattering from a perfectly conducting disk. This calculation
is a rare example of a non-diagonal $T$-matrix that can nonetheless be obtained
in a semi-analytic form. We then use this result to compute the electromagnetic
Casimir interaction energy for a disk opposite a plane, for arbitrary
orientation angle of the disk, for separations greater than the disk radius. We
find that the proximity force approximation (PFA) significantly overestimates
the Casimir energy, both in the case of the ordinary PFA, which applies when
the disk is parallel to the plane, and the \"edge PFA,\" which applies when the
disk is perpendicular to the plane.
</p>
<br><br><br><p>We give a systematic review of the adiabatic theorem and the leading
non-adiabatic corrections in periodically-driven (Floquet) systems. These
corrections have a two-fold origin: (i) conventional ones originating from the
gradually changing Floquet Hamiltonian and (ii) corrections originating from
changing the micro-motion operator. These corrections conspire to give a
Hall-type linear response for non-stroboscopic (time-averaged) observables
allowing one to measure the Berry curvature and the Chern number related to the
Floquet Hamiltonian, thus extending these concepts to periodically-driven
many-body systems. The non-zero Floquet Chern number allows one to realize a
Thouless energy pump, where one can adiabatically add energy to the system in
discrete units of the driving frequency. We discuss the validity of Floquet
Adiabatic Perturbation Theory (FAPT) using five different models covering
linear and non-linear few and many-particle systems. We argue that in
interacting systems, even in the stable high-frequency regimes, FAPT breaks
down at ultra slow ramp rates due to avoided crossings of photon resonances,
not captured by the inverse-frequency expansion, leading to a counter-intuitive
stronger heating at slower ramp rates. Nevertheless, large windows in the ramp
rate are shown to exist for which the physics of interacting driven systems is
well captured by FAPT.
</p>
<br><br><br><p>Quantum secure communication brings a new direction for information security.
As an important component of quantum secure communication, deterministic joint
remote state preparation (DJRSP) could securely transmit a quantum state with
100\% success probability. In this paper, we study how the efficiency of DJRSP
is affected when qubits involved in the protocol are subjected to noise or
decoherence. Taking a GHZ based DJRSP scheme as an example, we study all types
of noise usually encountered in real-world implementations of quantum
communication protocols, i.e., the bit-flip, phase-flip (phase-damping),
depolarizing, and amplitude-damping noise. Our study shows that the fidelity of
the output state depends on the phase factor, the amplitude factor and the
noise parameter in the bit-flip noise, while the fidelity only depends on the
amplitude factor and the noise parameter in the other three types of noise. And
the receiver will get different output states depending on the first preparer's
measurement result in the amplitude-damping noise. Our results will be helpful
for improving quantum secure communication in real implementation.
</p>
<br><br><br><p>We analyze the simulation of Dirac neutrino oscillations using quantum walks,
both in vacuum and in matter. We show that this simulation, in the continuum
limit, reproduces a set of coupled Dirac equations that describe neutrino
flavor oscillations, and we make use of this to establish a connection with
neutrino phenomenology, thus allowing to fix the parameters of the simulation
for a given neutrino experiment. We also analyze how matter effects for
neutrino propagation can be simulated in the quantum walk. In this way,
important features, such as the MSW effect, can be incorporated. Thus, the
simulation of neutrino oscillations with the help of quantum walks might be
useful to explore these effects in extreme conditions, such as the solar
interior or supernovae, in a complementary way to existing experiments.
</p>
<br><br><br><p>Instability-induced random branching of deterministic dynamics is discussed
as a possible mechanism of random wave function collapse. In the case of two
level systems, the Born probability rule emerges as the simplest linear
solution to the equation for measurement probabilities.
</p>
<br><br><br><p>When classically searching a database, having additional correct answers
makes the search easier. For a discrete-time quantum walk searching a graph for
a marked vertex, however, additional marked vertices can make the search harder
by causing the system to approximately begin in a stationary state, so the
system fails to evolve. In this paper, we completely characterize the
stationary states, or 1-eigenvectors, of the quantum walk search operator for
general graphs and configurations of marked vertices by decomposing their
amplitudes into uniform and flip states. This infinitely expands the number of
known stationary states and gives an optimization procedure to find the
stationary state closest to the initial uniform state of the walk. We further
prove theorems on the existence of stationary states, with them conditionally
existing if the marked vertices form a bipartite connected component and always
existing if non-bipartite. These results utilize the standard oracle in
Grover's algorithm, but we show that a different type of oracle prevents
stationary states from interfering with the search algorithm.
</p>
<br><br><br><p>It was proposed recently that the out-of-time-ordered four-point correlator
(OTOC) may serve as a useful characteristic of quantum-chaotic behavior,
because in the semi-classical limit, $\hbar \to 0$, its rate of exponential
growth resembles the classical Lyapunov exponent. Here, we calculate the
four-point correlator, $C(t)$, for the classical and quantum kicked rotor -- a
textbook driven chaotic system -- and compare its growth rate at initial times
with the standard definition of the classical Lyapunov exponent. Using both
quantum and classical arguments, we show that the OTOC's growth rate and the
Lyapunov exponent are in general distinct quantities, corresponding to the
logarithm of phase-space averaged divergence rate of classical trajectories and
to the phase-space average of the logarithm, respectively. The difference
appears to be more pronounced in the regime of low kicking strength, where no
classical chaos exists globally. In this case, the Lyapunov exponent vanishes,
while the OTOC's growth rate may remain finite corresponding to disconnected
local chaotic islands in the phase space. We also show that the quantum
correlator as a function of time exhibits a clear singularity at the Ehrenfest
time $t_E$: transitioning from a time-independent value of $t^{-1} \ln{C(t)}$
at $t &lt; t_E$ to its monotonous decrease with time at $t&gt;t_E$. We note that the
underlying physics here is the same as in the theory of weak (dynamical)
localization [Aleiner and Larkin, Phys. Rev. B 54, 14423 (1996); Tian, Kamenev,
and Larkin, Phys. Rev. Lett. 93, 124101 (2004)] and is due to a delay in the
onset of quantum interference effects, which occur sharply at a time of the
order of the Ehrenfest time.
</p>
<br><br><br><p>Quantum illumination (QI) is an entanglement-enhanced sensing scheme for the
thermal-noise bosonic channel that discriminates between a target's absence or
presence with 6 dB better error-probability exponent than a classical
(coherent-state) sensor of the same average transmitted photon number.
Moreover, it does so despite the initial entanglement's being completely broken
by loss and noise. Unfortunately, a realization of the optimum quantum receiver
for QI sensing remains elusive, owing to its necessity of dealing with a huge
number of very noisy optical modes. We propose exploiting sum-frequency
generation (SFG) to realize such an optimum receiver. Analysis and numerical
evaluations confirm that our SFG receiver reaches QI's quantum Chernoff bound.
Augmenting our SFG receiver with a feed-forward (FF) mechanism, we achieve the
Helstrom bound in the weak-signal limit. The FF-SFG receiver thus realizes
optimum multi-mode Gaussian mixed-state discrimination, opening the door to
optimum quantum-enhanced sensing in practical situations.
</p>
<br><br><br><p>We propose a Berry phase effect on the chiral degrees of freedom of a
triangular single-molecule magnet (SMM). The phase is induced by adiabatically
varying an external electric field in the plane of the molecule via a
spin-electric coupling mechanism present in these frustrated SMMs. The Berry
phase effect depends on the spin-orbit interaction splitting and on the
electric dipole moment. By varying the amplitude of the applied electric field,
the Berry phase difference between the two spin states can take any arbitrary
value between 0 and $\pi$, which can be measured as a phase shift between the
two chiral states by using spin-echo techniques. Our result can be used to
realize an electric field induced geometric phase-shift gate acting on a chiral
qubit encoded in the ground state manifold of the SMM.
</p>
<br><br><br><p>The dynamics of Fisher information for an accelerated system initially
prepared in the $X$-state is discussed. An analytical solution, which consists
of three parts: classical, the average over all pure states and a mixture of
pure states is derived for the general state and for Werner state. It is shown
that, the Unruh acceleration has a depleting effect on the Fisher information.
This depletion depends on the degree of entanglement of the initial state
settings. For the $X$-state, for some intervals of Unruh acceleration, the
Fisher information remains constant, irrespective to the Unruh acceleration. In
general, the possibility of estimating the state's parameters decreases as the
acceleration increases. However, the precision of estimation can be maximized
for certain values of the Unruh acceleration. We also investigate the
contribution of the different parts of the Fisher information on the dynamics
of the total Fisher information.
</p>
<br><br><br><p>Various methods to automate traffic data collection have recently been
developed by many researchers. A macroscopic data collection through image
processing has been proposed. For microscopic traffic flow data, such as
individual speed and time or distance headway, tracking of individual movement
is needed. The tracking algorithms for pedestrian or vehicle have been
developed to trace the movement of one or two pedestrians based on sign
pattern, and feature detection. No research has been done to track many
pedestrians or vehicles at once. This paper describes a new and fast algorithm
to track the movement of many individual vehicles or pedestrians
</p>
<br><br><br><p>We consider a non-stationary formulation of the stochastic multi-armed bandit
where the rewards are no longer assumed to be identically distributed. For the
best-arm identification task, we introduce a version of Successive Elimination
based on random shuffling of the $K$ arms. We prove that under a novel and mild
assumption on the mean gap $\Delta$, this simple but powerful modification
achieves the same guarantees in term of sample complexity and cumulative regret
than its original version, but in a much wider class of problems, as it is not
anymore constrained to stationary distributions. We also show that the original
{\sc Successive Elimination} fails to have controlled regret in this more
general scenario, thus showing the benefit of shuffling. We then remove our
mild assumption and adapt the algorithm to the best-arm identification task
with switching arms. We adapt the definition of the sample complexity for that
case and prove that, against an optimal policy with $N-1$ switches of the
optimal arm, this new algorithm achieves an expected sample complexity of
$O(\Delta^{-2}\sqrt{NK\delta^{-1} \log(K \delta^{-1})})$, where $\delta$ is the
probability of failure of the algorithm, and an expected cumulative regret of
$O(\Delta^{-1}{\sqrt{NTK \log (TK)}})$ after $T$ time steps.
</p>
<br><br><br><p>In this paper we study the effect on crowd worker efficiency and
effectiveness of the dominance of one class in the data they process. We aim at
understanding if there is any positive or negative bias in workers seeing many
negative examples in the identification of positive labels. To test our
hypothesis, we design an experiment where crowd workers are asked to judge the
relevance of documents presented in different orders. Our findings indicate
that there is a significant improvement in the quality of relevance judgements
when presenting relevant results before the non-relevant ones.
</p>
<br><br><br><p>This paper considers the distributed sampled-data control problem of a group
of mobile robots connected via distance-induced proximity networks. A dwell
time is assumed in order to avoid chattering in the neighbor relations that may
be caused by abrupt changes of positions when updating information from
neighbors. Distributed sampled-data control laws are designed based on nearest
neighbour rules, which in conjunction with continuous-time dynamics results in
hybrid closed-loop systems. For uniformly and independently initial states, a
sufficient condition is provided to guarantee synchronization for the system
without leaders. In order to steer all robots to move with the desired
orientation and speed, we then introduce a number of leaders into the system,
and quantitatively establish the proportion of leaders needed to track either
constant or time-varying signals. All these conditions depend only on the
neighborhood radius, the maximum initial moving speed and the dwell time,
without assuming a prior properties of the neighbor graphs as are used in most
of the existing literature.
</p>
<br><br><br><p>The present research examines two problems inherent to the creation of
crowdsourcing games: how to give feedback when the right answer is not always
known by the game and how much time to give players without sacrificing data
quality. Taken together, the present research provides an important first step
in considering how to create fun, challenging crowdsourcing games that generate
quality data.
</p>
<br><br><br><p>We study the dynamics of an online algorithm for learning a sparse leading
eigenvector from samples generated from a spiked covariance model. This
algorithm combines the classical Oja's method for online PCA with an
element-wise nonlinearity at each iteration to promote sparsity. In the
high-dimensional limit, the joint empirical measure of the underlying sparse
eigenvector and its estimate provided by the algorithm is shown to converge
weakly to a deterministic, measure-valued process. This scaling limit is
characterized as the unique solution of a nonlinear PDE, and it provides exact
information regarding the asymptotic performance of the algorithm. For example,
performance metrics such as the cosine similarity and the misclassification
rate in sparse support recovery can be obtained by examining the limiting
dynamics. A steady-state analysis of the nonlinear PDE also reveals an
interesting phase transition phenomenon. Although our analysis is asymptotic in
nature, numerical simulations show that the theoretical predictions are
accurate for moderate signal dimensions.
</p>
<br><br><br><p>Energy transparency is a concept that makes a program's energy consumption
visible from hardware up to software, through the different system layers. Such
transparency can enable energy optimizations at each layer and between layers,
and help both programmers and operating systems make energy-aware decisions. In
this paper, we focus on deeply embedded devices, typically used for Internet of
Things (IoT) applications, and demonstrate how to enable energy transparency
through existing Static Resource Analysis (SRA) techniques and a new
target-agnostic profiling technique, without the need of hardware energy
measurements. A novel mapping technique enables software energy consumption
estimations at a higher level than the Instruction Set Architecture (ISA),
namely the LLVM Intermediate Representation (IR) level, and therefore
introduces energy transparency directly to the LLVM optimizer. We apply our
energy estimation techniques to a comprehensive set of benchmarks, including
single-threaded and also multi-threaded embedded programs from two commonly
used concurrency patterns, task farms and pipelines. Using SRA, our LLVM IR
results demonstrate a high accuracy with a deviation in the range of 1\% from
the ISA SRA. Our profiling technique captures the actual energy consumption at
the LLVM IR level with an average error of less than 3\%.
</p>
<br><br><br><p>Many security techniques working at the physical layer need a correct channel
state information (CSI) at the transmitter, especially when devices are
equipped with multiple antennas. Therefore such techniques are vulnerable to
pilot contamination attacks (PCAs) by which an attacker aims at inducing false
CSI. In this paper we provide a solution to some PCA methods, by letting two
legitimate parties to compare their channel estimates. The comparison is made
in order to minimize the information leakage on the channel to a possible
attacker. By reasonable assumptions on both the channel knowledge by the
attacker and the correlation properties of the attacker and legitimate channels
we show the validity of our solution. An accurate analysis of possible attacks
and countermeasures is provided, together with a numerical evaluation of the
attainable secrecy outage probability when our solution is used in conjunction
with beamforming for secret communications.
</p>
<br><br><br><p>In this contribution, we introduce an efficient method for solving the
optimal control problem for an unconstrained nonlinear switched system with an
arbitrary cost function. We assume that the sequence of the switching modes are
given but the switching time in between consecutive modes remains to be
optimized. The proposed method uses a two--stage approach as introduced in [1]
where the original optimal control problem is transcribed into an equivalent
problem parametrized by the switching times and the optimal control policy is
obtained based on the solution of a two point boundary value differential
algebraic equation. The main contribution of this paper is to use a Sequential
Linear Quadratic approach to synthesize the optimal controller instead of
solving a boundary value problem. The proposed method is numerically more
efficient and scales very well to the high-dimension problems. In order to
evaluate its performance, we use two numerical examples as benchmarks to
compare against the baseline algorithm introduced in [1]. In the third
numerical example, we apply the proposed algorithm to the Center of Mass
control problem in a quadruped locomotion task.
</p>
<br><br><br><p>Probabilistic models with discrete latent variables naturally capture
datasets composed of discrete classes. However, they are difficult to train
efficiently, since backpropagation through discrete variables is generally not
possible. We introduce a novel class of probabilistic models, comprising an
undirected discrete component and a directed hierarchical continuous component,
that can be trained efficiently using the variational autoencoder framework.
The discrete component captures the distribution over the disconnected smooth
manifolds induced by the continuous component. As a result, this class of
models efficiently learns both the class of objects in an image, and their
specific realization in pixels, from unsupervised data; and outperforms
state-of-the-art methods on the permutation-invariant MNIST, OMNIGLOT, and
Caltech-101 Silhouettes datasets.
</p>
<br><br><br><p>Estimators of information theoretic measures such as entropy and mutual
information are a basic workhorse for many downstream applications in modern
data science. State of the art approaches have been either geometric (nearest
neighbor (NN) based) or kernel based (with a globally chosen bandwidth). In
this paper, we combine both these approaches to design new estimators of
entropy and mutual information that outperform state of the art methods. Our
estimator uses local bandwidth choices of $k$-NN distances with a finite $k$,
independent of the sample size. Such a local and data dependent choice improves
performance in practice, but the bandwidth is vanishing at a fast rate, leading
to a non-vanishing bias. We show that the asymptotic bias of the proposed
estimator is universal; it is independent of the underlying distribution.
Hence, it can be pre-computed and subtracted from the estimate. As a byproduct,
we obtain a unified way of obtaining both kernel and NN estimators. The
corresponding theoretical contribution relating the asymptotic geometry of
nearest neighbors to order statistics is of independent mathematical interest.
</p>
<br><br><br><p>Optical coherence tomography (OCT) is a non-invasive imaging technique that
can produce images of the eye at the microscopic level. OCT image segmentation
to localise retinal layer boundaries is a fundamental procedure for diagnosing
and monitoring the progression of retinal and optical nerve disorders. In this
paper, we introduce a novel and accurate geodesic distance method (GDM) for OCT
segmentation of both healthy and pathological images in either two- or
three-dimensional spaces. The method uses a weighted geodesic distance by an
exponential function, taking into account both horizontal and vertical
intensity variations. The weighted geodesic distance is efficiently calculated
from an Eikonal equation via the fast sweeping method. The segmentation is then
realised by solving an ordinary differential equation with the geodesic
distance. The results of the GDM are compared with manually segmented retinal
layer boundaries/surfaces. Extensive experiments demonstrate that the proposed
GDM is robust to complex retinal structures with large curvatures and
irregularities and it outperforms the parametric active contour algorithm as
well as the graph theoretic based approaches for delineating the retinal layers
in both healthy and pathological images.
</p>
<br><br><br><p>Though deep learning has pushed the boundaries of classification forward, in
recent years hints of the limits of standard classification have begun to
emerge. Problems such as fooling, adding new classes over time, and the need to
retrain learning models only for small changes to the original problem all
point to a potential shortcoming in the classic classification regime, where a
comprehensive a priori knowledge of the possible classes or concepts is
critical. Without such knowledge, classifiers misjudge the limits of their
knowledge and overgeneralization therefore becomes a serious obstacle to
consistent performance. In response to these challenges, this paper extends the
classic regime by reframing classification instead with the assumption that
concepts present in the training set are only a sample of the hypothetical
final set of concepts. To bring learning models into this new paradigm, a novel
elaboration of standard architectures called the competitive overcomplete
output layer (COOL) neural network is introduced. Experiments demonstrate the
effectiveness of COOL by applying it to fooling, separable concept learning,
one-class neural networks, and standard classification benchmarks. The results
suggest that, unlike conventional classifiers, the amount of generalization in
COOL networks can be tuned to match the problem.
</p>
<br><br><br><p>There is a paucity of random access protocols designed for alleviating
collisions in visible light communication (VLC) systems, where carrier sensing
is hard to be achieved due to the directionality of light. To resolve the
problem of collisions, we adopt the successive interference cancellation (SIC)
algorithm to enable the coordinator to simultaneously communicate with multiple
devices, which is referred to as the multi-packet reception (MPR) capability.
However, the MPR capability could be fully utilized only when random access
algorithms are properly designed. Considering the characteristics of the SIC
aided random access VLC system, we propose a novel effective capacity
(EC)-based ALOHA-like distributed random access algorithm for MPR-aided uplink
VLC systems having heterogeneous quality-of-service (QoS) guarantees. Firstly,
we model the VLC network as a conflict graph and derive the EC for each device.
Then, we formulate the VLC QoS-guaranteed random access problem as a saturation
throughput maximization problem subject to multiple statistical QoS
constraints. Finally, the resultant non-concave optimization problem (OP) is
solved by a memetic search algorithm relying on invasive weed optimization and
differential evolution (IWO-DE). We demonstrate that our derived EC expression
matches the Monte Carlo simulation results accurately, and the performance of
our proposed algorithms is competitive.
</p>
<br><br><br><p>Hebbian plasticity allows biological agents to learn from their lifetime
experience, extending the fixed information provided by evolutionary search.
Conversely, backpropagation methods can build high-performance fixed-weights
networks, but are not currently equipped to design networks with Hebbian
connections. Here we use backpropagation to train fully-differentiable plastic
networks, such that backpropagation determines not only the baseline weights,
but also the plasticity of each connection. To perform this backpropagation of
Hebbian plasticity (BOHP), we derive error gradients for neural networks with
Hebbian plastic connections. The equations for these gradients turn out to
follow a simple, recursive form. We apply this method to train small networks
for simple learning tasks inspired from classical conditioning. We show that,
through Hebbian plasticity, the networks perform fast learning of unpredictable
environmental features during their lifetime, successfully solving a task that
fixed-weight feedforward networks cannot possibly solve. We conclude that
backpropagation of Hebbian plasticity offers a powerful model for lifelong
learning.
</p>
<br><br><br><p>Nowadays, auto insurance companies set personalized insurance rate based on
data gathered directly from their customers' cars. In this paper, we show such
a personalized insurance mechanism -- wildly adopted by many auto insurance
companies -- is vulnerable to exploit. In particular, we demonstrate that an
adversary can leverage off-the-shelf hardware to manipulate the data to the
device that collects drivers' habits for insurance rate customization and
obtain a fraudulent insurance discount. In response to this type of attack, we
also propose a defense mechanism that escalates the protection for insurers'
data collection. The main idea of this mechanism is to augment the insurer's
data collection device with the ability to gather unforgeable data acquired
from the physical world, and then leverage these data to identify manipulated
data points. Our defense mechanism leveraged a statistical model built on
unmanipulated data and is robust to manipulation methods that are not foreseen
previously. We have implemented this defense mechanism as a proof-of-concept
prototype and tested its effectiveness in the real world. Our evaluation shows
that our defense mechanism exhibits a false positive rate of 0.032 and a false
negative rate of 0.013.
</p>
<br><br><br><p>Probabilistic modeling is one of the foundations of modern machine learning
and artificial intelligence. In this paper, we propose a novel type of
probabilistic models named latent dependency forest models (LDFMs). A LDFM
models the dependencies between random variables with a forest structure that
can change dynamically based on the variable values. It is therefore capable of
modeling context-specific independence. We parameterize a LDFM using a
first-order non-projective dependency grammar. Learning LDFMs from data can be
formulated purely as a parameter learning problem, and hence the difficult
problem of model structure learning is circumvented. Our experimental results
show that LDFMs are competitive with existing probabilistic models.
</p>
<br><br><br><p>One of the objectives of understanding pedestrian behavior is to predict the
effect of proposed changes in the design or evaluation of pedestrian
facilities. We want to know the impact to the user of the facilities, as the
design of the facilities change. That impact was traditionally evaluated by
level of service standards. Another design criterion to measure the impact of
design change is measured by the pedestrian flow performance index. This paper
describes the determination of pedestrian flow performance based video tracking
or any microscopic pedestrian simulation models. Most of pedestrian researches
have been done on a macroscopic level, which is an aggregation of all
pedestrian movement in pedestrian areas into flow, average speed and area
module. Macroscopic level, however, does not consider the interaction between
pedestrians. It is also not well suited for prediction of pedestrian flow
performance in pedestrian areas or in buildings with some obstruction, that
reduces the effective width of the walkways. On the other hand, the microscopic
level has a more general usage and considers detail in the design. More
efficient pedestrian flow can even be reached with less space. Those results
have rejected the linearity assumption of space and flow in the macroscopic
level.
</p>
<br><br><br><p>Recently we proposed relative observability for supervisory control of
discrete-event systems under partial observation. Relative observability is
closed under set unions and hence there exists the supremal relatively
observable sublanguage of a given language. In this paper we present a new
characterization of relative observability, based on which an operator on
languages is proposed whose largest fixpoint is the supremal relatively
observable sublanguage. Iteratively applying this operator yields a monotone
sequence of languages; exploiting the linguistic concept of support based on
Nerode equivalence, we prove for regular languages that the sequence converges
finitely to the supremal relatively observable sublanguage, and the operator is
effectively computable. Moreover, for the purpose of control, we propose a
second operator that in the regular case computes the supremal relatively
observable and controllable sublanguage. The computational effectiveness of the
operator is demonstrated on a case study.
</p>
<br><br><br><p>Understanding the real achievable performance of mobile ad hoc networks
(MANETs) under practical network constraints is of great importance for their
applications in future highly heterogeneous wireless network environments. This
paper explores, for the first time, the performance modeling for MANETs under a
general limited buffer constraint, where each network node maintains a limited
source buffer of size $B_s$ to store its locally generated packets and also a
limited shared relay buffer of size $B_r$ to store relay packets for other
nodes. Based on the Queuing theory and birth-death chain theory, we first
develop a general theoretical framework to fully depict the source/relay buffer
occupancy process in such a MANET, which applies to any distributed MAC
protocol and any mobility model that leads to the uniform distribution of
nodes' locations in steady state. With the help of this framework, we then
derive the exact expressions of several key network performance metrics,
including achievable throughput, throughput capacity, and expected end-to-end
delay. We further conduct case studies under two network scenarios and provide
the corresponding theoretical/simulation results to demonstrate the application
as well as the efficiency of our theoretical framework. Finally, we present
extensive numerical results to illustrate the impacts of buffer constraint on
the performance of a buffer-limited MANET.
</p>
<br><br><br><p>Generalized matrix approximation plays a fundamental role in many machine
learning problems, such as CUR decomposition, kernel approximation, and matrix
low rank approximation. Especially with today's applications involved in larger
and larger dataset, more and more efficient generalized matrix approximation
algorithems become a crucially important research issue. In this paper, we find
new sketching techniques to reduce the size of the original data matrix to
develop new matrix approximation algorithms. Our results derive a much tighter
bound for the approximation than previous works: we obtain a $(1+\epsilon)$
approximation ratio with small sketched dimensions which implies a more
efficient generalized matrix approximation.
</p>
<br><br><br><p>In this paper, we present an end-to-end machine-human image annotation system
where each component can be attached in a plug-and-play fashion. These
components include Feature Extraction, Machine Annotation, Task Sampling and
Crowd Consensus.
</p>
<br><br><br><p>In the classical context of robotic mapping and localization, map matching is
typically defined as the task of finding a rigid transformation (i.e., 3DOF
rotation/translation on the 2D moving plane) that aligns the query and
reference maps built by mobile robots. This definition is valid in loop-rich
trajectories that enable a mapper robot to close many loops, for which precise
maps can be assumed. The same cannot be said about the newly emerging
autonomous navigation and driving systems, which typically operate in loop-less
trajectories that have no large loop (e.g., straight paths). In this paper, we
propose a solution that overcomes this limitation by merging the two maps. Our
study is motivated by the observation that even when there is no large loop in
either the query or reference map, many loops can often be obtained in the
merged map. We add two new aspects to map matching: (1) image retrieval with
discriminative deep convolutional neural network (DCNN) features, which
efficiently generates a small number of good initial alignment hypotheses; and
(2) map merge, which jointly deforms the two maps to minimize differences in
shape between them. To realize practical computation time, we also present a
preemption scheme that avoids excessive evaluation of useless map-matching
hypotheses. To verify our approach experimentally, we created a novel
collection of uncertain loop-less maps by utilizing the recently published
North Campus Long-Term (NCLT) dataset and its ground-truth GPS data. The
results obtained using these map collections confirm that our approach improves
on previous map-matching approaches.
</p>
<br><br><br><p>Action classification in still images has been a popular research topic in
computer vision. Labelling large scale datasets for action classification
requires tremendous manual work, which is hard to scale up. Besides, the action
categories in such datasets are pre-defined and vocabularies are fixed. However
humans may describe the same action with different phrases, which leads to the
difficulty of vocabulary expansion for traditional fully-supervised methods. We
observe that large amounts of images with sentence descriptions are readily
available on the Internet. The sentence descriptions can be regarded as weak
labels for the images, which contain rich information and could be used to
learn flexible expressions of action categories. We propose a method to learn
an Action Concept Tree (ACT) and an Action Semantic Alignment (ASA) model for
classification from image-description data via a two-stage learning process. A
new dataset for the task of learning actions from descriptions is built.
Experimental results show that our method outperforms several baseline methods
significantly.
</p>
<br><br><br><p>Base station switching (BSS) can results in significant reduction in energy
consumption of cellular networks during low traffic conditions. We show that
the coverage loss due to BSS can be compensated via coordinated multi-point
(CoMP) based transmission in a cluster of base stations. For a BSS with CoMP
based system, we propose various BSS patterns to achieve suitable trade-off
between energy efficiency and throughput. We formulate the CoMP resource
allocation and $\alpha$-Fair user scheduling as a joint optimization problem.
We derive the optimal time fraction and user scheduling for this problem. We
utilize these results to formulate the BSS with CoMP as an optimization
problem. A heuristic that solves this problem for a given rate threshold is
presented. Through extensive simulations, we show that suitable trade-offs
among energy, coverage, and rate can be achieved by appropriately selecting the
BSS pattern, CoMP cluster, and rate threshold.
</p>
<br><br><br><p>The application of physical layer security in wireless ad hoc networks
(WANETs) has attracted considerable academic attention recently. However, the
available studies mainly focus on the single-hop and two-hop network scenarios,
and the price in terms of degradation of communication quality of service (QoS)
caused by improving security is largely uninvestigated. As a step to address
these issues, this paper explores the physical layer security-aware routing and
performance tradeoffs in a multi-hop WANET. Specifically, for any given
end-to-end path in a general multi-hop WANET, we first derive its connection
outage probability (COP) and secrecy outage probability (SOP) in closed-form,
which serve as the performance metrics of communication QoS and transmission
security, respectively. Based on the closed-form expressions, we then study the
QoS-security tradeoffs to minimize COP (resp. SOP) conditioned on that SOP
(resp. COP) is guaranteed. With the help of analysis of a given path, we
further propose the routing algorithms which can achieve the optimal
performance tradeoffs for any pair of source and destination nodes in a
distributed manner. Finally, simulation and numerical results are presented to
validate the efficiency of our theoretical analysis, as well as to illustrate
the QoS-security tradeoffs and the routing performance.
</p>
<br><br><br><p>The problem of Carrier Sense Multiple Access (CSMA) with multi-packet
reception (MPR) is studied. Most prior work has focused on the homogeneous
case, where all the mobile users are assumed to have identical packet arrival
rates and transmission probabilities. The inhomogeneous case remains largely
open in the literature. In this work, we make a first step towards this open
problem by deriving throughput and delay expressions for inhomogeneous CSMA,
with a particular focus on a family of MPR models called the \"all-or-nothing\"
symmetric MPR. This family of MPR models allows us to overcome several
technical challenges associated with conventional analysis and to derive
accurate throughput and delay expressions in the large-systems regime.
Interestingly, this family of MPR models is still general enough to include a
number of useful MPR techniques - such as successive interference cancellation
(SIC), compute-and-forward (C&amp;F), and successive compute-and-forward (SCF) - as
special cases. Based on these throughput and delay expressions, we provide
theoretical guidelines for meeting quality-of-service requirements and for
achieving global stability; we also evaluate the performances of various MPR
techniques, highlighting the clear advantages offered by SCF.
</p>
<br><br><br><p>This article provides a new type of analysis of a compressed-sensing based
technique for recovering column-sparse matrices, namely minimization of the
$\ell_{1,2}$-norm. Rather than providing conditions on the measurement matrix
which guarantees the solution of the program to be exactly equal to the ground
truth signal (which already has been thoroughly investigated), it presents a
condition which guarantees that the solution is approximately equal to the
ground truth. Soft recovery statements of this kind are to the best knowledge
of the author a novelty in Compressed Sensing. Apart from the theoretical
analysis, we present two heuristic proposes how this property of the
$\ell_{1,2}$-program can be utilized to design algorithms for recovery of
matrices which are sparse and have low rank at the same time.
</p>
<br><br><br><p>Computer networks have become a critical infrastructure. Designing dependable
computer networks however is challenging, as such networks should not only meet
strict requirements in terms of correctness, availability, and performance, but
they should also be flexible enough to support fast updates, e.g., due to a
change in the security policy, an increasing traffic demand, or a failure. The
advent of Software-Defined Networks (SDNs) promises to provide such
flexiblities, allowing to update networks in a fine-grained manner, also
enabling a more online traffic engineering. In this paper, we present a
structured survey of mechanisms and protocols to update computer networks in a
fast and consistent manner. In particular, we identify and discuss the
different desirable update consistency properties a network should provide, the
algorithmic techniques which are needed to meet these consistency properties,
their implications on the speed and costs at which updates can be performed. We
also discuss the relationship of consistent network update problems to classic
algorithmic optimization problems. While our survey is mainly motivated by the
advent of Software-Defined Networks (SDNs), the fundamental underlying problems
are not new, and we also provide a historical perspective of the subject.
</p>
<br><br><br><p>We calculate the probability that random polynomial matrices over a finite
field with certain structures are right prime or left prime, respectively. In
particular, we give an asymptotic formula for the probability that finitely
many nonsingular polynomial matrices are mutually left coprime. These results
are used to estimate the number of reachable and observable linear systems as
well as the number of non-catastrophic convolutional codes. Moreover, we are
able to achieve an asymptotic formula for the probability that a parallel
connected linear system is reachable.
</p>
<br><br><br><p>This paper introduces the revival of the popular Ms. Pac-Man Versus Ghost
Team competition. We present an updated game engine with Partial Observability
constraints, a new Multi-Agent Systems approach to developing Ghost agents and
several sample controllers to ease the development of entries. A restricted
communication protocol is provided for the Ghosts, providing a more challenging
environment than before. The competition will debut at the IEEE Computational
Intelligence and Games Conference 2016. Some preliminary results showing the
effects of Partial Observability and the benefits of simple communication are
also presented.
</p>
<br><br><br><p>The channel law for amplitude-modulated solitons transmitted through a
nonlinear optical fibre with ideal distributed amplification and a receiver
based on the nonlinear Fourier transform is a noncentral chi distribution with
$2n$ degrees of freedom, where $n=2$ and $n=3$ correspond to the single- and
dual-polarisation cases, respectively. In this paper, we study the capacity of
this channel in bits per channel use, and develop a semi-analytic capacity
lower bound for arbitrary $n$ and a Rayleigh input distribution. An asymptotic
analysis of the bound is also presented, which shows that this lower bound
grows logarithmically with signal-to-noise ratio (SNR), independently of the
value of $n$. Numerical results for other input distributions are also
provided. A half-Gaussian input distribution is shown to give larger rates than
a Rayleigh input distribution for $n=1,2,3$. At an effective SNR of 30~dB, the
obtained lower bounds are approximately 4 bit per channel use.
</p>
<br><br><br><p>This paper presents a distributed agent-based automated theorem proving
framework based on order-sorted first-order logic. Each agent in our framework
has its own knowledge base, communicating to its neighboring agent(s) using
message-passing algorithms. The communication language between agents is
restricted in such a manner that each agent can only communicate to its
neighboring agent(s) by means of their common language. In this paper we
provide a refutation-complete report procedure for automated theorem proving in
order-sorted first-order logic in a subclass of distributed agent-based
networks. Rather than studying and evaluating the performance improvement of
the automated theorem proving in order-sorted first-order logic using parallel
or distributed agents, this paper focuses on building proofs in order-sorted
first-order logic in a distributed manner under the restriction that agents may
report their knowledge or observations only with their predefined language.
</p>
<br><br><br><p>Computer networks today typically do not provide any mechanisms to the users
to learn, in a reliable manner, which paths have (and have not) been taken by
their packets. Rather, it seems inevitable that as soon as a packet leaves the
network card, the user is forced to trust the network provider to forward the
packets as expected or agreed upon. This can be undesirable, especially in the
light of today's trend toward more programmable networks: after a successful
cyber attack on the network management system or Software-Defined Network (SDN)
control plane, an adversary in principle has complete control over the network.
</p>
<p>This paper presents a low-cost and efficient solution to detect misbehaviors
and ensure trustworthy routing over untrusted or insecure providers, in
particular providers whose management system or control plane has been
compromised (e.g., using a cyber attack). We propose
Routing-Verification-as-a-Service (RVaaS): RVaaS offers clients a flexible
interface to query information relevant to their traffic, while respecting the
autonomy of the network provider. RVaaS leverages key features of
OpenFlow-based SDNs to combine (passive and active) configuration monitoring,
logical data plane verification and actual in-band tests, in a novel manner.
</p>
<br><br><br><p>The application of psychophysiological in human-computer interaction is a
growing field with significant potential for future smart personalised systems.
Working in this emerging field requires comprehension of an array of
physiological signals and analysis techniques.
</p>
<p>Stream processing sytems (SPS) are emerging computational platforms that can
be utilized in human-computer interaction for real-time analysis of high-volume
multimodal signals. Usage of complementary information contained in multiple
signals is desireable as it can make HCI systems more robust. In this preprint
we review existing software and hardware solutions for HCI-centric stream
processing systems. The preprint also includes a brief introduction into SPS
design considerations and structure from the perspective of analysing
physiological signals.
</p>
<p>This paper aims to serve as a primer for the novice, enabling rapid
familiarisation with the latest core concepts. We put special emphasis on
everyday human-computer interface applications to distinguish from the more
common clinical or sports uses of psychophysiology.
</p>
<p>This paper is an extract from a comprehensive review of the entire field of
ambulatory psychophysiology, including 12 similar chapters, plus application
guidelines and systematic review. Thus any citation should be made using the
following reference: B. Cowley, M. Filetti, K. Lukander, J. Torniainen, A.
Henelius, L. Ahonen, O. Barral, I. Kosunen, T. Valtonen, M. Huotilainen, N.
Ravaja, G. Jacucci. The Psychophysiology Primer: a guide to methods and a broad
review with a focus on human-computer interaction. Foundations and Trends in
Human-Computer Interaction, vol. 9, no. 3-4, pp. 150--307, 2016.
</p>
<br><br><br><p>In this paper, we introduce new Stein identities for gamma target
distribution as well as a new non-linear channel specifically designed for
gamma inputs. From these two ingredients, we derive an explicit and simple
formula for the derivative of the input-output mutual information of this
non-linear channel with respect to the channel quality parameter. This relation
is reminiscent of the well-known link between the derivative of the
input-output mutual information of additive Gaussian noise channel with respect
to the signal-to-noise ratio and the minimum mean-square error. The proof
relies on a rescaled version of De Bruijn identity for gamma target
distribution together with a stochastic representation for the gamma specific
Fisher information. Finally, we are able to derive precise bounds and
asymptotics for the input-output mutual information of the non-linear channel
with gamma inputs.
</p>
<br><br><br><p>The 911 emergency service belongs to one of the 16 critical infrastructure
sectors in the United States. Distributed denial of service (DDoS) attacks
launched from a mobile phone botnet pose a significant threat to the
availability of this vital service. In this paper we show how attackers can
exploit the cellular network protocols in order to launch an anonymized DDoS
attack on 911. The current FCC regulations require that all emergency calls be
immediately routed regardless of the caller's identifiers (e.g., IMSI and
IMEI). A rootkit placed within the baseband firmware of a mobile phone can mask
and randomize all cellular identifiers, causing the device to have no genuine
identification within the cellular network. Such anonymized phones can issue
repeated emergency calls that cannot be blocked by the network or the emergency
call centers, technically or legally. We explore the 911 infrastructure and
discuss why it is susceptible to this kind of attack. We then implement
different forms of the attack and test our implementation on a small cellular
network. Finally, we simulate and analyze anonymous attacks on a model of
current 911 infrastructure in order to measure the severity of their impact. We
found that with less than 6K bots (or $100K hardware), attackers can block
emergency services in an entire state (e.g., North Carolina) for days. We
believe that this paper will assist the respective organizations, lawmakers,
and security professionals in understanding the scope of this issue in order to
prevent possible 911-DDoS attacks in the future.
</p>
<br><br><br><p>We propose an adaptive parameter balancing scheme in a variational framework
where a convex composite energy functional that consists of data fidelity and
regularization is optimized. In our adaptive parameter balancing, the relative
weight is assigned to each term of the energy for indicating its significance
to the total energy, and is automatically determined based on the data fidelity
measuring how well the data fits the model at each energy optimization step.
The adaptive balancing parameter is designed to locally control the regularity
by taking into consideration the residual at each point without introducing any
a-priori knowledge. We demonstrate that our adaptive balancing parameter is
effective to improve the quality of the solution by determining the degree of
regularity based on the local residual in the application of imaging problems.
We apply our adaptive parameter balancing scheme to the classical imaging
problems that are denoising, segmentation and motion estimation, and provide
their optimization algorithms based on the alternating direction method of
multipliers (ADMM) method. The robustness and effectiveness of our adaptive
parameter balancing is demonstrated through experimental results presenting
that the qualitative and quantitative evaluation results of each imaging task
with an adaptive balancing parameter is superior to the results with a static
one. The desired properties, robustness and effectiveness, of the parameter
selection in a variational framework for imaging problems are achieved by
merely replacing the static balancing parameter with our adaptive one.
</p>
<br><br><br><p>We present a practical approach to capturing ear-to-ear face models
comprising both 3D meshes and intrinsic textures (i.e. diffuse and specular
albedo). Our approach is a hybrid of geometric and photometric methods and
requires no geometric calibration. Photometric measurements made in a
lightstage are used to estimate view dependent high resolution normal maps. We
overcome the problem of having a single photometric viewpoint by capturing in
multiple poses. We use uncalibrated multiview stereo to estimate a coarse base
mesh to which the photometric views are registered. We propose a novel approach
to robustly stitching surface normal and intrinsic texture data into a
seamless, complete and highly detailed face model. The resulting relightable
models provide photorealistic renderings in any view.
</p>
<br><br><br><p>Melanoma is amongst most aggressive types of cancer. However, it is highly
curable if detected in its early stages. Prescreening of suspicious moles and
lesions for malignancy is of great importance. Detection can be done by images
captured by standard cameras, which are more preferable due to low cost and
availability. One important step in computerized evaluation of skin lesions is
accurate detection of lesion region, i.e. segmentation of an image into two
regions as lesion and normal skin. Accurate segmentation can be challenging due
to burdens such as illumination variation and low contrast between lesion and
healthy skin. In this paper, a method based on deep neural networks is proposed
for accurate extraction of a lesion region. The input image is preprocessed and
then its patches are fed to a convolutional neural network (CNN). Local texture
and global structure of the patches are processed in order to assign pixels to
lesion or normal classes. A method for effective selection of training patches
is used for more accurate detection of a lesion border. The output segmentation
mask is refined by some post processing operations. The experimental results of
qualitative and quantitative evaluations demonstrate that our method can
outperform other state-of-the-art algorithms exist in the literature.
</p>
<br><br><br><p>We are highly vulnerable to either natural or artificial catastrophes and
therefore, Public Protection and Disaster Relief (PPDR) operators need reliable
wireless communications for successful operations especially in critical rescue
missions. PPDR dedicated or commercial terrestrial networks have always been
used which at most times lead to unsuccessful operations. This is due to the
fact these networks are all infrastructure-based which can be destroyed, fail
to deliver the required service or the networks are not able to support and
sustain the sudden traffic surge. Long-Term Evolution (LTE) is earmarked as the
future candidate technology for PPDR purpose and so much have been put into it
in terms of research, perhaps suitable architecture that will meet
mission-critical requirements can be developed. This can only work if
terrestrial networks will always be available. Unfortunately, in worst case
scenarios, infrastructures might get damaged totally or might be destroyed by
subsequent disasters. As a result, adequate guarantees can only be possible in
the hypothesis of very high financial involvement. Fortunately, considering
availability, coverage ubiquity and reliability, satellite technologies have
lately proven good. So, to maximize the high channel performance of terrestrial
networks and the availability and reliability of non-terrestrial networks, the
solution lies in a hybrid system. It is on this ground that this work deals
with the integration of LTE and satellite networks in both infrastructure-based
and infrastructure-less topologies for PPDR purpose. It is aim at providing
people trapped in disaster and field operators with a transparent accessibility
and guaranteed coverage even when infrastructures are damaged. The requirements
are defined and the model simulated. The network is able to provide network
coverage, enhanced capacity and promised greater resilience.
</p>
<br><br><br><p>Online Convex Optimization plays a key role in large scale machine learning.
Early approaches to this problem were conservative, in which the main focus was
protection against the worst case scenario. But recently several algorithms
have been developed for tightening the regret bounds in easy data instances
such as sparsity, predictable sequences, and curved losses. In this work we
unify some of these existing techniques to obtain new update rules for the
cases when these easy instances occur together. First we analyse an adaptive
and optimistic update rule which achieves tighter regret bound when the loss
sequence is sparse and predictable. Then we explain an update rule that
dynamically adapts to the curvature of the loss function and utilizes the
predictable nature of the loss sequence as well. Finally we extend these
results to composite losses.
</p>
<br><br><br><p>Video on Demand (VoD) services like Netflix and YouTube account for ever
increasing fractions of Internet traffic. It is estimated that this fraction
will cross 80% in the next three years. Most popular VoD services have
recommendation engines which recommend videos to users based on their viewing
history, thus introducing time-correlation in user requests. Understanding and
modeling this time-correlation in user requests is critical for network traffic
engineering. The primary goal of this work is to use empirically observed
properties of user requests to model the effect of recommendation engines on
the request patterns in VoD services. We propose a Markovian request model to
capture the time-correlation in user requests and show that our model is
consistent with the observations of existing empirical studies.
</p>
<p>Most large-scale VoD services deliver content to users via a distributed
network of servers as serving users requests via geographically co-located
servers reduces latency and network bandwidth consumption. The content
replication policy, i.e., determining which contents to cache on the servers is
a key resource allocation problem for VoD services. Recent studies show that
low start-up delay is a key Quality of Service (QoS) requirement of users of
VoD services. This motivates the need to pre-fetch (fetch before contents are
requested) and cache content likely to be request in the near future. Since
pre-fetching leads to an increase in the network bandwidth usage, we use our
Markovian model to explore the trade-offs and feasibility of implementing
recommendation based pre-fetching.
</p>
<br><br><br><p>We present two reduced-rank channel estimators for large-scale
multiple-input, multiple-output (MIMO) systems and analyze their mean square
error (MSE) performance. Taking advantage of the channel's transform domain
sparseness, the estimators yield outstanding performance and may offer
additional mean angle-of-arrival (AoA) information. It is shown that, for the
estimators to be effective, one has to select a proper predetermined unitary
basis (transform) and be able to determine the dominant channel rank and the
associated subspace. Further MSE analysis reveals the relations among the array
size, channel rank, signal-to-noise ratio (SNR), and the estimators'
performance. It provides rationales for the proposed rank determination and
mean AoA estimation algorithms as well.
</p>
<p>An angle alignment operation included in one of our channel models is proved
to be effective in further reducing the required rank, shifting the dominant
basis vectors' range (channel spectrum) and improving the estimator's
performance when a suitable basis is used. We also draw insightful analogies
among MIMO channel modeling, transform coding, parallel spatial search, and
receive beamforming. Computer experiment results are provided to examine the
numerical effects of various estimator parameters and the advantages of the
proposed channel estimators and rank determination method.
</p>
<br><br><br><p>Malware creators have been getting their way for too long now. String-based
similarity measures can leverage ground truth in a scalable way and can operate
at a level of abstraction that is difficult to combat from the code level. We
introduce ITect, a scalable approach to malware similarity detection based on
information theory. ITect targets file entropy patterns in different ways to
achieve 100% precision with 90% accuracy but it could target 100% recall
instead. It outperforms VirusTotal for precision and accuracy on combined
Kaggle and VirusShare malware.
</p>
<br><br><br><p>The widespread adoption of smartphones in recent years has made it possible
for us to collect large amounts of traffic data. Special software installed on
the phones of drivers allow us to gather GPS trajectories of their vehicles on
the road network. In this paper, we simulate the trajectories of multiple
agents on a road network and use various models to forecast the short-term
traffic speed of various links. Our results show that traditional techniques
like multiple regression and artificial neural networks work well but simpler
adaptive models that do not require prior training also perform comparatively
well.
</p>
<br><br><br><p>Since its inception the memristive fuse has been a good example of how small
numbers of memristors can be combined to obtain useful behaviours unachievable
by individual devices. In this work, we link the memristive fuse concept with
that of the Complementary Resistive Switch (CRS), exploit that link to
experimentally demonstrate a practical memristive fuse using TiOx-based ReRAM
cells and explain its basic operational principles. The fuse is stimulated by
trains of identical pulses where successive pulse trains feature opposite
polarities. In response, we observe a gradual (analogue) drop in resistive
state followed by a gradual recovery phase regardless of input stimulus
polarity; echoing traditional, binary CRS behaviour. This analogue switching
property opens the possibility of operating the memristive fuse as a
single-component step change detector. Moreover, we discover that the
characteristics of the individual memristors used to demonstrate the memristive
fuse concept in this work allow our fuse to be operated in a regime where one
of the two constituent devices can be switched largely independently from the
other. This property, not present in the traditional CRS, indicates that the
inherently analogue memristive fuse architecture may support additional
operational flexibility through e.g. allowing finer control over its resistive
state.
</p>
<br><br><br><p>While network densification is considered an important solution to cater the
ever-increasing capacity demand, its effect on the handover (HO) rate is
overlooked. In dense 5G networks, HO delays may neutralize or even negate the
gains offered by network densification. Hence, user mobility imposes a
nontrivial challenge to harvest capacity gains via network densification. In
this paper, we propose a velocity-aware HO management scheme for two-tier
downlink cellular network to mitigate the HO effect on the foreseen
densification throughput gains. The proposed HO scheme sacrifices the best BS
connectivity, by skipping HO to some BSs along the user's trajectory, to
maintain longer connection durations and reduce HO rates. Furthermore, the
proposed scheme enables cooperative BS service and strongest interference
cancellation to compensate for skipping the best connectivity. To this end, we
consider different HO skipping scenarios and develop a velocity-aware
mathematical model, via stochastic geometry, to quantify the performance of the
proposed HO scheme in terms of the coverage probability and user throughput.
The results highlight the HO rate problem in dense cellular environments and
show the importance of the proposed HO schemes. Finally, the value of BS
cooperation along with handover skipping is quantified for different user
mobility profiles.
</p>
<br><br><br><p>Logical probability theory was developed as a quantitative measure based on
Boole's logic of subsets. But information theory was developed into a mature
theory by Claude Shannon with no such connection to logic. But a recent
development in logic changes this situation. In category theory, the notion of
a subset is dual to the notion of a quotient set or partition, and recently the
logic of partitions has been developed in a parallel relationship to the
Boolean logic of subsets (subset logic is usually mis-specified as the special
case of propositional logic). What then is the quantitative measure based on
partition logic in the same sense that logical probability theory is based on
subset logic? It is a measure of information that is named \"logical entropy\" in
view of that logical basis. This paper develops the notion of logical entropy
and the basic notions of the resulting logical information theory. Then an
extensive comparison is made with the corresponding notions based on Shannon
entropy.
</p>
<br><br><br><p>The incentive ratio measures the utility gains from strategic behaviour.
Without any restrictions on the setup, ratios for linear, Leontief and
Cobb-Douglas exchange markets are unbounded, showing that manipulating the
equilibrium is a worthwhile endeavour, even if it is computationally
challenging. Such unbounded improvements can be achieved even if agents only
misreport their utility functions. This provides a sharp contrast with previous
results from Fisher markets. When the Cobb-Douglas setup is more restrictive,
the maximum utility gain is bounded by the number of commodities. By means of
an example, we show that it is possible to exceed a known upper bound for
Fisher markets in exchange economies.
</p>
<br><br><br><p>As the next generation cellular system, 5G network is required to provide a
large variety of services for different kinds of terminals, from traditional
voice and data services over mobile phones to small packet transmission over
massive machine-type terminals. Although orthogonal-subcarrier based waveform
has been widely used nowadays in many practical systems, it can hardly meet the
future requirements in the coming 5G networks. Therefore, more flexible
waveforms have been proposed to address the unprecedented challenges. In this
article, we will provide comprehensive analysis and comparison for the typical
waveform candidates. To obtain insightful analysis, we will not only introduce
the basic principles of the waveforms but also reveal the underlying
characteristics of each waveform. Moreover, a comprehensive comparison in terms
of different performance metrics will be also presented in this article, which
provide an overall understanding of the new waveforms.
</p>
<br><br><br><p>Programming with replicated objects is difficult. Developers must face the
fundamental trade-off between consistency and performance head on, while
struggling with the complexity of distributed storage stacks. We introduce
Correctables, a novel abstraction that hides most of this complexity, allowing
developers to focus on the task of balancing consistency and performance. To
aid developers with this task, Correctables provide incremental consistency
guarantees, which capture successive refinements on the result of an ongoing
operation on a replicated object. In short, applications receive both a
preliminary---fast, possibly inconsistent---result, as well as a
final---consistent---result that arrives later.
</p>
<p>We show how to leverage incremental consistency guarantees by speculating on
preliminary values, trading throughput and bandwidth for improved latency. We
experiment with two popular storage systems (Cassandra and ZooKeeper) and three
applications: a Twissandra-based microblogging service, an ad serving system,
and a ticket selling system. Our evaluation on the Amazon EC2 platform with
YCSB workloads A, B, and C shows that we can reduce the latency of strongly
consistent operations by up to 40% (from 100ms to 60ms) at little cost (10%
bandwidth increase, 6% throughput drop) in the ad system. Even if the
preliminary result is frequently inconsistent (25% of accesses), incremental
consistency incurs a bandwidth overhead of only 27%.
</p>
<br><br><br><p>Wireless Power Transfer (WPT) is expected to be a technology reshaping the
landscape of low-power applications such as the Internet of Things, radio
frequency identification (RFID) networks, etc. Although there has been some
progress towards multi-antenna multi-sine WPT design, the large-scale design of
WPT, reminiscent of massive MIMO in communications, remains an open challenge.
In this paper, we derive efficient multiuser algorithms based on a
generalizable optimization framework, in order to design transmit sinewaves
that maximize the weighted-sum/minimum rectenna output DC voltage. The study
highlights the significant effect of the nonlinearity introduced by the
rectification process on the design of waveforms in multiuser systems.
Interestingly, in the single-user case, the optimal spatial domain beamforming,
obtained prior to the frequency domain power allocation optimization, turns out
to be maximum ratio combining (MRC). In contrast, in the general weighted sum
criterion maximization problem, the spatial domain beamforming optimization and
the frequency domain power allocation optimization are coupled. Assuming
channel hardening, low-complexity algorithms are proposed based on asymptotic
analysis, to maximize the two criteria. The structure of the asymptotically
optimal spatial domain precoder can be found prior to the optimization. The
performance of the proposed algorithms is evaluated. Numerical results confirm
the inefficiency of the linear model-based design for the single and multi-user
scenarios. It is also shown that as nonlinear model-based designs, the proposed
algorithms can benefit from an increasing number of sinewaves.
</p>
<br><br><br><p>This is the arXiv index for the electronic proceedings of the 24th
International Symposium on Graph Drawing and Network Visualization (GD 2016),
which was held in Athens, Greece, September 19-21 2016. It contains the
peer-reviewed and revised accepted papers with an optional appendix.
</p>
<br><br><br><p>We study the performance of cognitive Underlay System (US) that employ power
control mechanism at the Secondary Transmitter (ST) from a deployment
perspective. Existing baseline models considered for performance analysis
either assume the knowledge of involved channels at the ST or retrieve this
information by means of a band manager or a feedback channel, however, such
situations rarely exist in practice. Motivated by this fact, we propose a novel
approach that incorporates estimation of the involved channels at the ST, in
order to characterize the performance of the US in terms of interference power
received at the primary receiver and throughput at the secondary receiver (or
\textit{secondary throughput}). Moreover, we apply an outage constraint that
captures the impact of imperfect channel knowledge, particularly on the
uncertain interference. Besides this, we employ a transmit power constraint at
the ST to classify the operation of the US in terms of an interference-limited
regime and a power-limited regime. In addition, we characterize the expressions
of the uncertain interference and the secondary throughput for the case where
the involved channels encounter Nakagami-$m$ fading. Finally, we investigate a
fundamental tradeoff between the estimation time and the secondary throughput
depicting an optimized performance of the US.
</p>
<br><br><br><p>Conventional erasure codes such as Reed-Solomon (RS) provide savings in the
storage space, but at the cost of higher repair bandwidth and more complex
computations than replication. Minimum-Storage Regenerating (MSR) codes have
emerged as a viable alternative to RS codes as they minimize the repair
bandwidth while still being optimal in terms of reliability and storage.
Although several MSR code constructions exist, so far they have not been
practically implemented. One of the main reasons for their practical
abandonment is that existing MSR code constructions imply much bigger number of
I/O operations than RS codes.
</p>
<p>In this paper, we analyze MDS codes that are simultaneously optimal in terms
of storage, reliability, I/O operations and repair-bandwidth for single and
multiple failures of the systematic nodes. Due to the resemblance between the
hashtag sign \# and the construction procedure of these codes, we call them
\emph{HashTag Erasure Codes (HTECs)}.
</p>
<p>HTECs provide the lowest data-read and data-transfer for an arbitrary
sub-packetization level $\alpha$ where $\alpha \leq r^{\Bigl\lceil \frac{k}{r}
\Bigr\rceil}$ among all existing solutions for distributed storage. The repair
process is linear and highly parallel. Additionally, we show that HTECs are the
first high-rate MDS codes that reduce the repair bandwidth for multiple
failures.
</p>
<br><br><br><p>Recommendation systems are being explored by Cable TV operators to improve
user satisfaction with services, such as Live TV and Video on Demand (VOD)
services. More recently, Catch-up TV has been introduced, allowing users to
watch recent broadcast content whenever they want to. These services give users
a large set of options from which they can choose from, creating an information
overflow problem. Thus, recommendation systems arise as essential tools to
solve this problem by helping users in their selection, which increases not
only user satisfaction but also user engagement and content consumption. In
this paper we present a learning to rank approach that uses contextual
information and implicit feedback to improve recommendation systems for a Cable
TV operator that provides Live and Catch-up TV services. We compare our
approach with existing state-of-the-art algorithms and show that our approach
is superior in accuracy, while maintaining high scores of diversity and
serendipity.
</p>
<br><br><br><p>Common computational methods for automated eye movement detection - i.e. the
task of detecting different types of eye movement in a continuous stream of
gaze data - are limited in that they either involve thresholding on
hand-crafted signal features, require individual detectors each only detecting
a single movement, or require pre-segmented data. We propose a novel approach
for eye movement detection that only involves learning a single detector
end-to-end, i.e. directly from the continuous gaze data stream and
simultaneously for different eye movements without any manual feature crafting
or segmentation. Our method is based on convolutional neural networks (CNN)
that recently demonstrated superior performance in a variety of tasks in
computer vision, signal processing, and machine learning. We further introduce
a novel multi-participant dataset that contains scripted and free-viewing
sequences of ground-truth annotated saccades, fixations, and smooth pursuits.
We show that our CNN-based method outperforms state-of-the-art baselines by a
large margin on this challenging dataset, thereby underlining the significant
potential of this approach for holistic, robust, and accurate eye movement
protocol analysis.
</p>
<br><br><br><p>Nowadays, Cable TV operators provide their users multiple ways to watch TV
content, such as Live TV and Video on Demand (VOD) services. In the last years,
Catch-up TV has been introduced, allowing users to watch recent broadcast
content whenever they want to. Understanding how the users interact with such
services is important to develop solutions that may increase user satisfaction
, user engagement and user consumption. In this paper, we characterize, for the
first time, how users interact with a large European Cable TV operator that
provides Live TV, Catch-up TV and VOD services. We analyzed many
characteristics, such as the service usage, user engagement, program type,
program genres and time periods. This characterization will help us to have a
deeper understanding on how users interact with these different services, that
may be used to enhance the recommendation systems of Cable TV providers.
</p>
<br><br><br><p>We consider the problem of stabilizing voltages in DC microGrids (mGs) given
by the interconnection of Distributed Generation Units (DGUs), power lines and
loads. We propose a decentralized control architecture where the primary
controller of each DGU can be designed in a Plug-and-Play (PnP) fashion,
allowing the seamless addition of new DGUs. Differently from several other
approaches to primary control, local design is independent of the parameters of
power lines. Moreover, differently from the PnP control scheme in [1], the
plug-in of a DGU does not require to update controllers of neighboring DGUs.
Local control design is cast into a Linear Matrix Inequality (LMI) problem
that, if unfeasible, allows one to deny plug-in requests that might be
dangerous for mG stability. The proof of closed-loop stability of voltages
exploits structured Lyapunov functions, the LaSalle invariance theorem and
properties of graph Laplacians. Theoretical results are backed up by
simulations in PSCAD.
</p>
<br><br><br><p>We introduce a definition for \emph{Families of Optimal Binary Non-MDS
Erasure Codes} for $[n, k]$ codes over $GF(2)$, and propose an algorithm for
finding those families by using hill climbing techniques over Balanced XOR
codes. Due to the hill climbing search, those families of codes have always
better decoding probability than the codes generated in a typical Random Linear
Network Coding scenario, i.e., random linear codes. We also show a surprising
result that for small values of $k$, the decoding probability of our codes in
$GF(2)$ is very close to the decoding probability of the codes obtained by
Random Linear Network Coding but in the higher finite field $GF(4)$.
</p>
<br><br><br><p>In order to deal with the scaling problem of volumetric map representations
we propose spatially local methods for high-ratio compression of 3D maps,
represented as truncated signed distance fields. We show that these compressed
maps can be used as meaningful descriptors for selective decompression in
scenarios relevant to robotic applications. As compression methods, we compare
using PCA-derived low-dimensional bases to non-linear auto-encoder networks and
novel mixed architectures that combine both. Selecting two application-oriented
performance metrics, we evaluate the impact of different compression rates on
reconstruction fidelity as well as to the task of map-aided ego-motion
estimation. It is demonstrated that lossily compressed distance fields used as
cost functions for ego-motion estimation, can outperform their uncompressed
counterparts in challenging scenarios from standard RGB-D data-sets.
</p>
<br><br><br><p>This paper proposes a new approach to automatically quantify the severity of
knee osteoarthritis (OA) from radiographs using deep convolutional neural
networks (CNN). Clinically, knee OA severity is assessed using Kellgren \&amp;
Lawrence (KL) grades, a five point scale. Previous work on automatically
predicting KL grades from radiograph images were based on training shallow
classifiers using a variety of hand engineered features. We demonstrate that
classification accuracy can be significantly improved using deep convolutional
neural network models pre-trained on ImageNet and fine-tuned on knee OA images.
Furthermore, we argue that it is more appropriate to assess the accuracy of
automatic knee OA severity predictions using a continuous distance-based
evaluation metric like mean squared error than it is to use classification
accuracy. This leads to the formulation of the prediction of KL grades as a
regression problem and further improves accuracy. Results on a dataset of X-ray
images and KL grades from the Osteoarthritis Initiative (OAI) show a sizable
improvement over the current state-of-the-art.
</p>
<br><br><br><p>We present a method to determine Fashion DNA, coordinate vectors locating
fashion items in an abstract space. Our approach is based on a deep neural
network architecture that ingests curated article information such as tags and
images, and is trained to predict sales for a large set of frequent customers.
In the process, a dual space of customer style preferences naturally arises.
Interpretation of the metric of these spaces is straightforward: The product of
Fashion DNA and customer style vectors yields the forecast purchase likelihood
for the customer-item pair, while the angle between Fashion DNA vectors is a
measure of item similarity. Importantly, our models are able to generate
unbiased purchase probabilities for fashion items based solely on article
information, even in absence of sales data, thus circumventing the \"cold-start
problem\" of collaborative recommendation approaches. Likewise, it generalizes
easily and reliably to customers outside the training set. We experiment with
Fashion DNA models based on visual and/or tag item data, evaluate their
recommendation power, and discuss the resulting article similarities.
</p>
<br><br><br><p>This paper deals with the problem of detecting non-isotropic high-dimensional
geometric structure in random graphs. Namely, we study a model of a random
geometric graph in which vertices correspond to points generated randomly and
independently from a non-isotropic $d$-dimensional Gaussian distribution, and
two vertices are connected if the distance between them is smaller than some
pre-specified threshold. We derive new notions of dimensionality which depend
upon the eigenvalues of the covariance of the Gaussian distribution. If
$\alpha$ denotes the vector of eigenvalues, and $n$ is the number of vertices,
then the quantities $\left(\frac{||\alpha||_2}{||\alpha||_3}\right)^6/n^3$ and
$\left(\frac{||\alpha||_2}{||\alpha||_4}\right)^4/n^3$ determine upper and
lower bounds for the possibility of detection. This generalizes a recent result
by Bubeck, Ding, R\'acz and the first named author from [BDER14] which shows
that the quantity $d/n^3$ determines the boundary of detection for isotropic
geometry. Our methods involve Fourier analysis and the theory of characteristic
functions to investigate the underlying probabilities of the model. The proof
of the lower bound uses information theoretic tools, based on the method
presented in [BG15].
</p>
<br><br><br><p>Accurate pedestrian detection has a primary role in automotive safety: for
example, by issuing warnings to the driver or acting actively on car's brakes,
it helps decreasing the probability of injuries and human fatalities. In order
to achieve very high accuracy, recent pedestrian detectors have been based on
Convolutional Neural Networks (CNN). Unfortunately, such approaches require
vast amounts of computational power and memory, preventing efficient
implementations on embedded systems. This work proposes a CNN-based detector,
adapting a general-purpose convolutional network to the task at hand. By
thoroughly analyzing and optimizing each step of the detection pipeline, we
develop an architecture that outperforms methods based on traditional image
features and achieves an accuracy close to the state-of-the-art while having
low computational complexity. Furthermore, the model is compressed in order to
fit the tight constrains of low power devices with a limited amount of embedded
memory available. This paper makes two main contributions: (1) it proves that a
region based deep neural network can be finely tuned to achieve adequate
accuracy for pedestrian detection (2) it achieves a very low memory usage
without reducing detection accuracy on the Caltech Pedestrian dataset.
</p>
<br><br><br><p>This work draws its inspiration from three important sources of research on
dissimilarity-based clustering and intertwines those three threads into a
consistent principled functorial theory of clustering. Those three are the
overlapping clustering of Jardine and Sibson, the functorial approach of
Carlsson and Memoli to partition-based clustering, and the Isbell/Dress
school's study of injective envelopes. Carlsson and Memoli introduce the idea
of viewing clustering methods as functors from a category of metric spaces to a
category of clusters, with functoriality subsuming many desirable properties.
Our first series of results extends their theory of functorial clustering
schemes to methods that allow overlapping clusters in the spirit of Jardine and
Sibson. This obviates some of the unpleasant effects of chaining that occur,
for example with single-linkage clustering. We prove an equivalence between
these general overlapping clustering functors and projections of weight spaces
to what we term clustering domains, by focusing on the order structure
determined by the morphisms. As a specific application of this machinery, we
are able to prove that there are no functorial projections to cut metrics, or
even to tree metrics. Finally, although we focus less on the construction of
clustering methods (clustering domains) derived from injective envelopes, we
lay out some preliminary results, that hopefully will give a feel for how the
third leg of the stool comes into play.
</p>
<br><br><br><p>This paper considers a $K$-user single-input-single-output interference
channel with inter-symbol interference (ISI), in which the channel coefficients
are assumed to be linear time-invariant with finite-length impulse response.
The primary finding of this paper is that, with no channel state information at
a transmitter (CSIT), the sum-spectral efficiency can be made to scale linearly
with $K$, provided that the desired links have longer impulse response than do
the interfering links. This linear gain is achieved by a novel multi-carrier
communication scheme which we call \textit{interference-free orthogonal
frequency division multiplexing (IF-OFDM)}. Furthermore, when a transmitter is
able to learn CSIT from its paired receiver only, a higher sum-spectral
efficiency can be achieved by a two-stage transmission method that concatenates
IF-OFDM and vector coding based on singular value decomposition with
water-filling power allocation. A major implication of the derived results is
that separate encoding across subcarriers per link is sufficient to linearly
increase the sum-spectral efficiency with $K$ in the interference channel with
ISI. Simulation results support this claim.
</p>
<br><br><br><p>Almost all processes -- highly correlated, weakly correlated, or correlated
not at all---exhibit statistical fluctuations. Often physical laws, such as the
Second Law of Thermodynamics, address only typical realizations -- as
highlighted by Shannon's asymptotic equipartition property and as entailed by
taking the thermodynamic limit of an infinite number of degrees of freedom.
Indeed, our interpretations of the functioning of macroscopic thermodynamic
cycles are so focused. Using a recently derived Second Law for information
processing, we show that different subsets of fluctuations lead to distinct
thermodynamic functioning in Maxwellian Demons. For example, while typical
realizations may operate as an engine -- converting thermal fluctuations to
useful work -- even \"nearby\" fluctuations (nontypical, but probable
realizations) behave differently, as Landauer erasers -- converting available
stored energy to dissipate stored information. One concludes that ascribing a
single, unique functional modality to a thermodynamic system, especially one on
the nanoscale, is at best misleading, likely masking an array of simultaneous,
parallel thermodynamic transformations. This alters how we conceive of cellular
processes, engineering design, and evolutionary adaptation.
</p>
<br><br><br><p>Extreme multi-label classification refers to supervised multi-label learning
involving hundreds of thousands or even millions of labels. Datasets in extreme
classification exhibit fit to power-law distribution, i.e. a large fraction of
labels have very few positive instances in the data distribution. Most
state-of-the-art approaches for extreme multi-label classification attempt to
capture correlation among labels by embedding the label matrix to a
low-dimensional linear sub-space. However, in the presence of power-law
distributed extremely large and diverse label spaces, structural assumptions
such as low rank can be easily violated.
</p>
<p>In this work, we present DiSMEC, which is a large-scale distributed framework
for learning one-versus-rest linear classifiers coupled with explicit capacity
control to control model size. Unlike most state-of-the-art methods, DiSMEC
does not make any low rank assumptions on the label matrix. Using double layer
of parallelization, DiSMEC can learn classifiers for datasets consisting
hundreds of thousands labels within few hours. The explicit capacity control
mechanism filters out spurious parameters which keep the model compact in size,
without losing prediction accuracy. We conduct extensive empirical evaluation
on publicly available real-world datasets consisting upto 670,000 labels. We
compare DiSMEC with recent state-of-the-art approaches, including - SLEEC which
is a leading approach for learning sparse local embeddings, and FastXML which
is a tree-based approach optimizing ranking based loss function. On some of the
datasets, DiSMEC can significantly boost prediction accuracies - 10% better
compared to SLECC and 15% better compared to FastXML, in absolute terms.
</p>
<br><br><br><p>AND-OR networks are Boolean networks where each coordinate function is either
the AND or OR logical operator. We study the number of fixed points of these
Boolean networks in the case that they have a wiring diagram with chain
topology. We find closed formulas for subclasses of these networks and
recursive formulas in the general case. Our results allow for an effective
computation of the number of fixed points in the case that the topology of the
Boolean network is an open chain (finite or infinite) or a closed chain.
</p>
<br><br><br><p>This paper reports the activities and outcomes in the Workshop on Grasping
and Manipulation Datasets that was organized under the International Conference
on Robotics and Automation (ICRA) 2016. The half day workshop was packed with
nine invited talks, 12 interactive presentations, and one panel discussion with
ten panelists. This paper summarizes all the talks and presentations and recaps
what has been discussed in the panels session. This summary servers as a review
of recent developments in data collection in grasping and manipulation. Many of
the presentations describe ongoing efforts or explorations that could be
achieved and fully available in a year or two. The panel discussion not only
commented on the current approaches, but also indicates new directions and
focuses. The workshop clearly displayed the importance of quality datasets in
robotics and robotic grasping and manipulation field. Hopefully the workshop
could motivate larger efforts to create big datasets that are comparable with
big datasets in other communities such as computer vision.
</p>
<br><br><br><p>Named Data Networks provide a clean-slate redesign of the Future Internet for
efficient content distribution. Because Internet of Things are expected to
compose a significant part of Future Internet, most content will be managed by
constrained devices. Such devices are often equipped with limited CPU, memory,
bandwidth, and energy supply. However, the current Named Data Networks design
neglects the specific requirements of Internet of Things scenarios and many
data structures need to be further optimised. The purpose of this research is
to provide an efficient strategy to route in Named Data Networks by
constructing a Forwarding Information Base using Iterated Bloom Filters defined
as I(FIB)F. We propose the use of content names based on iterative hashes. This
strategy leads to reduce the overhead of packets. Moreover, the memory and the
complexity required in the forwarding strategy are lower than in current
solutions. We compare our proposal with solutions based on hierarchical names
and Standard Bloom Filters. We show how to further optimise I(FIB)F by
exploiting the structure information contained in hierarchical content names.
Finally, two strategies may be followed to reduce: (i) the overall memory for
routing or (ii) the probability of false positives.
</p>
<br><br><br><p>There is increasing interest in the potential advantages of using quantum
computing technologies as sampling engines to speedup machine learning and
probabilistic programming tasks. However, some pressing challenges in
state-of-the-art quantum annealers have to be overcome before we can assess
their actual performance. Most notably, the effective temperature at which
samples are generated is instance-dependent and unknown, the interaction graph
is sparse, the parameters are noisy, and the dynamic range of the parameters is
finite. Of all these limitations, the sparse connectivity resulting from the
local interaction between quantum bits in physical hardware implementations, is
considered the most severe limitation to the quality of constructing powerful
machine learning models. Here we show how to surpass this \"curse of limited
connectivity\" bottleneck and illustrate our findings by training probabilistic
generative models with arbitrary pairwise connectivity. Our model can be
trained in quantum hardware without full knowledge of the effective parameters
specifying the corresponding Boltzmann-like distribution. Therefore, inference
of the effective temperature is avoided and the effect of noise in the
parameters is mitigated. We illustrate our findings by successfully training
hardware-embedded models with all-to-all connectivity on a real dataset of
handwritten digits and two synthetic datasets. In each of these datasets we
show the generative capabilities of the models learned with the assistance of
the quantum annealer in experiments with up to 940 quantum bits. Additionally,
we show a visual Turing test with handwritten digit data, where the machine
generating the digits is a quantum processor. Such digits, with a remarkable
similarity to those generated by humans, are extracted from the experiments
with 940 quantum bits.
</p>
<br><br><br><p>In this paper we study the routing and rebalancing problem for a fleet of
autonomous vehicles providing on-demand transportation within a congested urban
road network (that is, a road network where traffic speed depends on vehicle
density). We show that the congestion-free routing and rebalancing problem is
NP-hard and provide a randomized algorithm which finds a low-congestion
solution to the routing and rebalancing problem that approximately minimizes
the number of vehicles on the road in polynomial time. We provide theoretical
bounds on the probability of violating the congestion constraints; we also
characterize the expected number of vehicles required by the solution with a
commonly-used empirical congestion model and provide a bound on the
approximation factor of the algorithm. Numerical experiments on a realistic
road network with real-world customer demands show that our algorithm
introduces very small amounts of congestion. The performance of our algorithm
in terms of travel times and required number of vehicles is very close to (and
sometimes better than) the optimal congestion-free solution.
</p>
<br><br><br><p>Robotic commands in natural language usually contain lots of spatial
descriptions which are semantically similar but syntactically different.
Mapping such syntactic variants into semantic concepts that can be understood
by robots is challenging due to the high flexibility of natural language
expressions. To tackle this problem, we collect robotic commands for navigation
and manipulation tasks using crowdsourcing. We further define a robot language
and use a generative machine translation model to translate robotic commands
from natural language to robot language. The main purpose of this paper is to
simulate the interaction process between human and robots using crowdsourcing
platforms, and investigate the possibility of translating natural language to
robot language with paraphrases.
</p>
<br><br><br><p>Neuromorphic chip refers to an unconventional computing architecture that is
modelled on biological brains. It is ideally suited for processing sensory data
for intelligence computing, decision-making or context cognition. Despite rapid
development, conventional artificial synapses exhibit poor connection
flexibility and require separate data acquisition circuitry, resulting in
limited functionalities and significant hardware redundancy. Here we report a
novel light-stimulated artificial synapse based on a graphene-nanotube hybrid
phototransistor that can directly convert optical stimuli into a \"neural image\"
for further neuronal analysis. Our optically-driven synapses involve multiple
steps of plasticity mechanisms and importantly exhibit flexible tuning of both
short- and long-term plasticity. Furthermore, our neuromorphic phototransistor
can take multiple pre-synaptic light stimuli via wavelength-division
multiplexing and allows advanced optical processing through
charge-trap-mediated optical coupling. The capability of complex neuromorphic
functionalities in a simple silicon-compatible device paves the way for novel
neuromorphic computing architectures involving photonics.
</p>
<br><br><br><p>The \emph{Fr\'echet distance} is a well studied similarity measures between
curves. The \emph{discrete Fr\'echet distance} is an analogous similarity
measure, defined for a sequence $A$ of $m$ points and a sequence $B$ of $n$
points, where the points are usually sampled from input curves. In this paper
we consider a variant, called the \emph{discrete Fr\'echet distance with
shortcuts}, which captures the similarity between (sampled) curves in the
presence of outliers. For the \emph{two-sided} case, where shortcuts are
allowed in both curves, we give an $O((m^{2/3}n^{2/3}+m+n)\log^3 (m+n))$-time
algorithm for computing this distance. When shortcuts are allowed only in one
noise-containing curve, we give an even faster randomized algorithm that runs
in $O((m+n)^{6/5+\varepsilon})$ expected time, for any $\varepsilon&gt;0$.
</p>
<p>Our techniques are novel and may find further applications. One of the main
new technical results is: Given two sets of points $A$ and $B$ and an interval
$I$, we develop an algorithm that decides whether the number of pairs $(x,y)\in
A\times B$ whose distance ${\rm dist}(x,y)$ is in $I$, is less than some given
threshold $L$. The running time of this algorithm decreases as $L$ increases.
In case there are more than $L$ pairs of points whose distance is in $I$, we
can get a small sample of pairs that contains a pair at approximate median
distance (i.e., we can approximately \"bisect\" $I$). We combine this procedure
with additional ideas to search, with a small overhead, for the optimal
one-sided Fr\'echet distance with shortcuts, using a very fast decision
procedure. We also show how to apply this technique for approximating distance
selection (with respect to rank), and for computing the semi-continuous
Fr\'echet distance with one-sided shortcuts.
</p>
<br><br><br><p>In this paper, we draw connections between ideal lattices and multivariate
polynomial rings over integers using Gr\\"obner bases. Ideal lattices are ideals
in the residue class ring, $\mathbb{Z}[x]/\langle f \rangle$ (here $f$ is a
monic polynomial), and cryptographic primitives have been built based on these
objects. As ideal lattices in the univariate case are generalizations of cyclic
lattices, we introduce the notion of multivariate cyclic lattices and show that
multivariate ideal lattices are indeed a generalization of them. Based on
multivariate ideal lattices, we establish the existence of collision resistant
hash functions using Gr\\"obner basis techniques. For the construction of hash
functions, we define a worst case problem, shortest substitution problem w.r.t.
an ideal in $\mathbb{Z}[x_1,\ldots, x_n]$, and establish hardness results using
functional fields.
</p>
<br><br><br><p>We propose regularization schemes for deformable registration and efficient
algorithms for their numerical approximation. We treat image registration as a
variational optimal control problem. The deformation map is parametrized by its
velocity. Tikhonov regularization ensures well-posedness. Our scheme augments
standard smoothness regularization operators based on $H^1$- and
$H^2$-seminorms with a constraint on the divergence of the velocity field,
which resembles variational formulations for Stokes incompressible flows. In
our formulation, we invert for a stationary velocity field and a mass source
map. This allows us to explicitly control the compressibility of the
deformation map and by that the determinant of the deformation gradient. We
also introduce a new regularization scheme that allows us to control shear. We
use a globalized, preconditioned, matrix-free, reduced space
(Gauss--)Newton--Krylov scheme for numerical optimization. We exploit variable
elimination techniques to reduce the number of unknowns of our system; we only
iterate on the reduced space of the velocity field. Our current implementation
is limited to the two-dimensional case. The numerical experiments demonstrate
that we can control the determinant of the deformation gradient without
compromising registration quality. This additional control allows us to avoid
oversmoothing of the deformation map. We also demonstrate that we can promote
or penalize shear while controlling the determinant of the deformation
gradient.
</p>
<br><br><br><p>Understanding the formation of subjective human traits, such as preference
and opinions, is an important research problem. An essential aspect of this
problem is that traits collectively evolve under the action of social influence
interactions, which is the focus of many quantitative studies of cultural
dynamics. In this paradigm, dynamical models require that all traits are fixed
when specifying the \"initial cultural state\", which is typically generated in a
uniformly random way. However, recent work has shown that the outcome of social
influence dynamics strongly depends on the nature of the initial state: if this
is sampled from empirical data, a higher level of cultural diversity is found
after long-term dynamics, for the same level of propensity towards collective
behaviour in the short-term. First, this study shows that the empirical
properties responsible for this effect are remarkably robust across data sets.
In a certain sense, the analysis also suggests that socio-cultural systems
generally function close to criticality. Second, this study presents a
stochastic model for generating cultural states that retain the robust,
empirical properties. One ingredient of the model, already used in previous
work, assumes that every individual's set of traits is partly dictated by one
of several, universal \"rationalities\", informally postulated by several social
science theories. The second, new ingredient, taken from the same theories,
assumes that apart from a dominant rationality, each individual also has a
certain exposure to the other rationalities. The fact that this combination of
ingredients is compatible with empirical regularities suggests that the effects
of preference formation in the real world can be described as an interplay of
multiple, mixing rationalities, providing indirect evidence for the class of
social science theories that convey this picture.
</p>
<br><br><br><p>The main objective of this work is to study mathematical properties of
computational paths. Originally proposed by de Queiroz \&amp; Gabbay (1994) as
`sequences or rewrites', computational paths are taken to be terms of the
identity type of Martin L\\"of's Intensional Type Theory, since these paths can
be seen as the grounds on which the propositional equality between two
computational objects stand. From this perspective, this work aims to show that
one of the properties of the identity type is present on computational paths.
We are referring to the fact that that the identity type induces a groupoid
structure, as proposed by Hofmann \&amp; Streicher (1994). Using categorical
semantics, we show that computational paths induce a groupoid structure. We
also show that computational paths are capable of inducing higher categorical
structures.
</p>
<br><br><br><p>A distance labeling scheme labels the $n$ nodes of a graph with binary
strings such that, given the labels of any two nodes, one can determine the
distance in the graph between the two nodes by looking only at the labels. A
$D$-preserving distance labeling scheme only returns precise distances between
pairs of nodes that are at distance at least $D$ from each other. In this paper
we consider distance labeling schemes for the classical case of unweighted
graphs with both directed and undirected edges.
</p>
<p>We present a $O(\frac{n}{D}\log^2 D)$ bit $D$-preserving distance labeling
scheme, improving the previous bound by Bollob\'as et. al. [SIAM J. Discrete
Math. 2005]. We also give an almost matching lower bound of
$\Omega(\frac{n}{D})$. With our $D$-preserving distance labeling scheme as a
building block, we additionally achieve the following results:
</p>
<p>1. We present the first distance labeling scheme of size $o(n)$ for sparse
graphs (and hence bounded degree graphs). This addresses an open problem by
Gavoille et. al. [J. Algo. 2004], hereby separating the complexity from
distance labeling in general graphs which require $\Omega(n)$ bits, Moon [Proc.
of Glasgow Math. Association 1965].
</p>
<p>2. For approximate $r$-additive labeling schemes, that return distances
within an additive error of $r$ we show a scheme of size $O\left ( \frac{n}{r}
\cdot\frac{\operatorname{polylog} (r\log n)}{\log n} \right )$ for $r \ge 2$.
This improves on the current best bound of $O\left(\frac{n}{r}\right)$ by
Alstrup et. al. [SODA 2016] for sub-polynomial $r$, and is a generalization of
a result by Gawrychowski et al. [arXiv preprint 2015] who showed this for
$r=2$.
</p>
<br><br><br><p>We consider the problem of recovering a signal consisting of a superposition
of point sources from low-resolution data with a cut-off frequency f. If the
distance between the sources is under 1/f, this problem is not well posed in
the sense that the low-pass data corresponding to two different signals may be
practically the same. We show that minimizing a continuous version of the l1
norm achieves exact recovery as long as the sources are separated by at least
1.26/f. The proof is based on the construction of a dual certificate for the
optimization problem, which can be used to establish that the procedure is
stable to noise. Finally, we illustrate the flexibility of our
optimization-based framework by describing extensions to the demixing of sines
and spikes and to the estimation of point sources that share a common support.
</p>
<br><br><br><p>Let $A(x) = A_0+x_1A_1+\cdots+x_nA_n$ be a linear matrix, or pencil,
generated by given symmetric matrices $A_0,A_1,\ldots,A_n$ of size $m$ with
rational entries. The set of real vectors $x$ such that the pencil is positive
semidefinite is a convex semi-algebraic set called spectrahedron, described by
a linear matrix inequality (LMI). We design an exact algorithm that, up to
genericity assumptions on the input matrices, computes an exact algebraic
representation of at least one point in the spectrahedron, or decides that it
is empty. The algorithm does not assume the existence of an interior point, and
the computed point minimizes the rank of the pencil on the spectrahedron. The
degree $d$ of the algebraic representation of the point coincides
experimentally with the algebraic degree of a generic semidefinite program
associated to the pencil. We provide explicit bounds for the complexity of our
algorithm, proving that the maximum number of arithmetic operations that are
performed is essentially quadratic in a multilinear B\'ezout bound of $d$. When
$m$ (resp. $n$) is fixed, such a bound, and hence the complexity, is polynomial
in $n$ (resp. $m$). We conclude by providing results of experiments showing
practical improvements with respect to state-of-the-art computer algebra
algorithms.
</p>
<br><br><br><p>Numerical weather prediction (NWP) has proven to be computationally
challenging due to its inherent multiscale nature. Currently, the highest
resolution NWP models use a horizontal resolution of about 10km. In order to
increase the resolution of NWP models highly scalable atmospheric models are
needed.
</p>
<p>The Non-hydrostatic Unified Model of the Atmosphere (NUMA), developed by the
authors at the Naval Postgraduate School, was designed to achieve this purpose.
NUMA is used by the Naval Research Laboratory, Monterey as the engine inside
its next generation weather prediction system NEPTUNE. NUMA solves the fully
compressible Navier-Stokes equations by means of high-order Galerkin methods
(both spectral element as well as discontinuous Galerkin methods can be used).
Mesh generation is done using the p4est library. NUMA is capable of running
middle and upper atmosphere simulations since it does not make use of the
shallow-atmosphere approximation.
</p>
<p>This paper presents the performance analysis and optimization of the spectral
element version of NUMA. The performance at different optimization stages is
analyzed using a theoretical performance model as well as measurements via
hardware counters. Machine independent optimization is compared to machine
specific optimization using BG/Q vector intrinsics. By using vector intrinsics
the main computations reach 1.2 PFlops on the entire machine Mira (12% of the
theoretical peak performance). The paper also presents scalability studies for
two idealized test cases that are relevant for NWP applications. The
atmospheric model NUMA delivers an excellent strong scaling efficiency of 99%
on the entire supercomputer Mira using a mesh with 1.8 billion grid points.
This allows to run a global forecast of a baroclinic wave test case at 3km
uniform horizontal resolution and double precision within the time frame
required for operational weather prediction.
</p>
<br><br><br><p>Multi-mode resource and precedence-constrained project scheduling is a
well-known challenging real-world optimisation problem. An important variant of
the problem requires scheduling of activities for multiple projects considering
availability of local and global resources while respecting a range of
constraints. A critical aspect of the benchmarks addressed in this paper is
that the primary objective is to minimise the sum of the project completion
times, with the usual makespan minimisation as a secondary objective. We
observe that this leads to an expected different overall structure of good
solutions and discuss the effects this has on the algorithm design. This paper
presents a carefully designed hybrid of Monte-Carlo tree search, novel
neighbourhood moves, memetic algorithms, and hyper-heuristic methods. The
implementation is also engineered to increase the speed with which iterations
are performed, and to exploit the computing power of multicore machines.
Empirical evaluation shows that the resulting information-sharing
multi-component algorithm significantly outperforms other solvers on a set of
\"hidden\" instances, i.e. instances not available at the algorithm design phase.
</p>
<br><br><br><p>The temporal-difference methods TD($\lambda$) and Sarsa($\lambda$) form a
core part of modern reinforcement learning. Their appeal comes from their good
performance, low computational cost, and their simple interpretation, given by
their forward view. Recently, new versions of these methods were introduced,
called true online TD($\lambda$) and true online Sarsa($\lambda$), respectively
(van Seijen &amp; Sutton, 2014). These new versions maintain an exact equivalence
with the forward view at all times, whereas the traditional versions only
approximate it for small step-sizes. We hypothesize that these true online
methods not only have better theoretical properties, but also dominate the
regular methods empirically. In this article, we put this hypothesis to the
test by performing an extensive empirical comparison. Specifically, we compare
the performance of true online TD($\lambda$)/Sarsa($\lambda$) with regular
TD($\lambda$)/Sarsa($\lambda$) on random MRPs, a real-world myoelectric
prosthetic arm, and a domain from the Arcade Learning Environment. We use
linear function approximation with tabular, binary, and non-binary features.
Our results suggest that the true online methods indeed dominate the regular
methods. Across all domains/representations the learning speed of the true
online methods are often better, but never worse than that of the regular
methods. An additional advantage is that no choice between traces has to be
made for the true online methods. Besides the empirical results, we provide an
in-depth analysis of the theory behind true online temporal-difference
learning. In addition, we show that new true online temporal-difference methods
can be derived by making changes to the online forward view and then rewriting
the update equations.
</p>
<br><br><br><p>This paper specifies a notation for Markov decision processes.
</p>
<br><br><br><p>Optimization on manifolds is a class of methods for optimization of an
objective function, subject to constraints which are smooth, in the sense that
the set of points which satisfy the constraints admits the structure of a
differentiable manifold. While many optimization problems are of the described
form, technicalities of differential geometry and the laborious calculation of
derivatives pose a significant barrier for experimenting with these methods.
</p>
<p>We introduce Pymanopt (available at https://pymanopt.github.io), a toolbox
for optimization on manifolds, implemented in Python, that---similarly to the
Manopt Matlab toolbox---implements several manifold geometries and optimization
algorithms. Moreover, we lower the barriers to users further by using automated
differentiation for calculating derivative information, saving users time and
saving them from potential calculation and implementation errors.
</p>
<br><br><br><p>Data seems cheap to get, and in many ways it is, but the process of creating
a high quality labeled dataset from a mass of data is time-consuming and
expensive.
</p>
<p>With the advent of rich 3D repositories, photo-realistic rendering systems
offer the opportunity to provide nearly limitless data. Yet, their primary
value for visual learning may be the quality of the data they can provide
rather than the quantity. Rendering engines offer the promise of perfect labels
in addition to the data: what the precise camera pose is; what the precise
lighting location, temperature, and distribution is; what the geometry of the
object is.
</p>
<p>In this work we focus on semi-automating dataset creation through use of
synthetic data and apply this method to an important task -- object viewpoint
estimation. Using state-of-the-art rendering software we generate a large
labeled dataset of cars rendered densely in viewpoint space. We investigate the
effect of rendering parameters on estimation performance and show realism is
important. We show that generalizing from synthetic data is not harder than the
domain adaptation required between two real-image datasets and that combining
synthetic images with a small amount of real data improves estimation accuracy.
</p>
<br><br><br><p>In this paper, we develop an analytical framework for the initial access
(a.k.a. Base Station (BS) discovery) in a millimeter-wave (mm-wave)
communication system and propose an effective strategy for transmitting the
Reference Signals (RSs) used for BS discovery. Specifically, by formulating the
problem of BS discovery at User Equipments (UEs) as hypothesis tests, we derive
a detector based on the Generalised Likelihood Ratio Test (GLRT) and
characterise the statistical behaviour of the detector. The theoretical results
obtained allow analysis of the impact of key system parameters on the
performance of BS discovery, and show that RS transmission with narrow beams
may not be helpful in improving the overall BS discovery performance due to the
cost of spatial scanning. Using the method of large deviations, we identify the
desirable beam pattern that minimises the average miss-discovery probability of
UEs within a targeted detectable region. We then propose to transmit the RS
with sequential scanning, using a pre-designed codebook with narrow and/or wide
beams to approximate the desirable patterns. The proposed design allows
flexible choices of the codebook sizes and the associated beam widths to better
approximate the desirable patterns. Numerical results demonstrate the
effectiveness of the proposed method.
</p>
<br><br><br><p>Multi-scale deep CNNs have been used successfully for problems mapping each
pixel to a label, such as depth estimation and semantic segmentation. It has
also been shown that such architectures are reusable and can be used for
multiple tasks. These networks are typically trained independently for each
task by varying the output layer(s) and training objective. In this work we
present a new model for simultaneous depth estimation and semantic segmentation
from a single RGB image. Our approach demonstrates the feasibility of training
parts of the model for each task and then fine tuning the full, combined model
on both tasks simultaneously using a single loss function. Furthermore we
couple the deep CNN with fully connected CRF, which captures the contextual
relationships and interactions between the semantic and depth cues improving
the accuracy of the final results. The proposed model is trained and evaluated
on NYUDepth V2 dataset outperforming the state of the art methods on semantic
segmentation and achieving comparable results on the task of depth estimation.
</p>
<br><br><br><p>Constant envelope (CE) precoding is an appealing transmission technique,
which enables highly efficient power amplification, and is realizable with a
single radio frequency (RF) chain at the multi-antenna transmitter. In this
paper, we study the transceiver design for a point-to-point multiple-input
multiple-output (MIMO) system with CE precoding. Both single-stream
transmission (i.e., beamforming) and multi-stream transmission (i.e., spatial
multiplexing) are considered. For single-stream transmission, we optimize the
receive beamforming vector to minimize the symbol error rate (SER) for any
given channel realization and desired constellation at the combiner output. By
reformulating the problem as an equivalent quadratically constrained quadratic
program (QCQP), we propose an efficient semi-definite relaxation (SDR) based
algorithm to find an approximate solution. Next, for multi-stream transmission,
we propose a new scheme based on antenna grouping at the transmitter and
minimum mean squared error (MMSE) or zero-forcing (ZF) based beamforming at the
receiver. The transmit antenna grouping and receive beamforming vectors are
then jointly designed to minimize the maximum SER over all data streams.
Finally, the error-rate performance of single- versus multi-stream transmission
is compared via simulations under different setups.
</p>
<br><br><br><p>We consider the Massive Multiple-Input Multiple-Output (MIMO) downlink with
maximum-ratio and zero-forcing processing and time-division duplex (TDD)
operation. To decode, the terminals must know their instantaneous effective
channel gain. Conventionally, it is assumed that by virtue of channel
hardening, this instantaneous gain is close to its average and hence that
terminals can rely on knowledge of that average (also known as statistical
channel information). However, in some propagation environments, such as
keyhole channels, channel hardening does not hold.
</p>
<p>We propose a blind algorithm to estimate the effective channel gain at each
user, that does not require any downlink pilots. We derive a capacity lower
bound of each user for our proposed scheme, applicable to any propagation
channel. Compared to the case of no downlink pilots (relying on channel
hardening), and compared to training-based estimation using downlink pilots,
our blind algorithm performs significantly better. The difference is especially
pronounced in environments that do not offer channel hardening.
</p>
<br><br><br><p>We introduce an online popularity prediction and tracking task as a benchmark
task for reinforcement learning with a combinatorial, natural language action
space. A specified number of discussion threads predicted to be popular are
recommended, chosen from a fixed window of recent comments to track. Novel deep
reinforcement learning architectures are studied for effective modeling of the
value function associated with actions comprised of interdependent sub-actions.
The proposed model, which represents dependence between sub-actions through a
bi-directional LSTM, gives the best performance across different experimental
configurations and domains, and it also generalizes well with varying numbers
of recommendation requests.
</p>
<br><br><br><p>Convolutional neural networks (CNNs) have yielded the excellent performance
in a variety of computer vision tasks, where CNNs typically adopt a similar
structure consisting of convolution layers, pooling layers and fully connected
layers. In this paper, we propose to apply a novel method, namely Hybrid
Orthogonal Projection and Estimation (HOPE), to CNNs in order to introduce
orthogonality into the CNN structure. The HOPE model can be viewed as a hybrid
model to combine feature extraction using orthogonal linear projection with
mixture models. It is an effective model to extract useful information from the
original high-dimension feature vectors and meanwhile filter out irrelevant
noises. In this work, we present three different ways to apply the HOPE models
to CNNs, i.e., {\em HOPE-Input}, {\em single-HOPE-Block} and {\em
multi-HOPE-Blocks}. For {\em HOPE-Input} CNNs, a HOPE layer is directly used
right after the input to de-correlate high-dimension input feature vectors.
Alternatively, in {\em single-HOPE-Block} and {\em multi-HOPE-Blocks} CNNs, we
consider to use HOPE layers to replace one or more blocks in the CNNs, where
one block may include several convolutional layers and one pooling layer. The
experimental results on both Cifar-10 and Cifar-100 data sets have shown that
the orthogonal constraints imposed by the HOPE layers can significantly improve
the performance of CNNs in these image classification tasks (we have achieved
one of the best performance when image augmentation has not been applied, and
top 5 performance with image augmentation).
</p>
<br><br><br><p>In order to avoid the curse of dimensionality, frequently encountered in Big
Data analysis, there was a vast development in the field of linear and
non-linear dimension reduction techniques in recent years. These techniques
(sometimes referred to as manifold learning) assume that the scattered input
data is lying on a lower dimensional manifold, thus the high dimensionality
problem can be overcome by learning the lower dimensionality behavior. However,
in real life applications, data is often very noisy. In this work, we propose a
method to approximate a $d$-dimensional $C^{m+1}$ smooth submanifold
$\mathcal{M}$ residing in $\mathbb{R}^n$ ($d &lt;&lt; n$) based upon scattered data
points (i.e., a data cloud). We assume that the data points are located \"near\"
the noisy lower dimensional manifold and perform a non-linear moving
least-squares projection on an approximating manifold. Under some mild
assumptions, the resulting approximant is shown to be infinitely smooth and of
high approximation order (i.e., $O(h^{m+1})$, where $h$ is the fill distance
and $m$ is the degree of the local polynomial approximation). Furthermore, the
method presented here assumes no analytic knowledge of the approximated
manifold and the approximation algorithm is linear in the large dimension $n$.
</p>
<br><br><br><p>There is an abundance of temporal and non-temporal data in banking (and other
industries), but such temporal activity data can not be used directly with
classical machine learning models. In this work, we perform extensive feature
extraction from the temporal user activity data in an attempt to predict user
visits to different branches and credit card up-selling utilizing user
information and the corresponding activity data, as part of \emph{ECML/PKDD
Discovery Challenge 2016 on Bank Card Usage Analysis}. Our solution ranked
\nth{4} for \emph{Task 1} and achieved an AUC of \textbf{$0.7056$} for
\emph{Task 2} on public leaderboard.
</p>
<br><br><br><p>In this work, we propose a robust Head-Related Transfer Function (HRTF)-based
polynomial beamformer design which accounts for the influence of a humanoid
robot's head on the sound field. In addition, it allows for a flexible steering
of our previously proposed robust HRTF-based beamformer design. We evaluate the
HRTF-based polynomial beamformer design and compare it to the original
HRTF-based beamformer design by means of signal-independent measures as well as
word error rates of an off-the-shelf speech recognition system. Our results
confirm the effectiveness of the polynomial beamformer design, which makes it a
promising approach to robust beamforming for robot audition.
</p>
<br><br><br><p>While there has been a success in 2D human pose estimation with convolutional
neural networks (CNNs), 3D human pose estimation has not been thoroughly
studied. In this paper, we tackle the 3D human pose estimation task with
end-to-end learning using CNNs. Relative 3D positions between one joint and the
other joints are learned via CNNs. The proposed method improves the performance
of CNN with two novel ideas. First, we added 2D pose information to estimate a
3D pose from an image by concatenating 2D pose estimation result with the
features from an image. Second, we have found that more accurate 3D poses are
obtained by combining information on relative positions with respect to
multiple joints, instead of just one root joint. Experimental results show that
the proposed method achieves comparable performance to the state-of-the-art
methods on Human 3.6m dataset.
</p>
<br><br><br><p>The study of graph-based submodular maximization problems was initiated in a
seminal work of Kempe, Kleinberg, and Tardos (2003): An {\em influence}
function of subsets of nodes is defined by the graph structure and the aim is
to find subsets of seed nodes with (approximately) optimal tradeoff of size and
influence. Applications include viral marketing, monitoring, and active
learning of node labels. This powerful formulation was studied for
(generalized) {\em coverage} functions, where the influence of a seed set on a
node is the maximum utility of a seed item to the node, and for pairwise {\em
utility} based on reachability, distances, or reverse ranks.
</p>
<p>We define a rich class of influence functions which unifies and extends
previous work beyond coverage functions and specific utility functions. We
present a meta-algorithm for approximate greedy maximization with strong
approximation quality guarantees and worst-case near-linear computation for all
functions in our class. Our meta-algorithm generalizes a recent design by Cohen
et al (2014) that was specific for distance-based coverage functions.
</p>
<br><br><br><p>With the agreement of my coauthors, I Zhangyang Wang would like to withdraw
the manuscript “Stacked Approximated Regression Machine: A Simple Deep Learning
Approach”. Some experimental procedures were not included in the manuscript,
which makes a part of important claims not meaningful. In the relevant
research, I was solely responsible for carrying out the experiments; the other
coauthors joined in the discussions leading to the main algorithm.
</p>
<p>Please see the updated text for more details.
</p>
<br><br><br><p>This paper presents our method to retrieve relevant queries given a new
question in the context of Discovery Challenge: Learning to Re-Ranking
Questions for Community Question Answering competition. In order to do that, a
set of learning to rank methods was investigated to select an appropriate
method. The selected method was optimized on training data by using a search
strategy. After optimizing, the method was applied to development and test set.
Results from the competition indicate that the performance of our method
outperforms almost participants and show that Ranking SVM is efficient for
retrieving relevant queries in community question answering.
</p>
<br><br><br><p>While two-way relaying is a promising way to en- hance the spectral
efficiency of wireless networks, the imbalance of relay-user distances may lead
to excessive wireless power at the nearby-users. To exploit the excessive
power, the recently pro- posed harvest-then-transmit technique can be applied.
However, it is well-known that harvest-then-transmit introduces uplink-
downlink coupling for a user. Together with the co-dependent relationship
between paired users and interference among multi- ple user pairs, wirelessly
powered two-way relay network suffers from the unique pairwise uplink-downlink
coupling, and the joint uplink-downlink network design is nontrivial. To this
end, for the one pair users case, we show that a global optimal solution can be
obtained. For the general case of multi-pair users, based on the
rank-constrained difference of convex program, a convergence guaranteed
iterative algorithm with an efficient initialization is proposed. Furthermore,
a lower bound to the performance of the optimal solution is derived by
introducing virtual receivers at relay. Numerical results on total transmit
power show that the proposed algorithm achieves a transmit power value close to
the lower bound.
</p>
<br><br><br><p>A great video title describes the most salient event compactly and captures
the viewer's attention. In contrast, video captioning tends to generate
sentences that describe the video as a whole. Although generating a video title
automatically is a very useful task, it is much less addressed than video
captioning. We address video title generation for the first time by proposing
two methods that extend state-of-the-art video captioners to this new task.
First, we make video captioners highlight sensitive by priming them with a
highlight detector. Our framework allows for jointly training a model for title
generation and video highlight localization. Second, we induce high sentence
diversity in video captioners, so that the generated titles are also diverse
and catchy. This means that a large number of sentences might be required to
learn the sentence structure of titles. Hence, we propose a novel sentence
augmentation method to train a captioner with additional sentence-only examples
that come without corresponding videos. We collected a large-scale Video Titles
in the Wild (VTW) dataset of 18100 automatically crawled user-generated videos
and titles. On VTW, our methods consistently improve title prediction accuracy,
and achieve the best performance in both automatic and human evaluation.
Finally, our sentence augmentation method also outperforms the baselines on the
M-VAD dataset.
</p>
<br><br><br><p>Programmability and verifiability lie at the heart of the software-defined
networking paradigm. While OpenFlow and its match-action concept provide
primitive operations to manipulate hardware configurations, over the last
years, several more expressive network programming languages have been
developed. This paper presents WNetKAT, the first network programming language
accounting for the fact that networks are inherently weighted, and
communications subject to capacity constraints (e.g., in terms of bandwidth)
and costs (e.g., latency or monetary costs). WNetKAT is based on a syntactic
and semantic extension of the NetKAT algebra. We demonstrate several relevant
applications for WNetKAT, including cost- and capacity-aware reachability, as
well as quality-of-service and fairness aspects. These applications do not only
apply to classic, splittable and unsplittable (s; t)-flows, but also generalize
to more complex network functions and service chains. For example, WNetKAT
allows to model flows which need to traverse certain waypoint functions, which
may change the traffic rate. This paper also shows the relation between the
equivalence problem of WNetKAT and the equivalence problem of the weighted
finite automata, which implies undecidability of the former. However, this
paper also succeeds to prove the decidability of another useful problem, which
is sufficient in many practical scnearios: whether an expression equals to 0.
Moreover, we initiate the discussion of decidable subsets of the whole
language.
</p>
<br><br><br><p>Hybrid Communicating Sequential Processes (HCSP) is a powerful formal
modeling language for hybrid systems, which is an extension of CSP by
introducing differential equations for modeling continuous evolution and
interrupts for modeling interaction between continuous and discrete dynamics.
In this paper, we investigate the semantic foundation for HCSP from an
operational point of view by proposing notion of approximate bisimulation,
which provides an appropriate criterion to characterize the equivalence between
HCSP processes with continuous and discrete behaviour. We give an algorithm to
determine whether two HCSP processes are approximately bisimilar. In addition,
based on that, we propose an approach on how to discretize HCSP, i.e., given an
HCSP process A, we construct another HCSP process B which does not contain any
continuous dynamics such that A and B are approximately bisimilar with given
precisions. This provides a rigorous way to transform a verified control model
to a correct program model, which fills the gap in the design of embedded
systems.
</p>
<br><br><br><p>To optimize the use of the spectrum, it is expected that the next generation
of wireless networks (5G) will enable coexistence of newly introduced services
with legacy cellular networks. These new services, like Device-To-Device (D2D)
communication, should require limited synchronization with the legacy cell to
limit the amount of signaling overhead in the network. However, it is known
that Cyclic Prefix-Orthogonal Frequency Division Multiplexing (CP-OFDM) used in
Long Term Evolution-Advanced (LTE-A) is not fit for asynchronous environments.
This has motivated the search for a new waveform, able to enhance coexistence
with CP-OFDM. Namely, it has been widely suggested that new devices could use
OFDM/Offset-Quadrature Amplitude Modulation (OFDM/OQAM) to reduce the
interference they inject to legacy cellular users. However, values of
interference are usually measured at the input antenna of the receiver, based
on the PSD of the interfering signal. We showed in previous works that this
measurement is not representative of the actual interference that is seen after
the demodulation operations. Building on this finding, we provide in this paper
the first exact closed forms of cross-interference between OFDM/OQAM and
CP-OFDM users. Our results prove that using OFDM/OQAM only marginally reduces
interference to legacy users, in contradiction with many results in the
literature.
</p>
<br><br><br><p>Starting from a generalization of the standard axioms for a monoid we present
a stepwise development of various, mutually equivalent foundational axiom
systems for category theory. Our axiom sets have been formalized in the
Isabelle/HOL interactive proof assistant, and this formalization utilizes a
semantically correct embedding of free logic in classical higher-order logic.
The modeling and formal analysis of our axiom sets has been significantly
supported by series of experiments with automated reasoning tools integrated
with Isabelle/HOL. We also address the relation of our axiom systems to
alternative proposals from the literature, including an axiom set proposed by
Freyd and Scedrov for which we reveal a technical issue (when encoded in free
logic where free variables range over defined and undefined objects): either
all operations, e.g. morphism composition, are total or their axiom system is
inconsistent. The repair for this problem is quite straightforward, however.
</p>
<br><br><br><p>This work advocates Eulerian motion representation learning over the current
standard Lagrangian optical flow model. Eulerian motion is well captured by
using phase, as obtained by decomposing the image through a complex-steerable
pyramid. We discuss the gain of Eulerian motion in a set of practical use
cases: (i) action recognition, (ii) motion prediction in static images, (iii)
motion transfer in static images and, (iv) motion transfer in video. For each
task we motivate the phase-based direction and provide a possible approach.
</p>
<br><br><br><p>We calculate the moments of comparisons used by the randomized quick sort
algorithm by the approach of Kirschenofer, Prodinger and Tichy to analyze
Knuth's In Situ permutation algorithm. This approach helps in calculating these
quantities with less computation.
</p>
<br><br><br><p>Motion estimation for highly dynamic phenomena such as smoke is an open
challenge for Computer Vision. Traditional dense motion estimation algorithms
have difficulties with non-rigid and large motions, both of which are
frequently observed in smoke motion. We propose an algorithm for dense motion
estimation of smoke. Our algorithm is robust, fast, and has better performance
over different types of smoke compared to other dense motion estimation
algorithms, including state of the art and neural network approaches. The key
to our contribution is to use skeletal flow, without explicit point matching,
to provide a sparse flow. This sparse flow is upgraded to a dense flow. In this
paper we describe our algorithm in greater detail, and provide experimental
evidence to support our claims.
</p>
<br><br><br><p>This article proposes a performance analysis of kernel least squares support
vector machines (LS-SVMs) based on a random matrix approach, in the regime
where both the dimension of data $p$ and their number $n$ grow large at the
same rate. Under a two-class Gaussian mixture model for the input data, we
prove that the LS-SVM decision function is asymptotically normal with means and
covariances shown to depend explicitly on the derivatives of the kernel
function. This provides improved understanding along with new insights into the
internal workings of SVM-type methods for large datasets.
</p>
<br><br><br></div>
<div style="visibility:hidden; display: none;" id="links">http://arxiv.org/pdf/1609.02142<br><br><br>http://arxiv.org/pdf/1609.02143<br><br><br>http://arxiv.org/pdf/1609.02144<br><br><br>http://arxiv.org/pdf/1609.02145<br><br><br>http://arxiv.org/pdf/1609.02147<br><br><br>http://arxiv.org/pdf/1609.02148<br><br><br>http://arxiv.org/pdf/1609.02150<br><br><br>http://arxiv.org/pdf/1609.02152<br><br><br>http://arxiv.org/pdf/1609.02153<br><br><br>http://arxiv.org/pdf/1609.02155<br><br><br>http://arxiv.org/pdf/1609.02156<br><br><br>http://arxiv.org/pdf/1609.02159<br><br><br>http://arxiv.org/pdf/1609.02178<br><br><br>http://arxiv.org/pdf/1609.02183<br><br><br>http://arxiv.org/pdf/1609.02185<br><br><br>http://arxiv.org/pdf/1609.02186<br><br><br>http://arxiv.org/pdf/1609.02201<br><br><br>http://arxiv.org/pdf/1609.02203<br><br><br>http://arxiv.org/pdf/1609.02204<br><br><br>http://arxiv.org/pdf/1609.02207<br><br><br>http://arxiv.org/pdf/1609.02215<br><br><br>http://arxiv.org/pdf/1609.02222<br><br><br>http://arxiv.org/pdf/1609.02232<br><br><br>http://arxiv.org/pdf/1609.02237<br><br><br>http://arxiv.org/pdf/1609.02248<br><br><br>http://arxiv.org/pdf/1609.02256<br><br><br>http://arxiv.org/pdf/1609.02266<br><br><br>http://arxiv.org/pdf/1609.02267<br><br><br>http://arxiv.org/pdf/1609.02278<br><br><br>http://arxiv.org/pdf/1609.02279<br><br><br>http://arxiv.org/pdf/1609.02283<br><br><br>http://arxiv.org/pdf/1609.02307<br><br><br>http://arxiv.org/pdf/1609.02311<br><br><br>http://arxiv.org/pdf/1609.02312<br><br><br>http://arxiv.org/pdf/1609.02327<br><br><br>http://arxiv.org/pdf/1609.02333<br><br><br>http://arxiv.org/pdf/1609.02336<br><br><br>http://arxiv.org/pdf/1609.02350<br><br><br>http://arxiv.org/pdf/1609.02358<br><br><br>http://arxiv.org/pdf/1609.02365<br><br><br>http://arxiv.org/pdf/1609.02367<br><br><br>http://arxiv.org/pdf/1609.02370<br><br><br>http://arxiv.org/pdf/1609.02372<br><br><br>http://arxiv.org/pdf/1609.02373<br><br><br>http://arxiv.org/pdf/1609.02376<br><br><br>http://arxiv.org/pdf/1609.02379<br><br><br>http://arxiv.org/pdf/1609.02386<br><br><br>http://arxiv.org/pdf/1609.02388<br><br><br>http://arxiv.org/pdf/1609.02396<br><br><br>http://arxiv.org/pdf/1609.02400<br><br><br>http://arxiv.org/pdf/1609.02401<br><br><br>http://arxiv.org/pdf/1609.02405<br><br><br>http://arxiv.org/pdf/1609.02417<br><br><br>http://arxiv.org/pdf/1609.02436<br><br><br>http://arxiv.org/pdf/1609.02448<br><br><br>http://arxiv.org/pdf/1609.02459<br><br><br>http://arxiv.org/pdf/1609.02476<br><br><br>http://arxiv.org/pdf/1609.02477<br><br><br>http://arxiv.org/pdf/1609.02480<br><br><br>http://arxiv.org/pdf/1609.02481<br><br><br>http://arxiv.org/pdf/1609.02485<br><br><br>http://arxiv.org/pdf/1609.02488<br><br><br>http://arxiv.org/pdf/1609.02493<br><br><br>http://arxiv.org/pdf/1609.02497<br><br><br>http://arxiv.org/pdf/1609.02498<br><br><br>http://arxiv.org/pdf/1609.02508<br><br><br>http://arxiv.org/pdf/1609.02518<br><br><br>http://arxiv.org/pdf/1609.02530<br><br><br>http://arxiv.org/pdf/1609.02544<br><br><br>http://arxiv.org/pdf/1609.02553<br><br><br>http://arxiv.org/pdf/1501.05207<br><br><br>http://arxiv.org/pdf/1507.03021<br><br><br>http://arxiv.org/pdf/1511.02882<br><br><br>http://arxiv.org/pdf/1511.07586<br><br><br>http://arxiv.org/pdf/1602.02732<br><br><br>http://arxiv.org/pdf/1603.04082<br><br><br>http://arxiv.org/pdf/1603.08519<br><br><br>http://arxiv.org/pdf/1605.01022<br><br><br>http://arxiv.org/pdf/1605.07236<br><br><br>http://arxiv.org/pdf/1606.04510<br><br><br>http://arxiv.org/pdf/1606.04538<br><br><br>http://arxiv.org/pdf/1607.00529<br><br><br>http://arxiv.org/pdf/1607.01379<br><br><br>http://arxiv.org/pdf/1607.04687<br><br><br>http://arxiv.org/pdf/1607.06769<br><br><br>http://arxiv.org/pdf/1608.02001<br><br><br>http://arxiv.org/pdf/1608.02130<br><br><br>http://arxiv.org/pdf/1608.06911<br><br><br>http://arxiv.org/pdf/1608.08453<br><br><br>http://arxiv.org/pdf/1609.00437<br><br><br>http://arxiv.org/pdf/1609.00726<br><br><br>http://arxiv.org/pdf/1609.01694<br><br><br>http://arxiv.org/pdf/1609.02449<br><br><br>http://arxiv.org/pdf/1609.02466<br><br><br>http://arxiv.org/pdf/1602.07742<br><br><br>http://arxiv.org/pdf/1607.04863<br><br><br>http://arxiv.org/pdf/1609.02207<br><br><br>http://arxiv.org/pdf/1609.02325<br><br><br>http://arxiv.org/pdf/1609.02341<br><br><br>http://arxiv.org/pdf/1609.02382<br><br><br>http://arxiv.org/pdf/1609.02449<br><br><br>http://arxiv.org/pdf/1609.02472<br><br><br>http://arxiv.org/pdf/1601.05291<br><br><br>http://arxiv.org/pdf/1609.01912<br><br><br>http://arxiv.org/pdf/1609.02154<br><br><br>http://arxiv.org/pdf/1609.02173<br><br><br>http://arxiv.org/pdf/1609.02196<br><br><br>http://arxiv.org/pdf/1609.02220<br><br><br>http://arxiv.org/pdf/1609.02223<br><br><br>http://arxiv.org/pdf/1609.02260<br><br><br>http://arxiv.org/pdf/1609.02326<br><br><br>http://arxiv.org/pdf/1609.02347<br><br><br>http://arxiv.org/pdf/1609.02359<br><br><br>http://arxiv.org/pdf/1609.02413<br><br><br>http://arxiv.org/pdf/1609.02425<br><br><br>http://arxiv.org/pdf/1609.02439<br><br><br>http://arxiv.org/pdf/1609.02447<br><br><br>http://arxiv.org/pdf/1609.02473<br><br><br>http://arxiv.org/pdf/1609.02511<br><br><br>http://arxiv.org/pdf/1609.02525<br><br><br>http://arxiv.org/pdf/1609.02527<br><br><br>http://arxiv.org/pdf/1203.0832<br><br><br>http://arxiv.org/pdf/1401.5938<br><br><br>http://arxiv.org/pdf/1508.06305<br><br><br>http://arxiv.org/pdf/1603.05409<br><br><br>http://arxiv.org/pdf/1604.07779<br><br><br>http://arxiv.org/pdf/1606.05230<br><br><br>http://arxiv.org/pdf/1609.01312<br><br><br>http://arxiv.org/pdf/1609.02160<br><br><br>http://arxiv.org/pdf/1609.02169<br><br><br>http://arxiv.org/pdf/1609.02170<br><br><br>http://arxiv.org/pdf/1609.02180<br><br><br>http://arxiv.org/pdf/1609.02224<br><br><br>http://arxiv.org/pdf/1609.02225<br><br><br>http://arxiv.org/pdf/1609.02230<br><br><br>http://arxiv.org/pdf/1609.02239<br><br><br>http://arxiv.org/pdf/1609.02241<br><br><br>http://arxiv.org/pdf/1609.02246<br><br><br>http://arxiv.org/pdf/1609.02255<br><br><br>http://arxiv.org/pdf/1609.02265<br><br><br>http://arxiv.org/pdf/1609.02282<br><br><br>http://arxiv.org/pdf/1609.02294<br><br><br>http://arxiv.org/pdf/1609.02295<br><br><br>http://arxiv.org/pdf/1609.02322<br><br><br>http://arxiv.org/pdf/1609.02323<br><br><br>http://arxiv.org/pdf/1609.02329<br><br><br>http://arxiv.org/pdf/1609.02337<br><br><br>http://arxiv.org/pdf/1609.02355<br><br><br>http://arxiv.org/pdf/1609.02378<br><br><br>http://arxiv.org/pdf/1609.02389<br><br><br>http://arxiv.org/pdf/1609.02403<br><br><br>http://arxiv.org/pdf/1609.02412<br><br><br>http://arxiv.org/pdf/1609.02416<br><br><br>http://arxiv.org/pdf/1609.02428<br><br><br>http://arxiv.org/pdf/1609.02429<br><br><br>http://arxiv.org/pdf/1609.02431<br><br><br>http://arxiv.org/pdf/1609.02432<br><br><br>http://arxiv.org/pdf/1609.02435<br><br><br>http://arxiv.org/pdf/1609.02439<br><br><br>http://arxiv.org/pdf/1609.02465<br><br><br>http://arxiv.org/pdf/1609.02492<br><br><br>http://arxiv.org/pdf/1609.02522<br><br><br>http://arxiv.org/pdf/1609.02542<br><br><br>http://arxiv.org/pdf/1408.0179<br><br><br>http://arxiv.org/pdf/1508.05691<br><br><br>http://arxiv.org/pdf/1509.09180<br><br><br>http://arxiv.org/pdf/1510.03875<br><br><br>http://arxiv.org/pdf/1511.07815<br><br><br>http://arxiv.org/pdf/1512.01636<br><br><br>http://arxiv.org/pdf/1512.01669<br><br><br>http://arxiv.org/pdf/1512.05927<br><br><br>http://arxiv.org/pdf/1601.08095<br><br><br>http://arxiv.org/pdf/1602.01093<br><br><br>http://arxiv.org/pdf/1603.00627<br><br><br>http://arxiv.org/pdf/1604.00689<br><br><br>http://arxiv.org/pdf/1604.02026<br><br><br>http://arxiv.org/pdf/1604.04985<br><br><br>http://arxiv.org/pdf/1605.08211<br><br><br>http://arxiv.org/pdf/1605.09275<br><br><br>http://arxiv.org/pdf/1606.01090<br><br><br>http://arxiv.org/pdf/1606.02229<br><br><br>http://arxiv.org/pdf/1606.02484<br><br><br>http://arxiv.org/pdf/1607.00529<br><br><br>http://arxiv.org/pdf/1607.06438<br><br><br>http://arxiv.org/pdf/1608.00136<br><br><br>http://arxiv.org/pdf/1609.01707<br><br><br>http://arxiv.org/pdf/1609.01968<br><br><br>http://arxiv.org/pdf/1609.02055<br><br><br>http://arxiv.org/pdf/1609.02092<br><br><br>http://arxiv.org/pdf/1609.02137<br><br><br>http://arxiv.org/pdf/1609.02139<br><br><br>http://arxiv.org/pdf/1609.02171<br><br><br>http://arxiv.org/pdf/1609.02174<br><br><br>http://arxiv.org/pdf/1609.02182<br><br><br>http://arxiv.org/pdf/1609.02191<br><br><br>http://arxiv.org/pdf/1609.02193<br><br><br>http://arxiv.org/pdf/1609.02197<br><br><br>http://arxiv.org/pdf/1609.02198<br><br><br>http://arxiv.org/pdf/1609.02200<br><br><br>http://arxiv.org/pdf/1609.02208<br><br><br>http://arxiv.org/pdf/1609.02214<br><br><br>http://arxiv.org/pdf/1609.02226<br><br><br>http://arxiv.org/pdf/1609.02227<br><br><br>http://arxiv.org/pdf/1609.02228<br><br><br>http://arxiv.org/pdf/1609.02234<br><br><br>http://arxiv.org/pdf/1609.02236<br><br><br>http://arxiv.org/pdf/1609.02243<br><br><br>http://arxiv.org/pdf/1609.02251<br><br><br>http://arxiv.org/pdf/1609.02252<br><br><br>http://arxiv.org/pdf/1609.02258<br><br><br>http://arxiv.org/pdf/1609.02271<br><br><br>http://arxiv.org/pdf/1609.02281<br><br><br>http://arxiv.org/pdf/1609.02284<br><br><br>http://arxiv.org/pdf/1609.02286<br><br><br>http://arxiv.org/pdf/1609.02288<br><br><br>http://arxiv.org/pdf/1609.02300<br><br><br>http://arxiv.org/pdf/1609.02302<br><br><br>http://arxiv.org/pdf/1609.02305<br><br><br>http://arxiv.org/pdf/1609.02310<br><br><br>http://arxiv.org/pdf/1609.02316<br><br><br>http://arxiv.org/pdf/1609.02318<br><br><br>http://arxiv.org/pdf/1609.02320<br><br><br>http://arxiv.org/pdf/1609.02324<br><br><br>http://arxiv.org/pdf/1609.02339<br><br><br>http://arxiv.org/pdf/1609.02342<br><br><br>http://arxiv.org/pdf/1609.02353<br><br><br>http://arxiv.org/pdf/1609.02356<br><br><br>http://arxiv.org/pdf/1609.02368<br><br><br>http://arxiv.org/pdf/1609.02374<br><br><br>http://arxiv.org/pdf/1609.02375<br><br><br>http://arxiv.org/pdf/1609.02383<br><br><br>http://arxiv.org/pdf/1609.02391<br><br><br>http://arxiv.org/pdf/1609.02398<br><br><br>http://arxiv.org/pdf/1609.02404<br><br><br>http://arxiv.org/pdf/1609.02409<br><br><br>http://arxiv.org/pdf/1609.02410<br><br><br>http://arxiv.org/pdf/1609.02411<br><br><br>http://arxiv.org/pdf/1609.02422<br><br><br>http://arxiv.org/pdf/1609.02423<br><br><br>http://arxiv.org/pdf/1609.02427<br><br><br>http://arxiv.org/pdf/1609.02434<br><br><br>http://arxiv.org/pdf/1609.02440<br><br><br>http://arxiv.org/pdf/1609.02443<br><br><br>http://arxiv.org/pdf/1609.02446<br><br><br>http://arxiv.org/pdf/1609.02450<br><br><br>http://arxiv.org/pdf/1609.02451<br><br><br>http://arxiv.org/pdf/1609.02452<br><br><br>http://arxiv.org/pdf/1609.02453<br><br><br>http://arxiv.org/pdf/1609.02456<br><br><br>http://arxiv.org/pdf/1609.02460<br><br><br>http://arxiv.org/pdf/1609.02462<br><br><br>http://arxiv.org/pdf/1609.02469<br><br><br>http://arxiv.org/pdf/1609.02489<br><br><br>http://arxiv.org/pdf/1609.02490<br><br><br>http://arxiv.org/pdf/1609.02500<br><br><br>http://arxiv.org/pdf/1609.02513<br><br><br>http://arxiv.org/pdf/1609.02517<br><br><br>http://arxiv.org/pdf/1609.02519<br><br><br>http://arxiv.org/pdf/1609.02521<br><br><br>http://arxiv.org/pdf/1609.02526<br><br><br>http://arxiv.org/pdf/1609.02531<br><br><br>http://arxiv.org/pdf/1609.02532<br><br><br>http://arxiv.org/pdf/1609.02542<br><br><br>http://arxiv.org/pdf/1609.02546<br><br><br>http://arxiv.org/pdf/1609.02549<br><br><br>http://arxiv.org/pdf/1609.02554<br><br><br>http://arxiv.org/pdf/1310.5245<br><br><br>http://arxiv.org/pdf/1410.2011<br><br><br>http://arxiv.org/pdf/1503.00757<br><br><br>http://arxiv.org/pdf/1506.01634<br><br><br>http://arxiv.org/pdf/1506.02721<br><br><br>http://arxiv.org/pdf/1507.02618<br><br><br>http://arxiv.org/pdf/1507.07034<br><br><br>http://arxiv.org/pdf/1508.03715<br><br><br>http://arxiv.org/pdf/1511.01561<br><br><br>http://arxiv.org/pdf/1511.04387<br><br><br>http://arxiv.org/pdf/1512.04087<br><br><br>http://arxiv.org/pdf/1512.09075<br><br><br>http://arxiv.org/pdf/1603.03236<br><br><br>http://arxiv.org/pdf/1603.08152<br><br><br>http://arxiv.org/pdf/1604.01160<br><br><br>http://arxiv.org/pdf/1604.07480<br><br><br>http://arxiv.org/pdf/1605.06290<br><br><br>http://arxiv.org/pdf/1606.02348<br><br><br>http://arxiv.org/pdf/1606.03667<br><br><br>http://arxiv.org/pdf/1606.05929<br><br><br>http://arxiv.org/pdf/1606.07104<br><br><br>http://arxiv.org/pdf/1607.06123<br><br><br>http://arxiv.org/pdf/1607.06642<br><br><br>http://arxiv.org/pdf/1608.03075<br><br><br>http://arxiv.org/pdf/1608.04036<br><br><br>http://arxiv.org/pdf/1608.04062<br><br><br>http://arxiv.org/pdf/1608.04185<br><br><br>http://arxiv.org/pdf/1608.06405<br><br><br>http://arxiv.org/pdf/1608.07068<br><br><br>http://arxiv.org/pdf/1608.08483<br><br><br>http://arxiv.org/pdf/1609.00091<br><br><br>http://arxiv.org/pdf/1609.01443<br><br><br>http://arxiv.org/pdf/1609.01493<br><br><br>http://arxiv.org/pdf/1609.01693<br><br><br>http://arxiv.org/pdf/1609.01870<br><br><br>http://arxiv.org/pdf/1609.02001<br><br><br>http://arxiv.org/pdf/1609.02020<br><br><br></div>

</body>
</html>